{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJqZ76rJh2rM",
    "outputId": "c7648628-a72f-467b-bca4-b7fb003885fe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your project ID is: gen-lang-client-0303567819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your active configuration is: [personal]\n"
     ]
    }
   ],
   "source": [
    "# Define project information\n",
    "\n",
    "import sys\n",
    "\n",
    "PROJECT_ID = \"gen-lang-client-0303567819\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "# if not running on colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "D48gUW5-h2rM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rtMowvm-yQ97",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rich import print as rich_print\n",
    "from rich.markdown import Markdown as rich_Markdown\n",
    "from IPython.display import Markdown, display\n",
    "from vertexai.generative_models import (\n",
    "    Content,\n",
    "    GenerationConfig,\n",
    "    GenerationResponse,\n",
    "    GenerativeModel,\n",
    "    HarmCategory,\n",
    "    HarmBlockThreshold,\n",
    "    Image,\n",
    "    Part,\n",
    ")\n",
    "from vertexai.language_models import TextEmbeddingModel\n",
    "\n",
    "# Suppress deprecation warning for vision_models (deprecated until June 2026)\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*vision_models.*\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*deprecated.*\")\n",
    "    from vertexai.vision_models import MultiModalEmbeddingModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SvMwSRJJh2rM",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n",
      "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n",
      "E0000 00:00:1762616269.378261   35855 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1762616269.657345   35855 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Multimodal models: Choose based on your performance/cost needs\n",
    "\n",
    "multimodal_model_2_0_flash = GenerativeModel(\n",
    "    \"gemini-2.0-flash-001\"\n",
    ") # Gemini latest Gemini 2.0 Flash Model\n",
    "\n",
    "multimodal_model_15 = GenerativeModel(\n",
    "    \"gemini-1.5-pro-001\"\n",
    ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
    "\n",
    "# Multimodal models: Choose based on your performance/cost needs\n",
    "multimodal_model_15_flash = GenerativeModel(\n",
    "    \"gemini-1.5-flash-001\"\n",
    ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
    "\n",
    "# Load text embedding model from pre-trained source\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
    "\n",
    "# Load multimodal embedding model from pre-trained source\n",
    "# Suppress deprecation warning when loading the model\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*vision_models.*\")\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*deprecated.*\")\n",
    "    multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
    "        \"multimodalembedding@001\"\n",
    "    )  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
      "  warning_logs.show_deprecation_warning()\n",
      "E0000 00:00:1762616269.891412   35855 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Load text embedding model from pre-trained source\n",
    "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: multimodal_embedding_model is already correctly defined in Cell 3\n",
    "# Do not overwrite it with a GenerativeModel - it must be a MultiModalEmbeddingModel\n",
    "# If you need a different generative model, use a different variable name\n",
    "# multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multimodal_qa_with_rag_utils import (\n",
    "    get_document_metadata,\n",
    "    set_global_variable,\n",
    ")\n",
    "\n",
    "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
    "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\u00c9LULA 75.4 - \ud83d\udd04 RECARREGAR M\u00d3DULO (Execute se o erro persistir)\n",
    "# Esta c\u00e9lula recarrega o m\u00f3dulo multimodal_qa_with_rag_utils com as corre\u00e7\u00f5es mais recentes\n",
    "\n",
    "import importlib\n",
    "import multimodal_qa_with_rag_utils\n",
    "importlib.reload(multimodal_qa_with_rag_utils)\n",
    "\n",
    "# Re-importar as fun\u00e7\u00f5es atualizadas\n",
    "from multimodal_qa_with_rag_utils import (\n",
    "    get_image_embedding_from_multimodal_embedding_model,\n",
    "    get_gemini_response,\n",
    "    set_global_variable\n",
    ")\n",
    "\n",
    "print(\"\u2705 M\u00f3dulo recarregado com sucesso!\")\n",
    "print(\"\ud83d\udccb Agora execute a C\u00c9LULA 75.5 (Diagn\u00f3stico) e depois a C\u00c9LULA 76\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C\u00c9LULA 75.5 - \ud83d\udd0d DIAGN\u00d3STICO: Verificar se os modelos est\u00e3o corretos\n",
    "# Execute esta c\u00e9lula ANTES da C\u00c9LULA 76 para verificar se tudo est\u00e1 OK\n",
    "\n",
    "print(\"=== DIAGN\u00d3STICO DOS MODELOS ===\\n\")\n",
    "\n",
    "# Verificar multimodal_embedding_model\n",
    "try:\n",
    "    print(f\"\ud83d\udcca Tipo do multimodal_embedding_model: {type(multimodal_embedding_model)}\")\n",
    "    print(f\"\ud83d\udccb Tem m\u00e9todo get_embeddings? {hasattr(multimodal_embedding_model, 'get_embeddings')}\")\n",
    "    \n",
    "    if hasattr(multimodal_embedding_model, 'get_embeddings'):\n",
    "        print(\"\u2705 multimodal_embedding_model est\u00e1 CORRETO!\")\n",
    "    else:\n",
    "        print(\"\u274c multimodal_embedding_model est\u00e1 INCORRETO!\")\n",
    "        print(\"   Execute a Cell 4 novamente para carregar o modelo correto.\")\n",
    "        print(f\"   Tipo encontrado: {type(multimodal_embedding_model).__name__}\")\n",
    "        print(f\"   Tipo esperado: MultiModalEmbeddingModel\")\n",
    "except NameError:\n",
    "    print(\"\u274c multimodal_embedding_model n\u00e3o est\u00e1 definido!\")\n",
    "    print(\"   Execute a Cell 4 primeiro.\")\n",
    "\n",
    "# Verificar se a vari\u00e1vel global foi definida\n",
    "try:\n",
    "    from multimodal_qa_with_rag_utils import multimodal_embedding_model as global_model\n",
    "    print(f\"\\n\ud83d\udcca Tipo do modelo global no m\u00f3dulo: {type(global_model)}\")\n",
    "    print(f\"\ud83d\udccb Tem m\u00e9todo get_embeddings? {hasattr(global_model, 'get_embeddings')}\")\n",
    "    if hasattr(global_model, 'get_embeddings'):\n",
    "        print(\"\u2705 Modelo global est\u00e1 CORRETO!\")\n",
    "    else:\n",
    "        print(\"\u274c Modelo global est\u00e1 INCORRETO!\")\n",
    "        print(\"   Execute a Cell 7 novamente para definir a vari\u00e1vel global.\")\n",
    "except (NameError, AttributeError) as e:\n",
    "    print(f\"\\n\u26a0\ufe0f  N\u00e3o foi poss\u00edvel verificar o modelo global: {e}\")\n",
    "    print(\"   Execute a Cell 7 para definir a vari\u00e1vel global.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\ud83d\udccb PR\u00d3XIMOS PASSOS:\")\n",
    "print(\"1. Se tudo estiver \u2705, execute a C\u00c9LULA 76\")\n",
    "print(\"2. Se houver \u274c, execute as c\u00e9lulas indicadas primeiro\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\n",
      "\n",
      "\ud83d\udcc1 Buscando imagens na pasta 'images/' e subdiret\u00f3rios...\n",
      "\n",
      "\ud83d\udcc2 Arquivos na pasta 'images/' (diret\u00f3rio raiz):\n",
      "  - A1.png (73671 bytes) [extens\u00e3o: .png]\n",
      "  - B2_room.jpeg (103810 bytes) [extens\u00e3o: .jpeg]\n",
      "\n",
      "\ud83d\udcca Total de imagens encontradas: 2\n",
      "\u2705 Imagens detectadas:\n",
      "  - A1.png\n",
      "  - B2_room.jpeg\n",
      "\u2705 J\u00e1 h\u00e1 m\u00faltiplas imagens na pasta\n",
      "Execute a C\u00c9LULA 76 para processar todas e depois a C\u00c9LULA 70 para testar similaridade\n",
      "\n",
      "\ud83d\udcca STATUS FINAL:\n",
      "\ud83d\udcca Total: 2 imagens na pasta 'images/'\n",
      "\ud83d\udccb Imagens encontradas:\n",
      "  - A1.png\n",
      "  - B2_room.jpeg\n",
      "\ud83c\udf89 Pronto para testar busca por similaridade!\n",
      "\ud83d\udccb PR\u00d3XIMOS PASSOS:\n",
      "  1. Execute C\u00c9LULA 76 (processar todas as imagens)\n",
      "  2. Execute C\u00c9LULA 70 (busca por similaridade)\n",
      "  3. Execute C\u00c9LULA 71 (an\u00e1lise contextual)\n"
     ]
    }
   ],
   "source": [
    "# C\u00c9LULA 78 (OPCIONAL) - \ud83d\udcc4 EXTRAIR MAIS IMAGENS DO PDF PARA COMPARA\u00c7\u00c3O\n",
    "# Esta c\u00e9lula extrai imagens do map.pdf para ter mais dados para compara\u00e7\u00e3o\n",
    "\n",
    "print(\"=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\\n\")\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "\n",
    "def extrair_imagens_do_pdf(pdf_path, output_dir=\"images/\", prefixo=\"map\"):\n",
    "    \"\"\"\n",
    "    Extrai imagens de um PDF e salva na pasta de imagens\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\udd0d Processando PDF: {pdf_path}\")\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"\u274c PDF n\u00e3o encontrado: {pdf_path}\")\n",
    "        return []\n",
    "    \n",
    "    # Criar diret\u00f3rio se n\u00e3o existir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Abrir PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    imagens_extraidas = []\n",
    "    \n",
    "    print(f\"\ud83d\udcca PDF tem {len(doc)} p\u00e1ginas\")\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        images = page.get_images()\n",
    "        \n",
    "        print(f\"\ud83d\udcc4 P\u00e1gina {page_num + 1}: {len(images)} imagens encontradas\")\n",
    "        \n",
    "        for img_index, img in enumerate(images):\n",
    "            try:\n",
    "                # Extrair imagem\n",
    "                xref = img[0]\n",
    "                pix = fitz.Pixmap(doc, xref)\n",
    "                \n",
    "                # Converter para RGB se necess\u00e1rio\n",
    "                if pix.colorspace and pix.colorspace.n > 3:\n",
    "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
    "                \n",
    "                # Nome do arquivo\n",
    "                img_filename = f\"{prefixo}_page_{page_num + 1}_img_{img_index + 1}.png\"\n",
    "                img_path = os.path.join(output_dir, img_filename)\n",
    "                \n",
    "                # Salvar imagem\n",
    "                pix.save(img_path)\n",
    "                imagens_extraidas.append(img_path)\n",
    "                \n",
    "                print(f\"  \u2705 Extra\u00edda: {img_filename}\")\n",
    "                \n",
    "                pix = None  # Liberar mem\u00f3ria\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  \u274c Erro ao extrair imagem {img_index}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    doc.close()\n",
    "    print(f\"\\n\ud83c\udf89 Total de {len(imagens_extraidas)} imagens extra\u00eddas!\")\n",
    "    return imagens_extraidas\n",
    "\n",
    "# Verificar quantas imagens temos atualmente\n",
    "# Lista todos os arquivos para debug\n",
    "all_files = os.listdir(\"images/\")\n",
    "print(f\"\ud83d\udcc1 Todos os arquivos na pasta 'images/':\")\n",
    "for f in sorted(all_files):\n",
    "    file_path = os.path.join(\"images/\", f)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path)\n",
    "        print(f\"  - {f} ({size} bytes)\")\n",
    "\n",
    "# Contar imagens com extens\u00f5es suportadas (case-insensitive)\n",
    "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif', '.JPG', '.JPEG', '.PNG', '.BMP')\n",
    "current_images = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif'))]\n",
    "print(f\"\\n\ud83d\udcca Imagens encontradas na pasta: {len(current_images)}\")\n",
    "for img in sorted(current_images):\n",
    "    print(f\"  \u2705 {img}\")\n",
    "\n",
    "if len(current_images) <= 1:\n",
    "    print(\"\ud83d\udd04 Extraindo imagens do PDF para ter mais dados...\")\n",
    "    \n",
    "    # Extrair do map.pdf se existir\n",
    "    if os.path.exists(\"map/map.pdf\"):\n",
    "        imagens_extraidas = extrair_imagens_do_pdf(\"map/map.pdf\", \"images/\", \"map\")\n",
    "        \n",
    "        if imagens_extraidas:\n",
    "            print(f\"\\n\u2705 {len(imagens_extraidas)} novas imagens adicionadas!\")\n",
    "            print(\"\ud83d\ude80 Agora execute a C\u00c9LULA 76 novamente para processar todas as imagens\")\n",
    "            print(\"   Depois execute a C\u00c9LULA 70 para testar similaridade com mais dados\")\n",
    "        else:\n",
    "            print(\"\u274c Nenhuma imagem foi extra\u00edda do PDF\")\n",
    "    else:\n",
    "        print(\"\u274c Arquivo map/map.pdf n\u00e3o encontrado\")\n",
    "        \n",
    "        # Verificar outros PDFs dispon\u00edveis\n",
    "        print(\"\\n\ud83d\udd0d Procurando outros PDFs...\")\n",
    "        pdf_paths = []\n",
    "        for root, dirs, files in os.walk(\".\"):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.pdf'):\n",
    "                    pdf_paths.append(os.path.join(root, file))\n",
    "        \n",
    "        if pdf_paths:\n",
    "            print(\"\ud83d\udccb PDFs encontrados:\")\n",
    "            for i, pdf_path in enumerate(pdf_paths[:3], 1):  # Mostrar apenas os 3 primeiros\n",
    "                print(f\"  {i}. {pdf_path}\")\n",
    "                \n",
    "            # Processar o primeiro PDF encontrado\n",
    "            if pdf_paths:\n",
    "                primeiro_pdf = pdf_paths[0]\n",
    "                print(f\"\\n\ud83d\udd04 Processando: {primeiro_pdf}\")\n",
    "                imagens_extraidas = extrair_imagens_do_pdf(primeiro_pdf, \"images/\", \"doc\")\n",
    "                \n",
    "                if imagens_extraidas:\n",
    "                    print(f\"\\n\u2705 {len(imagens_extraidas)} imagens extra\u00eddas de {primeiro_pdf}!\")\n",
    "                    print(\"\ud83d\ude80 Execute a C\u00c9LULA 76 novamente para processar todas as imagens\")\n",
    "        else:\n",
    "            print(\"\u274c Nenhum PDF encontrado para extrair imagens\")\n",
    "            \n",
    "else:\n",
    "    print(\"\u2705 J\u00e1 h\u00e1 m\u00faltiplas imagens na pasta\")\n",
    "    print(\"Execute a C\u00c9LULA 76 para processar todas e depois a C\u00c9LULA 70 para testar similaridade\")\n",
    "\n",
    "# Mostrar status final - usar a mesma l\u00f3gica de contagem\n",
    "all_files_final = os.listdir(\"images/\")\n",
    "final_images = [f for f in all_files_final if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif'))]\n",
    "print(f\"\\n\ud83d\udcca STATUS FINAL: {len(final_images)} imagens na pasta 'images/'\")\n",
    "print(\"\ud83d\udccb Imagens encontradas:\")\n",
    "for img in sorted(final_images):\n",
    "    print(f\"  - {img}\")\n",
    "\n",
    "if len(final_images) > 1:\n",
    "    print(\"\ud83c\udf89 Pronto para testar busca por similaridade!\")\n",
    "    print(\"\ud83d\udccb PR\u00d3XIMOS PASSOS:\")\n",
    "    print(\"  1. Execute C\u00c9LULA 76 (processar todas as imagens)\")\n",
    "    print(\"  2. Execute C\u00c9LULA 70 (busca por similaridade)\")\n",
    "    print(\"  3. Execute C\u00c9LULA 71 (an\u00e1lise contextual)\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Ainda h\u00e1 apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Fun\u00e7\u00e3o 'processar_imagens_da_pasta' criada com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# C\u00c9LULA 75 (NOVO) - \ud83d\udcc2 PROCESSAMENTO DIRETO DE IMAGENS DA PASTA\n",
    "# Fun\u00e7\u00e3o para ler todas as imagens da pasta images/ e gerar embeddings para RAG\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from multimodal_qa_with_rag_utils import (\n",
    "    get_image_embedding_from_multimodal_embedding_model,\n",
    "    get_gemini_response\n",
    ")\n",
    "def processar_imagens_da_pasta(\n",
    "    pasta_imagens=\"images/\",\n",
    "    embedding_size=512,\n",
    "    gerar_descricoes=True,\n",
    "    formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "):\n",
    "    \"\"\"\n",
    "    Processa todas as imagens de uma pasta, gerando embeddings e descri\u00e7\u00f5es para RAG\n",
    "    \n",
    "    Args:\n",
    "        pasta_imagens: Caminho da pasta com imagens\n",
    "        embedding_size: Tamanho do embedding (128, 256, 512, 1408)\n",
    "        gerar_descricoes: Se deve gerar descri\u00e7\u00f5es das imagens com Gemini\n",
    "        formatos_suportados: Lista de formatos de imagem aceitos\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame compat\u00edvel com o sistema RAG existente\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\udd0d PROCESSANDO IMAGENS DA PASTA: {pasta_imagens}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Verificar se a pasta existe\n",
    "    if not os.path.exists(pasta_imagens):\n",
    "        print(f\"\u274c Pasta '{pasta_imagens}' n\u00e3o encontrada!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Encontrar todas as imagens na pasta\n",
    "    imagens_encontradas = []\n",
    "    for formato in formatos_suportados:\n",
    "        pattern = os.path.join(pasta_imagens, f\"*{formato}\")\n",
    "        imagens_encontradas.extend(glob.glob(pattern))\n",
    "        pattern = os.path.join(pasta_imagens, f\"*{formato.upper()}\")\n",
    "        imagens_encontradas.extend(glob.glob(pattern))\n",
    "    \n",
    "    # Remover duplicatas\n",
    "    imagens_encontradas = list(set(imagens_encontradas))\n",
    "    \n",
    "    if not imagens_encontradas:\n",
    "        print(f\"\u274c Nenhuma imagem encontrada na pasta '{pasta_imagens}'\")\n",
    "        print(f\"Formatos suportados: {formatos_suportados}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"\ud83d\udcca Encontradas {len(imagens_encontradas)} imagens:\")\n",
    "    for img in imagens_encontradas:\n",
    "        print(f\"  - {os.path.basename(img)}\")\n",
    "    \n",
    "    # Lista para armazenar dados processados\n",
    "    dados_imagens = []\n",
    "    \n",
    "    # Prompt para descri\u00e7\u00e3o das imagens\n",
    "    prompt_descricao = \"\"\"Analise esta imagem detalhadamente e forne\u00e7a uma descri\u00e7\u00e3o precisa.\n",
    "    Inclua:\n",
    "    - O que voc\u00ea v\u00ea na imagem\n",
    "    - Elementos principais e detalhes importantes\n",
    "    - Texto vis\u00edvel (se houver)\n",
    "    - Tipo de imagem (mapa, diagrama, foto, etc.)\n",
    "    - Informa\u00e7\u00f5es relevantes para busca e recupera\u00e7\u00e3o\n",
    "    \n",
    "    Seja espec\u00edfico e detalhado para facilitar buscas futuras.\"\"\"\n",
    "    \n",
    "    print(f\"\\n\ud83d\ude80 PROCESSANDO CADA IMAGEM...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, caminho_imagem in enumerate(imagens_encontradas, 1):\n",
    "        nome_arquivo = os.path.basename(caminho_imagem)\n",
    "        print(f\"\\n\ud83d\udcf8 PROCESSANDO {i}/{len(imagens_encontradas)}: {nome_arquivo}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Gerar embedding da imagem\n",
    "            print(\"  \ud83d\udd04 Gerando embedding...\")\n",
    "            image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
    "                image_uri=caminho_imagem,\n",
    "                embedding_size=embedding_size,\n",
    "             model=multimodal_embedding_model,\n",
    "                return_array=True\n",
    "            )\n",
    "            print(f\"  \u2705 Embedding gerado: shape {image_embedding.shape}\")\n",
    "            \n",
    "            # 2. Gerar descri\u00e7\u00e3o da imagem (se solicitado)\n",
    "            descricao = \"\"\n",
    "            if gerar_descricoes:\n",
    "                print(\"  \ud83e\udd16 Gerando descri\u00e7\u00e3o com Gemini...\")\n",
    "                try:\n",
    "                    from vertexai.generative_models import Image as GeminiImage\n",
    "                    imagem_gemini = GeminiImage.load_from_file(caminho_imagem)\n",
    "                    \n",
    "                    descricao = get_gemini_response(\n",
    "                        multimodal_model_2_0_flash,\n",
    "                        model_input=[prompt_descricao, imagem_gemini],\n",
    "                        stream=False,\n",
    "                    )\n",
    "                    print(f\"  \u2705 Descri\u00e7\u00e3o gerada: {len(descricao)} caracteres\")\n",
    "                    \n",
    "                except Exception as desc_error:\n",
    "                    print(f\"  \u26a0\ufe0f  Erro ao gerar descri\u00e7\u00e3o: {desc_error}\")\n",
    "                    descricao = f\"Imagem: {nome_arquivo}\"\n",
    "            \n",
    "            # 3. Gerar embedding da descri\u00e7\u00e3o (para compatibilidade com RAG)\n",
    "            text_embedding = None\n",
    "            if descricao:\n",
    "                try:\n",
    "                    from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
    "                    text_embedding = get_text_embedding_from_text_embedding_model(descricao)\n",
    "                    print(\"  \u2705 Text embedding da descri\u00e7\u00e3o gerado\")\n",
    "                except Exception as text_emb_error:\n",
    "                    print(f\"  \u26a0\ufe0f  Erro ao gerar text embedding: {text_emb_error}\")\n",
    "            \n",
    "            # 4. Criar registro compat\u00edvel com o sistema existente\n",
    "            registro = {\n",
    "                'file_name': f\"pasta_images_{nome_arquivo}\",  # Nome \u00fanico\n",
    "                'page_num': 1,  # Imagens individuais = p\u00e1gina 1\n",
    "                'img_num': i,\n",
    "                'img_path': caminho_imagem,\n",
    "                'img_desc': descricao,\n",
    "                'mm_embedding_from_img_only': image_embedding.tolist(),  # Compatibilidade\n",
    "                'text_embedding_from_image_description': text_embedding if text_embedding else None,\n",
    "                'source_type': 'pasta_imagens',  # Identificar origem\n",
    "                'original_filename': nome_arquivo\n",
    "            }\n",
    "            \n",
    "            dados_imagens.append(registro)\n",
    "            print(f\"  \u2705 Processamento conclu\u00eddo para {nome_arquivo}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  \u274c Erro ao processar {nome_arquivo}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Criar DataFrame\n",
    "    if dados_imagens:\n",
    "        df_imagens = pd.DataFrame(dados_imagens)\n",
    "        print(f\"\\n\ud83c\udf89 PROCESSAMENTO CONCLU\u00cdDO!\")\n",
    "        print(f\"\ud83d\udcca DataFrame criado com {len(df_imagens)} imagens processadas\")\n",
    "        print(f\"\ud83d\udccb Colunas: {list(df_imagens.columns)}\")\n",
    "        \n",
    "        return df_imagens\n",
    "    else:\n",
    "        print(f\"\\n\u274c Nenhuma imagem foi processada com sucesso\")\n",
    "        return pd.DataFrame()\n",
    "print(\"\u2705 Fun\u00e7\u00e3o 'processar_imagens_da_pasta' criada com sucesso!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\n",
      "\n",
      "\ud83d\udd0d PROCESSANDO IMAGENS DA PASTA: images/\n",
      "============================================================\n",
      "\ud83d\udcca Encontradas 2 imagens:\n",
      "  - B2_room.jpeg\n",
      "  - A1.png\n",
      "\n",
      "\ud83d\ude80 PROCESSANDO CADA IMAGEM...\n",
      "============================================================\n",
      "\n",
      "\ud83d\udcf8 PROCESSANDO 1/2: B2_room.jpeg\n",
      "  \ud83d\udd04 Gerando embedding...\n",
      "  \u2705 Embedding gerado: shape (512,)\n",
      "  \ud83e\udd16 Gerando descri\u00e7\u00e3o com Gemini...\n",
      "  \u26a0\ufe0f  Erro ao gerar descri\u00e7\u00e3o: 'GenerationResponse' object is not iterable\n",
      "  \u2705 Text embedding da descri\u00e7\u00e3o gerado\n",
      "  \u2705 Processamento conclu\u00eddo para B2_room.jpeg\n",
      "\n",
      "\ud83d\udcf8 PROCESSANDO 2/2: A1.png\n",
      "  \ud83d\udd04 Gerando embedding...\n",
      "  \u2705 Embedding gerado: shape (512,)\n",
      "  \ud83e\udd16 Gerando descri\u00e7\u00e3o com Gemini...\n",
      "  \u26a0\ufe0f  Erro ao gerar descri\u00e7\u00e3o: 'GenerationResponse' object is not iterable\n",
      "  \u2705 Text embedding da descri\u00e7\u00e3o gerado\n",
      "  \u2705 Processamento conclu\u00eddo para A1.png\n",
      "\n",
      "\ud83c\udf89 PROCESSAMENTO CONCLU\u00cdDO!\n",
      "\ud83d\udcca DataFrame criado com 2 imagens processadas\n",
      "\ud83d\udccb Colunas: ['file_name', 'page_num', 'img_num', 'img_path', 'img_desc', 'mm_embedding_from_img_only', 'text_embedding_from_image_description', 'source_type', 'original_filename']\n",
      "\n",
      "\ud83c\udf89 SUCESSO TOTAL!\n",
      "\ud83d\udcca image_metadata_df criado com 2 imagens\n",
      "\n",
      "\ud83d\udccb RESUMO DAS IMAGENS PROCESSADAS:\n",
      "==================================================\n",
      "\n",
      "\ud83d\uddbc\ufe0f  Imagem 1:\n",
      "  \ud83d\udcc1 Arquivo: B2_room.jpeg\n",
      "  \ud83d\udcc2 Caminho: images/B2_room.jpeg\n",
      "  \ud83d\udcca Embedding shape: 512\n",
      "  \ud83d\udcdd Descri\u00e7\u00e3o: Imagem: B2_room.jpeg\n",
      "\n",
      "\ud83d\uddbc\ufe0f  Imagem 2:\n",
      "  \ud83d\udcc1 Arquivo: A1.png\n",
      "  \ud83d\udcc2 Caminho: images/A1.png\n",
      "  \ud83d\udcca Embedding shape: 512\n",
      "  \ud83d\udcdd Descri\u00e7\u00e3o: Imagem: A1.png\n",
      "\n",
      "\u2705 COMPATIBILIDADE COM SISTEMA RAG:\n",
      "  \u2705 img_path: OK\n",
      "  \u2705 mm_embedding_from_img_only: OK\n",
      "  \u2705 img_desc: OK\n",
      "  \u2705 file_name: OK\n",
      "  \u2705 page_num: OK\n",
      "\n",
      "\ud83d\udcbe DataFrame salvo em 'image_metadata_from_folder.pkl'\n",
      "\n",
      "\ud83d\ude80 PR\u00d3XIMOS PASSOS:\n",
      "1. Agora voc\u00ea pode executar a C\u00c9LULA 70 (Valida\u00e7\u00e3o)\n",
      "2. Depois executar a C\u00c9LULA 71 (An\u00e1lise Contextual)\n",
      "3. O sistema RAG est\u00e1 pronto para perguntas sobre as imagens!\n"
     ]
    }
   ],
   "source": [
    "# C\u00c9LULA 76 (EXECUTAR) - \ud83d\ude80 PROCESSAMENTO DAS IMAGENS DA PASTA images/\n",
    "# Executa o processamento de todas as imagens e cria o image_metadata_df\n",
    "\n",
    "print(\"=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\\n\")\n",
    "\n",
    "# Executar o processamento das imagens\n",
    "try:\n",
    "    image_metadata_df = processar_imagens_da_pasta(\n",
    "        pasta_imagens=\"images/\",\n",
    "        embedding_size=512,\n",
    "        gerar_descricoes=True,  # Gerar descri\u00e7\u00f5es detalhadas com Gemini\n",
    "        formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
    "    )\n",
    "    \n",
    "    if not image_metadata_df.empty:\n",
    "        print(f\"\\n\ud83c\udf89 SUCESSO TOTAL!\")\n",
    "        print(f\"\ud83d\udcca image_metadata_df criado com {len(image_metadata_df)} imagens\")\n",
    "        \n",
    "        # Mostrar resumo das imagens processadas\n",
    "        print(f\"\\n\ud83d\udccb RESUMO DAS IMAGENS PROCESSADAS:\")\n",
    "        print(\"=\"*50)\n",
    "        for idx, row in image_metadata_df.iterrows():\n",
    "            print(f\"\\n\ud83d\uddbc\ufe0f  Imagem {idx + 1}:\")\n",
    "            print(f\"  \ud83d\udcc1 Arquivo: {row['original_filename']}\")\n",
    "            print(f\"  \ud83d\udcc2 Caminho: {row['img_path']}\")\n",
    "            print(f\"  \ud83d\udcca Embedding shape: {len(row['mm_embedding_from_img_only'])}\")\n",
    "            \n",
    "            # Mostrar in\u00edcio da descri\u00e7\u00e3o\n",
    "            desc = row['img_desc']\n",
    "            if desc and len(desc) > 10:\n",
    "                print(f\"  \ud83d\udcdd Descri\u00e7\u00e3o: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
    "        \n",
    "        # Verificar compatibilidade com sistema RAG existente\n",
    "        print(f\"\\n\u2705 COMPATIBILIDADE COM SISTEMA RAG:\")\n",
    "        colunas_necessarias = ['img_path', 'mm_embedding_from_img_only', 'img_desc', 'file_name', 'page_num']\n",
    "        for col in colunas_necessarias:\n",
    "            if col in image_metadata_df.columns:\n",
    "                print(f\"  \u2705 {col}: OK\")\n",
    "            else:\n",
    "                print(f\"  \u274c {col}: FALTANDO\")\n",
    "        \n",
    "        # Salvar para uso futuro (opcional)\n",
    "        try:\n",
    "            image_metadata_df.to_pickle(\"image_metadata_from_folder.pkl\")\n",
    "            print(f\"\\n\ud83d\udcbe DataFrame salvo em 'image_metadata_from_folder.pkl'\")\n",
    "        except Exception as save_error:\n",
    "            print(f\"\\n\u26a0\ufe0f  N\u00e3o foi poss\u00edvel salvar: {save_error}\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\ude80 PR\u00d3XIMOS PASSOS:\")\n",
    "        print(f\"1. Agora voc\u00ea pode executar a C\u00c9LULA 70 (Valida\u00e7\u00e3o)\")\n",
    "        print(f\"2. Depois executar a C\u00c9LULA 71 (An\u00e1lise Contextual)\")\n",
    "        print(f\"3. O sistema RAG est\u00e1 pronto para perguntas sobre as imagens!\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n\u274c FALHA: Nenhuma imagem foi processada\")\n",
    "        print(f\"Verifique se:\")\n",
    "        print(f\"- A pasta 'images/' existe\")\n",
    "        print(f\"- H\u00e1 imagens v\u00e1lidas na pasta\")\n",
    "        print(f\"- Os modelos est\u00e3o carregados corretamente\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\u274c ERRO NO PROCESSAMENTO: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\n\ud83d\udca1 POSS\u00cdVEIS SOLU\u00c7\u00d5ES:\")\n",
    "    print(f\"- Verifique se os modelos est\u00e3o carregados\")\n",
    "    print(f\"- Verifique se a pasta 'images/' existe\")\n",
    "    print(f\"- Execute as c\u00e9lulas de setup dos modelos primeiro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resposta: Nenhuma das imagens representa um mapa da Am\u00e9rica do Sul. As imagens mostram plantas baixas de edif\u00edcios.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Este c\u00f3digo permite que voc\u00ea fa\u00e7a perguntas sobre as imagens usando o embedding e o Gemini\n",
    "\n",
    "import numpy as np\n",
    "from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
    "\n",
    "def responder_pergunta_sobre_imagem(pergunta, image_metadata_df, text_embedding_model, gemini_model, top_k=3):\n",
    "    \"\"\"\n",
    "    Dado uma pergunta, retorna a resposta do Gemini baseada nas imagens mais relevantes.\n",
    "    \n",
    "    Args:\n",
    "        pergunta: Texto da pergunta\n",
    "        image_metadata_df: DataFrame com metadados das imagens (deve ter coluna 'text_embedding_from_image_description')\n",
    "        text_embedding_model: Modelo de embedding de texto (TextEmbeddingModel)\n",
    "        gemini_model: Modelo Gemini para gerar respostas\n",
    "        top_k: N\u00famero de imagens mais relevantes para usar no contexto\n",
    "    \"\"\"\n",
    "\n",
    "    # Gere o embedding da pergunta usando o modelo de texto\n",
    "    pergunta_embedding = get_text_embedding_from_text_embedding_model(pergunta, return_array=True)\n",
    "    pergunta_embedding = np.array(pergunta_embedding)\n",
    "\n",
    "    # Calcule similaridades usando os embeddings de texto das descri\u00e7\u00f5es das imagens\n",
    "    # Primeiro, precisamos converter os embeddings de texto das descri\u00e7\u00f5es para arrays\n",
    "    text_embeddings_list = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in image_metadata_df.iterrows():\n",
    "        text_emb = row.get('text_embedding_from_image_description')\n",
    "        if text_emb is not None:\n",
    "            if isinstance(text_emb, list):\n",
    "                text_embeddings_list.append(np.array(text_emb))\n",
    "                valid_indices.append(idx)\n",
    "            elif isinstance(text_emb, np.ndarray):\n",
    "                text_embeddings_list.append(text_emb)\n",
    "                valid_indices.append(idx)\n",
    "    \n",
    "    if not text_embeddings_list:\n",
    "        return \"\u274c Erro: Nenhum embedding de texto encontrado nas descri\u00e7\u00f5es das imagens. Execute a C\u00c9LULA 76 primeiro para processar as imagens.\"\n",
    "    \n",
    "    # Calcular similaridades\n",
    "    text_embeddings = np.stack(text_embeddings_list)\n",
    "    similarities = np.dot(text_embeddings, pergunta_embedding) / (\n",
    "        np.linalg.norm(text_embeddings, axis=1) * np.linalg.norm(pergunta_embedding) + 1e-8\n",
    "    )\n",
    "    top_indices_local = np.argsort(similarities)[-top_k:][::-1]  # \u00edndices locais\n",
    "    top_indices = [valid_indices[i] for i in top_indices_local]  # \u00edndices originais do DataFrame\n",
    "\n",
    "    # Monte o contexto para o Gemini\n",
    "    contexto_imgs = []\n",
    "    imagens_para_gemini = []\n",
    "    for idx in top_indices:\n",
    "        row = image_metadata_df.iloc[idx]\n",
    "        contexto_imgs.append(\n",
    "            f\"Arquivo: {row['file_name']} (p\u00e1gina {row['page_num']}), descri\u00e7\u00e3o: {row['img_desc']}\"\n",
    "        )\n",
    "        # Carregar a imagem para incluir no contexto visual\n",
    "        try:\n",
    "            from vertexai.generative_models import Image as GeminiImage\n",
    "            img_obj = GeminiImage.load_from_file(row['img_path'])\n",
    "            imagens_para_gemini.append(img_obj)\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f  N\u00e3o foi poss\u00edvel carregar imagem {row['img_path']}: {e}\")\n",
    "    \n",
    "    contexto = \"\\n\".join(contexto_imgs)\n",
    "    prompt = (\n",
    "        \"Considere as seguintes descri\u00e7\u00f5es de imagens extra\u00eddas e responda a pergunta:\"\n",
    "        f\"\\n\\n{contexto}\\n\\nPergunta: {pergunta}\\nResposta:\"\n",
    "    )\n",
    "\n",
    "    # Chame o Gemini com texto e imagens\n",
    "    try:\n",
    "        if imagens_para_gemini:\n",
    "            # Incluir imagens no contexto\n",
    "            model_input = [prompt] + imagens_para_gemini\n",
    "        else:\n",
    "            model_input = [prompt]\n",
    "        \n",
    "        resposta = gemini_model.generate_content(model_input)\n",
    "        # Pode ser \"resposta.text\" ou apenas \"resposta\" dependendo do SDK\n",
    "        if hasattr(resposta, 'text'):\n",
    "            return resposta.text\n",
    "        else:\n",
    "            return str(resposta)\n",
    "    except Exception as e:\n",
    "        return f\"\u274c Erro ao gerar resposta: {e}\"\n",
    "\n",
    "# Exemplo de uso (corrigido para usar text_embedding_model):\n",
    "resposta = responder_pergunta_sobre_imagem(\"Qual mapa mostra a Am\u00e9rica do Sul?\", image_metadata_df, text_embedding_model, multimodal_model_2_0_flash)\n",
    "print(\"Resposta:\", resposta)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}