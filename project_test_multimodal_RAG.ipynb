{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsTUvKjwHBW"
      },
      "source": [
        "# Building a DIY Multimodal Question Answering System with Vertex AI (A Beginner's Guide - Multimodal RAG)\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fqa-ops%2Fbuilding_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/qa-ops/building_DIY_multimodal_qa_system_with_mRAG.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>    \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YR65ni7TRNYG"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Lavi Nigam](https://github.com/lavinigam-gcp) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "\n",
        "This guide is your hands-on introduction to creating a question answering system that understands both text and images. We'll build this system from the ground up using Google's Vertex AI, giving you a clear understanding of how it works without relying on complex third-party tools.\n",
        "\n",
        "\n",
        "## Why Build It Yourself?\n",
        "\n",
        "Large Language Models (LLMs) are powerful, but they can seem like a \"black box\". By building our own system, we'll break open that box and explore the core concepts. This will give you the knowledge to customize and optimize every aspect of your question answering system, whether you ultimately choose to code everything yourself or use external libraries.\n",
        "\n",
        "\n",
        "## What We'll Do:\n",
        "\n",
        "* **Focus on Fundamentals**: We'll start with the essential design pattern of \"Retrieval Augmented Generation\" (RAG) – a way to find and use relevant information to answer questions.\n",
        "\n",
        "* **Work with Text and Images**: We'll expand RAG to handle both text and images found in PDF documents. Future guides in this series will explore even more types of data, like videos and audio.\n",
        "\n",
        "* **Use Vertex AI**: We'll only use Google's Vertex AI Embeddings API and Gemini API, ensuring you have complete control and understanding of the building blocks.\n",
        "\n",
        "\n",
        "By the end of this guide, you'll have a solid foundation in building multimodal question answering systems, empowering you to create smarter applications that can understand and respond to a wider range of information.\n",
        "\n",
        "\n",
        "### Gemini\n",
        "\n",
        "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 2.0 & Gemini 1.5 models.\n",
        "\n",
        "### Comparing text-based and multimodal RAG\n",
        "\n",
        "Multimodal RAG offers several advantages over text-based RAG:\n",
        "\n",
        "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
        "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
        "\n",
        "This notebook shows you how to implement DIY RAG with Vertex AI Gemini API\n",
        " and Vertex AI Embeddings API; [text embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/text-embeddings), and [multimodal embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings), to build a document search engine.\n",
        "\n",
        "Through hands-on examples, you will discover how to construct a multimedia-rich metadata repository of your document sources, enabling search, comparison, and reasoning across diverse information streams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "This notebook provides a guide to building a document search engine using multimodal retrieval augmented generation (RAG), step by step:\n",
        "\n",
        "1. Extract and store metadata of documents containing both text and images, and generate embeddings the documents\n",
        "2. Search the metadata with text queries to find similar text or images\n",
        "3. Search the metadata with image queries to find similar images\n",
        "4. Using a text query as input, search for contextual answers using both text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpYxfesh2rI"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXJpXzKrh2rJ"
      },
      "source": [
        "## Getting Started\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc4WxYmLSBW5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip3 install --upgrade --user google-cloud-aiplatform pymupdf rich"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "tags": []
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'IPython' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Restart kernel after installs so that your environment can access the new packagesimport IPython\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m app = \u001b[43mIPython\u001b[49m.Application.instance()                                                                                                                                        \n\u001b[32m      5\u001b[39m app.kernel.do_shutdown(\u001b[38;5;28;01mTrue\u001b[39;00m)                                                                                                                                                                                                                                                                                                                                                                                \n",
            "\u001b[31mNameError\u001b[39m: name 'IPython' is not defined"
          ]
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packagesimport IPython\n",
        "import time\n",
        "\n",
        "app = IPython.Application.instance()                                                                                                                                        \n",
        "app.kernel.do_shutdown(True)                                                                                                                                                                                                                                                                                                                                                                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtsU9Bw9h2rL"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GpYEyLsOh2rL",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vKZZoEh2rL"
      },
      "source": [
        "### Define Google Cloud project information\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqZ76rJh2rM",
        "outputId": "c7648628-a72f-467b-bca4-b7fb003885fe",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your active configuration is: [personal]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your project ID is: gen-lang-client-0303567819\n"
          ]
        }
      ],
      "source": [
        "# Define project information\n",
        "\n",
        "import sys\n",
        "\n",
        "PROJECT_ID = \"gen-lang-client-0303567819\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "D48gUW5-h2rM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rtMowvm-yQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rich import print as rich_print\n",
        "from rich.markdown import Markdown as rich_Markdown\n",
        "from IPython.display import Markdown, display\n",
        "from vertexai.generative_models import (\n",
        "    Content,\n",
        "    GenerationConfig,\n",
        "    GenerationResponse,\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    Image,\n",
        "    Part,\n",
        ")\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from vertexai.vision_models import MultiModalEmbeddingModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-TX_R_xh2rM"
      },
      "source": [
        "### Load the Gemini 2.0 Flash , Gemini 1.5 Flash and Gemini 1.5 Pro Flash\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HySgZekYzCpW"
      },
      "source": [
        "Learn more about each models and their differences: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/send-multimodal-prompts)\n",
        "\n",
        "Learn about the quotas: [here](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPo-OPH9Bcsc"
      },
      "source": [
        "## Overview\n",
        "\n",
        "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
        "\n",
        "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
        "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
        "</a>\n",
        "\n",
        "[Gemini 2.0 Flash](https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2) is a new multimodal generative ai model from the Gemini family developed by [Google DeepMind](https://deepmind.google/). It is available through the Gemini API in Vertex AI and Vertex AI Studio. The model introduces new features and enhanced core capabilities:\n",
        "\n",
        "- Multimodal Live API: This new API helps you create real-time vision and audio streaming applications with tool use.\n",
        "- Speed and performance: Gemini 2.0 Flash is the fastest model in the industry, with a 3x improvement in time to first token (TTFT) over 1.5 Flash.\n",
        "- Quality: The model maintains quality comparable to larger models like Gemini 1.5 Pro and GPT-4o.\n",
        "- Improved agentic experiences: Gemini 2.0 delivers improvements to multimodal understanding, coding, complex instruction following, and function calling.\n",
        "- New Modalities: Gemini 2.0 introduces native image generation and controllable text-to-speech capabilities, enabling image editing, localized artwork creation, and expressive storytelling.\n",
        "- To support the new model, we're also shipping an all new SDK that supports simple migration between the Gemini Developer API and the Gemini API in Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SvMwSRJJh2rM",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759254526.859879   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1759254527.995631   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "\n",
        "multimodal_model_2_0_flash = GenerativeModel(\n",
        "    \"gemini-2.0-flash-001\"\n",
        ") # Gemini latest Gemini 2.0 Flash Model\n",
        "\n",
        "multimodal_model_15 = GenerativeModel(\n",
        "    \"gemini-1.5-pro-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
        "\n",
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "multimodal_model_15_flash = GenerativeModel(\n",
        "    \"gemini-1.5-flash-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
        "\n",
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
        "\n",
        "# Load multimodal embedding model from pre-trained source\n",
        "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
        "    \"multimodalembedding@001\"\n",
        ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7bKCQMFT7JT"
      },
      "source": [
        "#### Get documents and images from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwbL89zcY39N",
        "outputId": "c918f449-e9b6-4cbe-c70e-6fd5ef4c5ddc",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# download documents and images used in this notebook - will take ~30 sec\n",
        "!gsutil -m -q rsync -r gs://github-repo/rag/intro_multimodal_rag/intro_multimodal_rag_v2 .\n",
        "print(\"Download completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1G-cCfpibN"
      },
      "source": [
        "## Building metadata of documents containing text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7uv_PVR1T6B"
      },
      "source": [
        "### The data\n",
        "\n",
        "The source data that you will use in this notebook are:\n",
        "\n",
        "\n",
        "* [Google Cloud TPU Scaling blog](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/Google%20Cloud%20TPU%20blog.pdf)\n",
        "* [Gemini 1.5 Technical Report](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/gemini_v1_5_report_technical.pdf)\n",
        "* [Google Gemma Technical Paper](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/gemma_technical_paper.pdf)\n",
        "* [Med-Gemini Technical Paper](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/data/med_gemini.pdf)\n",
        "\n",
        "\n",
        "\n",
        "You can also use your data, by first deleting the current files and then placing your files in the `data/` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvt0sus5KSNX"
      },
      "source": [
        "### Import helper functions to build metadata\n",
        "\n",
        "Before building the Multimodal Question Answering System with Vertex AI, it's important to have metadata of all the text and images in the document. For references and citations purposes, the metadata should contain essential elements, including page number, file name, image counter, and so on. Hence, as a next step, you will generate embeddings from the metadata, which is required to perform similarity search when querying the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tStqXX32RNYK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_document_metadata,\n",
        "    set_global_variable,\n",
        ")\n",
        "\n",
        "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
        "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtaDuBXhFmkL"
      },
      "source": [
        " You can also view the code (`multimodal_qa_with_rag_utils`) [directly](https://storage.googleapis.com/github-repo/rag/intro_multimodal_rag/utils/multimodal_qa_with_rag_utils.py)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BOAkYN0KlSL"
      },
      "source": [
        "### Extract and store metadata of text and images from a document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9hBPPWs5CMd"
      },
      "source": [
        "You just imported a function called `get_document_metadata()`. This function extracts text and image metadata from a document, and returns two dataframes, namely *text_metadata* and *image_metadata*, as outputs. If you want to find out more about how `get_document_metadata()` function is implemented using Gemini and the embedding models, you can take look at the [source code](https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/utils/intro_multimodal_rag_utils.py) directly.\n",
        "\n",
        "The reason for extraction and storing both text metadata and image metadata is that just by using either of the two alone is not sufficient to come out with a relevent answer. For example, the relevant answers could be in visual form within a document, but text-based RAG won't be able to take into consideration of the visual images. You will also be exploring this example later in this notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnKru0sBh2rN"
      },
      "source": [
        "At the next step, you will use the function to extract and store metadata of text and images froma document. Please note that the following cell may take a few minutes to complete:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8h0XSG_7e5M"
      },
      "source": [
        "**NOTE: Given that we are loading 4 files with roughly 200 pages and approximately 84 images, the cell below will take approximately 7 minutes to run. We recommend loading pre-computed metadata instead.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8hE0tWD-lf8",
        "outputId": "86a01c45-d8eb-4911-8ba4-9fb0dcf89278",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing pre-exsisting images folder, since you are running the logic from scratch\n",
            "\n",
            "\n",
            " Processing the file: --------------------------------- map/map.pdf \n",
            "\n",
            "\n",
            "Processing page: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759254557.883849   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759254557.889838   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " \n",
            " Sleeping for  5  sec before processing the next document to avoid quota issues. You can disable it: \"add_sleep_after_document = False\"  \n",
            "\n",
            "\n",
            " --- Completed processing. ---\n",
            "CPU times: user 229 ms, sys: 77.1 ms, total: 306 ms\n",
            "Wall time: 8.29 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Specify the PDF folder with multiple PDF ~7m\n",
        "\n",
        "print(\"Removing pre-exsisting images folder, since you are running the logic from scratch\")\n",
        "! rm -rf images/\n",
        "\n",
        "pdf_folder_path = \"map/\"  # if running in Vertex AI Workbench.\n",
        "\n",
        "# Specify the image description prompt. Change it\n",
        "# image_description_prompt = \"\"\"Explain what is going on in the image.\n",
        "# If it's a table, extract all elements of the table.\n",
        "# If it's a graph, explain the findings in the graph.\n",
        "# Do not include any numbers that are not mentioned in the image.\n",
        "# \"\"\"\n",
        "\n",
        "image_description_prompt = \"\"\"You are a technical image analysis expert. You will be provided with various types of images extracted from documents like research papers, technical blogs, and more.\n",
        "Your task is to generate concise, accurate descriptions of the images without adding any information you are not confident about.\n",
        "Focus on capturing the key details, trends, or relationships depicted in the image.\n",
        "\n",
        "Important Guidelines:\n",
        "* Prioritize accuracy:  If you are uncertain about any detail, state \"Unknown\" or \"Not visible\" instead of guessing.\n",
        "* Avoid hallucinations: Do not add information that is not directly supported by the image.\n",
        "* Be specific: Use precise language to describe shapes, colors, textures, and any interactions depicted.\n",
        "* Consider context: If the image is a screenshot or contains text, incorporate that information into your description.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Extract text and image metadata from the PDF document\n",
        "text_metadata_df, image_metadata_df = get_document_metadata(\n",
        "    multimodal_model_2_0_flash,  # we are passing gemini 2.0 Flash Model\n",
        "    pdf_folder_path,\n",
        "    image_save_dir=\"images\",\n",
        "    image_description_prompt=image_description_prompt,\n",
        "    embedding_size=1408,\n",
        "    # add_sleep_after_page = True, # Uncomment this if you are running into API quota issues\n",
        "    # sleep_time_after_page = 5,\n",
        "    add_sleep_after_document=True,  # Uncomment this if you are running into API quota issues\n",
        "    sleep_time_after_document=5,    # Increase the value in seconds, if you are still getting quota issues. It will slow down the processing.\n",
        "    # generation_config = # see next cell\n",
        "    # safety_settings =  # see next cell\n",
        ")\n",
        "\n",
        "print(\"\\n\\n --- Completed processing. ---\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWtOx1wb86Az"
      },
      "source": [
        "If you would like to pass additional parameters to Gemini while building metadata, here are some options:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlNK0o2DRNYK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Parameters for Gemini API call.\n",
        "# # reference for parameters: https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini\n",
        "\n",
        "# generation_config=  GenerationConfig(temperature=0.2, max_output_tokens=2048)\n",
        "\n",
        "# # Set the safety settings if Gemini is blocking your content or you are facing \"ValueError(\"Content has no parts\")\" error or \"Exception occured\" in your data.\n",
        "# # ref for settings and thresholds: https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes\n",
        "\n",
        "# safety_settings = {\n",
        "#                   HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "#                   }\n",
        "\n",
        "# # You can also pass parameters and safety_setting to \"get_gemini_response\" function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW3Ci1IL8wSW"
      },
      "source": [
        "### Load pre-computed metadata of text and images from source document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c10dCy6Cig3H"
      },
      "source": [
        "**If you are facing constant issues with Quota or want to focus on the outputs, you should load pre-computed metadata.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1AGYOYb0In7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # [Optional]\n",
        "# import pickle\n",
        "\n",
        "# # Load the pickle file\n",
        "# with open(\"mrag_metadata.pkl\", \"rb\") as f:\n",
        "#     data = pickle.load(f)\n",
        "\n",
        "# # Extract the DataFrames\n",
        "# text_metadata_df = data[\"text_metadata\"]\n",
        "# image_metadata_df = data[\"image_metadata\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miBBoEXwh2rN"
      },
      "source": [
        "#### Inspect the processed text metadata\n",
        "\n",
        "\n",
        "The following cell will produce a metadata table which describes the different parts of text metadata, including:\n",
        "\n",
        "- **text**: the original text from the page\n",
        "- **text_embedding_page**: the embedding of the original text from the page\n",
        "- **chunk_text**: the original text divided into smaller chunks\n",
        "- **chunk_number**: the index of each text chunk\n",
        "- **text_embedding_chunk**: the embedding of each text chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "6t3AIGFar8Mo",
        "outputId": "0a3eb7ac-5eed-4767-9c97-86d0f10d45dc",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>page_num</th>\n",
              "      <th>text</th>\n",
              "      <th>text_embedding_page</th>\n",
              "      <th>chunk_number</th>\n",
              "      <th>chunk_text</th>\n",
              "      <th>text_embedding_chunk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>map.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...</td>\n",
              "      <td>[-0.00932532362639904, -0.033891454339027405, ...</td>\n",
              "      <td>1</td>\n",
              "      <td>I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...</td>\n",
              "      <td>[-0.017712244763970375, -0.032180916517972946,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>map.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...</td>\n",
              "      <td>[-0.00932532362639904, -0.033891454339027405, ...</td>\n",
              "      <td>2</td>\n",
              "      <td>Housing Registry (F1002)\\nResidence Office (R...</td>\n",
              "      <td>[0.022167837247252464, -0.03107241913676262, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>map.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...</td>\n",
              "      <td>[-0.00932532362639904, -0.033891454339027405, ...</td>\n",
              "      <td>3</td>\n",
              "      <td>\\nHealth &amp; Safety/Facilities (D1021)\\nHuman Re...</td>\n",
              "      <td>[0.00795393344014883, -0.03503992408514023, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>map.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...</td>\n",
              "      <td>[-0.00932532362639904, -0.033891454339027405, ...</td>\n",
              "      <td>4</td>\n",
              "      <td>dle Building\\nat 137 Dundas Street.\\nTo Fansha...</td>\n",
              "      <td>[0.006327552255243063, -0.07151074707508087, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>map.pdf</td>\n",
              "      <td>1</td>\n",
              "      <td>I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...</td>\n",
              "      <td>[-0.00932532362639904, -0.033891454339027405, ...</td>\n",
              "      <td>5</td>\n",
              "      <td>a\\nThe Junction\\nBookstore\\nCareer Services\\nC...</td>\n",
              "      <td>[0.011586909182369709, -0.03213056921958923, 0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  file_name  page_num                                               text  \\\n",
              "0   map.pdf         1  I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...   \n",
              "1   map.pdf         1  I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...   \n",
              "2   map.pdf         1  I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...   \n",
              "3   map.pdf         1  I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...   \n",
              "4   map.pdf         1  I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...   \n",
              "\n",
              "                                 text_embedding_page  chunk_number  \\\n",
              "0  [-0.00932532362639904, -0.033891454339027405, ...             1   \n",
              "1  [-0.00932532362639904, -0.033891454339027405, ...             2   \n",
              "2  [-0.00932532362639904, -0.033891454339027405, ...             3   \n",
              "3  [-0.00932532362639904, -0.033891454339027405, ...             4   \n",
              "4  [-0.00932532362639904, -0.033891454339027405, ...             5   \n",
              "\n",
              "                                          chunk_text  \\\n",
              "0  I \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n   ...   \n",
              "1   Housing Registry (F1002)\\nResidence Office (R...   \n",
              "2  \\nHealth & Safety/Facilities (D1021)\\nHuman Re...   \n",
              "3  dle Building\\nat 137 Dundas Street.\\nTo Fansha...   \n",
              "4  a\\nThe Junction\\nBookstore\\nCareer Services\\nC...   \n",
              "\n",
              "                                text_embedding_chunk  \n",
              "0  [-0.017712244763970375, -0.032180916517972946,...  \n",
              "1  [0.022167837247252464, -0.03107241913676262, 0...  \n",
              "2  [0.00795393344014883, -0.03503992408514023, 0....  \n",
              "3  [0.006327552255243063, -0.07151074707508087, 0...  \n",
              "4  [0.011586909182369709, -0.03213056921958923, 0...  "
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjIQYI3mh2rO"
      },
      "source": [
        "#### Inspect the processed image metadata\n",
        "\n",
        "The following cell will produce a metadata table which describes the different parts of image metadata, including:\n",
        "* **img_desc**: Gemini-generated textual description of the image.\n",
        "* **mm_embedding_from_text_desc_and_img**: Combined embedding of image and its description, capturing both visual and textual information.\n",
        "* **mm_embedding_from_img_only**: Image embedding without description, for comparison with description-based analysis.\n",
        "* **text_embedding_from_image_description**: Separate text embedding of the generated description, enabling textual analysis and comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "tkHtAYIK-y-q",
        "outputId": "225c6236-7736-43e4-b3cb-80ac1ac6d3a4",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_metadata_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBhoOkutUtPr"
      },
      "source": [
        "### Import the helper functions to implement RAG\n",
        "\n",
        "You will be importing the following functions which will be used in the remainder of this notebook to implement RAG:\n",
        "\n",
        "* **get_similar_text_from_query():** Given a text query, finds text from the document which are relevant, using cosine similarity algorithm. It uses text embeddings from the metadata to compute and the results can be filtered by top score, page/chunk number, or embedding size.\n",
        "* **print_text_to_text_citation():** Prints the source (citation) and details of the retrieved text from the `get_similar_text_from_query()` function.\n",
        "* **get_similar_image_from_query():** Given an image path or an image, finds images from the document which are relevant. It uses image embeddings from the metadata.\n",
        "* **print_text_to_image_citation():** Prints the source (citation) and the details of retrieved images from the `get_similar_image_from_query()` function.\n",
        "* **get_gemini_response():** Interacts with a Gemini model to answer questions based on a combination of text and image inputs.\n",
        "* **display_images():**  Displays a series of images provided as paths or PIL Image objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tngn_vrIKdE1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_similar_text_from_query,\n",
        "    print_text_to_text_citation,\n",
        "    get_similar_image_from_query,\n",
        "    print_text_to_image_citation,\n",
        "    get_gemini_response,\n",
        "    display_images,\n",
        "    get_answer_from_qa_system,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9jGEj6DY1Rj"
      },
      "source": [
        "Before implementing a Multimodal Question Answering System with Vertex AI, let's explore what you can achieve with just text or image embeddings. This will set the foundation for implementing a multimodal Retrieval Augmented Generation (RAG) system, which you will do later in this notebook.\n",
        "\n",
        "You can also use these essential elements together to build applications for multimodal use cases, extracting meaningful information from documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHuLlEvSKFWt"
      },
      "source": [
        "## Text Search\n",
        "\n",
        "Let's start the search with a simple question and see if the simple text search using text embeddings can answer it. The expected answer is to show the value of basic and diluted net income per share of Google for different share types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What are the buildings in the map, give letters and names of them?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"List the buildings in the map, give letters and names of them? And in the count of them\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5mrFVhtCut7t",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#query = \"What are various med-gemini medical benchmarks that shows its performance relative to other models?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWw7-AIar-S8"
      },
      "source": [
        "### Search similar text with text query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEzP6Yyv7N-G",
        "outputId": "2b9c957d-6045-401d-8993-26f1ec25eafe",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mCitation 1: Matched text: \n",
            "\u001b[0m\n",
            "\u001b[94mscore: \u001b[0m 0.54\n",
            "\u001b[94mfile_name: \u001b[0m map.pdf\n",
            "\u001b[94mpage_number: \u001b[0m 1\n",
            "\u001b[94mchunk_number: \u001b[0m 5\n",
            "\u001b[94mchunk_text: \u001b[0m a\n",
            "The Junction\n",
            "Bookstore\n",
            "Career Services\n",
            "Co-operative\n",
            "Education\n",
            "D1063\n",
            "Saffrons\n",
            "Restaurant\n",
            "Olive\n",
            "Oyle's\n",
            "Deli\n",
            "Parking Office\n",
            "D1018\n",
            "Security\n",
            "Control Centre\n",
            "Print\n",
            "Shop\n",
            "B Food\n",
            "Court\n",
            "JamesA. Colvin\n",
            "Atrium\n",
            "(Cafeteria)\n",
            "Receiving\n",
            "After Hours Entrance\n",
            "H Cafeteria\n",
            "Siskind Gallery\n",
            "Falcon House\n",
            "Residence\n",
            "Merlin House\n",
            "Residence\n",
            "K\n",
            "R1\n",
            "R3\n",
            "Student Centre\n",
            "SC\n",
            "Peregrine House\n",
            "Residence\n",
            "R2\n",
            "Gym 1 & 2\n",
            "Gym 3\n",
            "J\n",
            "Student Union Building\n",
            "SUB\n",
            "Library\n",
            "M\n",
            "F\n",
            "C\n",
            "N\n",
            "L\n",
            "D\n",
            "G\n",
            "H\n",
            "E\n",
            "Fanshawe Aviation Centre\n",
            "Y\n",
            "Kestrel Court\n",
            "Residence\n",
            "R4\n",
            "Z\n",
            "Centre for\n",
            "Applied\n",
            "Transportation \n",
            "Technology \n",
            "Spriet Family\n",
            "Greenhouse\n",
            "B\n",
            "T\n",
            "A\n",
            "1764 Oxford Street East \n",
            "Welcome\n",
            "Kiosk\n",
            "F1008\n",
            "Lot 7\n",
            "Residence Parking\n",
            "P\n",
            "P 7 Meters\n",
            "P\n",
            "P 13\n",
            "Meters\n",
            "P P 5 Meters\n",
            "P\n",
            "Lot 8\n",
            "General Parking\n",
            "P\n",
            "P 6\n",
            "Meters\n",
            "P\n",
            "P 8\n",
            "Meters\n",
            "Lot 30\n",
            "Reserved Parking\n",
            "P\n",
            "Lot 32\n",
            "General Parking\n",
            "P\n",
            "P P 4 Meters\n",
            "Lot 9\n",
            "Assigned\n",
            "Parking \n",
            "P\n",
            "P 3 Meters\n",
            "Lot 15\n",
            "Residence Parking\n",
            "Lot 10\n",
            "Reserved Parking\n",
            "M\n",
            "Lot 11\n",
            "Assigned\n",
            "Parking\n",
            "P\n",
            "Lot 20\n",
            "Reserved\n",
            "Parking\n",
            "P\n",
            "Lot 21\n",
            "Assigned\n",
            "Parking\n",
            "P\n",
            "P 23\n",
            "Meters\n",
            "M\n",
            "Motorcycle Parking\n",
            "Vi\n"
          ]
        }
      ],
      "source": [
        "# Matching user text query with \"chunk_embedding\" to find relevant chunks.\n",
        "matching_results_text = get_similar_text_from_query(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    column_name=\"text_embedding_chunk\",\n",
        "    top_n=3,\n",
        "    chunk_text=True,\n",
        ")\n",
        "\n",
        "# Print the matched text citations\n",
        "print_text_to_text_citation(\n",
        "    matching_results_text, print_top=True, chunk_text=True\n",
        ")  # print_top=False to see all text matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY1J2sHr-N8f"
      },
      "source": [
        "### Get answer with text-RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ORCistIdDWoE",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# All relevant text chunk found across documents based on user query\n",
        "context = \"\\n\".join(\n",
        "    [value[\"chunk_text\"] for key, value in matching_results_text.items()]\n",
        ")\n",
        "\n",
        "prompt = f\"\"\"Answer the question with the given context. If the specific answer is not in the context, please answer \"I don't know\".\n",
        "Question: {query}\n",
        "Context: {context}\n",
        "Answer:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GKD-Ew8IM287",
        "tags": []
      },
      "outputs": [],
      "source": [
        "safety_settings = {\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "m8jyc5_SAwlF",
        "outputId": "def58273-e032-4263-b6ae-52834791ae03",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759254627.310296   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 71.1 ms, sys: 19.6 ms, total: 90.7 ms\n",
            "Wall time: 2.43 s\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Based on the context provided, here are the buildings with their letters and names, and the count:                 \n",
              "\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">A:</span> Welcome Kiosk                                                                                                \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">B:</span> Food Court, James A. Colvin Atrium (Cafeteria)                                                               \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">C:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">D:</span> The Junction Bookstore, Career Services, Co-operative Education, Saffrons Restaurant, Olive Oyle's Deli,     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Parking Office                                                                                                  \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">E:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">F:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">G:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">H:</span> Cafeteria, Siskind Gallery                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">J:</span> Gym 1 &amp; 2, Gym 3                                                                                             \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">K:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">L:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">M:</span> Library                                                                                                      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">N:</span> I don't know                                                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">SC:</span> Student Centre                                                                                              \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">SUB:</span> Student Union Building                                                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Y:</span> Fanshawe Aviation Centre                                                                                     \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">Z:</span> Centre for Applied Transportation Technology                                                                 \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">R1:</span> Falcon House Residence                                                                                      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">R2:</span> Peregrine House Residence                                                                                   \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">R3:</span> Merlin House Residence                                                                                      \n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span><span style=\"font-weight: bold\">R4:</span> Kestrel Court Residence                                                                                     \n",
              "\n",
              "<span style=\"font-weight: bold\">Total Count:</span> 21                                                                                                    \n",
              "</pre>\n"
            ],
            "text/plain": [
              "Based on the context provided, here are the buildings with their letters and names, and the count:                 \n",
              "\n",
              "\u001b[1;33m • \u001b[0m\u001b[1mA:\u001b[0m Welcome Kiosk                                                                                                \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mB:\u001b[0m Food Court, James A. Colvin Atrium (Cafeteria)                                                               \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mC:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mD:\u001b[0m The Junction Bookstore, Career Services, Co-operative Education, Saffrons Restaurant, Olive Oyle's Deli,     \n",
              "\u001b[1;33m   \u001b[0mParking Office                                                                                                  \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mE:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mF:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mG:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mH:\u001b[0m Cafeteria, Siskind Gallery                                                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mJ:\u001b[0m Gym 1 & 2, Gym 3                                                                                             \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mK:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mL:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mM:\u001b[0m Library                                                                                                      \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mN:\u001b[0m I don't know                                                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSC:\u001b[0m Student Centre                                                                                              \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mSUB:\u001b[0m Student Union Building                                                                                     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mY:\u001b[0m Fanshawe Aviation Centre                                                                                     \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mZ:\u001b[0m Centre for Applied Transportation Technology                                                                 \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mR1:\u001b[0m Falcon House Residence                                                                                      \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mR2:\u001b[0m Peregrine House Residence                                                                                   \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mR3:\u001b[0m Merlin House Residence                                                                                      \n",
              "\u001b[1;33m • \u001b[0m\u001b[1mR4:\u001b[0m Kestrel Court Residence                                                                                     \n",
              "\n",
              "\u001b[1mTotal Count:\u001b[0m 21                                                                                                    \n"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "# Generate response with Gemini 2.0 Flash\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_2_0_flash,\n",
        "        model_input=prompt,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXm271jdD-Rl"
      },
      "source": [
        "### Search similar images with text query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPxwfyVrr9-G"
      },
      "source": [
        "Since plain text search and RAG didn't provide the detailed answer and the information may be visually represented in a table or another image format, you can use multimodal capability of Gemini 2.0 Flash for the similar task.\n",
        "\n",
        "The goal here also is to find an image similar to the text query. You may also print the citations to verify."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"How many buildings are in the map?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "nwXpqMSq4ppr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "#query = \"What are various med-gemini medical benchmarks that shows its performance relative to other models?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "knj4qQ4xni24",
        "outputId": "603899ae-e829-47ce-ce77-4d4f67f4b685",
        "tags": []
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "single positional indexer is out-of-bounds",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m matching_results_image = \u001b[43mget_similar_image_from_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_metadata_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_metadata_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_embedding_from_image_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use image description text embedding\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use text embedding instead of image embedding\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1408\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m **** Result: ***** \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/multimodal_qa_with_rag_utils.py:818\u001b[39m, in \u001b[36mget_similar_image_from_query\u001b[39m\u001b[34m(text_metadata_df, image_metadata_df, query, image_query_path, column_name, image_emb, top_n, embedding_size)\u001b[39m\n\u001b[32m    815\u001b[39m \u001b[38;5;66;03m# Get top N cosine scores and their indices\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[38;5;66;03m# Convert to Series if it's a DataFrame\u001b[39;00m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cosine_scores, pd.DataFrame):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     cosine_scores = \u001b[43mcosine_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Get first column as Series\u001b[39;00m\n\u001b[32m    820\u001b[39m top_n_cosine_scores = cosine_scores.nlargest(top_n).index.tolist()\n\u001b[32m    821\u001b[39m top_n_cosine_values = cosine_scores.nlargest(top_n).values.tolist()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1184\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m   1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.obj._get_value(*key, takeable=\u001b[38;5;28mself\u001b[39m._takeable)\n\u001b[32m-> \u001b[39m\u001b[32m1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[32m   1187\u001b[39m     axis = \u001b[38;5;28mself\u001b[39m.axis \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m0\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1690\u001b[39m, in \u001b[36m_iLocIndexer._getitem_tuple\u001b[39m\u001b[34m(self, tup)\u001b[39m\n\u001b[32m   1689\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_getitem_tuple\u001b[39m(\u001b[38;5;28mself\u001b[39m, tup: \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1690\u001b[39m     tup = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_tuple_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1691\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[32m   1692\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_lowerdim(tup)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:966\u001b[39m, in \u001b[36m_LocationIndexer._validate_tuple_indexer\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(key):\n\u001b[32m    965\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    968\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    969\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLocation based indexing can only have \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    970\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._valid_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] types\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    971\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1592\u001b[39m, in \u001b[36m_iLocIndexer._validate_key\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1591\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_integer(key):\n\u001b[32m-> \u001b[39m\u001b[32m1592\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m   1594\u001b[39m     \u001b[38;5;66;03m# a tuple should already have been caught by this point\u001b[39;00m\n\u001b[32m   1595\u001b[39m     \u001b[38;5;66;03m# so don't treat a tuple as a valid indexer\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\u001b[33m\"\u001b[39m\u001b[33mToo many indexers\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/pandas/core/indexing.py:1685\u001b[39m, in \u001b[36m_iLocIndexer._validate_integer\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1683\u001b[39m len_axis = \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj._get_axis(axis))\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key >= len_axis \u001b[38;5;129;01mor\u001b[39;00m key < -len_axis:\n\u001b[32m-> \u001b[39m\u001b[32m1685\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33msingle positional indexer is out-of-bounds\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mIndexError\u001b[39m: single positional indexer is out-of-bounds"
          ]
        }
      ],
      "source": [
        "matching_results_image = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,\n",
        "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding\n",
        "    image_emb=False,  # Use text embedding instead of image embedding\n",
        "    top_n=5,\n",
        "    embedding_size=1408,\n",
        ")\n",
        "\n",
        "# Markdown(print_text_to_image_citation(matching_results_image, print_top=True))\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "# Display the top matching image\n",
        "display_images(\n",
        "    [\n",
        "        matching_results_image[0][\"img_path\"],\n",
        "        #matching_results_image[1][\"img_path\"],\n",
        "        #matching_results_image[2][\"img_path\"],\n",
        "        #matching_results_image[3][\"img_path\"],\n",
        "    ],\n",
        "    resize_ratio=0.3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "HuRD1lZ8RNYP",
        "outputId": "6766990e-62ad-4926-8120-15519472cdb4",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " **** Result: ***** \n",
            "\n",
            "CPU times: user 407 μs, sys: 0 ns, total: 407 μs\n",
            "Wall time: 427 μs\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'matching_results_image' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtime\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mprint(\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn **** Result: ***** \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43minstruction = f\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAnswer the question and explain results with the given Image:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mQuestion: \u001b[39;49m\u001b[38;5;132;43;01m{query}\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mImage:\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Prepare the model input\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mmodel_input = [\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    instruction,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    # passing all matched images to Gemini\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    matching_results_image[0][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDescription:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    matching_results_image[0][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #matching_results_image[1][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDescription:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #matching_results_image[1][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #matching_results_image[2][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDescription:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #matching_results_image[2][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mImage:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #matching_results_image[3][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDescription:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    #matching_results_image[3][\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m],\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m# Generate Gemini response with streaming output\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43mrich_Markdown(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    get_gemini_response(\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        multimodal_model_2_0_flash,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        model_input=model_input,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        stream=True,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        safety_settings=safety_settings,\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m    )\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2565\u001b[39m, in \u001b[36mInteractiveShell.run_cell_magic\u001b[39m\u001b[34m(self, magic_name, line, cell)\u001b[39m\n\u001b[32m   2563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m   2564\u001b[39m     args = (magic_arg_s, cell)\n\u001b[32m-> \u001b[39m\u001b[32m2565\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2568\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2569\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1470\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupt_occured:\n\u001b[32m   1469\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exit_on_interrupt \u001b[38;5;129;01mand\u001b[39;00m captured_exception:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m captured_exception\n\u001b[32m   1471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/IPython/core/magics/execution.py:1434\u001b[39m, in \u001b[36mExecutionMagics.time\u001b[39m\u001b[34m(self, line, cell, local_ns)\u001b[39m\n\u001b[32m   1432\u001b[39m st = clock2()\n\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m     out = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1436\u001b[39m     \u001b[38;5;66;03m# multi-line %%time case\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:13\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m: name 'matching_results_image' is not defined"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "instruction = f\"\"\"Answer the question and explain results with the given Image:\n",
        "Question: {query}\n",
        "Image:\n",
        "\"\"\"\n",
        "\n",
        "# Prepare the model input\n",
        "model_input = [\n",
        "    instruction,\n",
        "    # passing all matched images to Gemini\n",
        "    \"Image:\",\n",
        "    matching_results_image[0][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    matching_results_image[0][\"image_description\"],\n",
        "    \"Image:\",\n",
        "    #matching_results_image[1][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    #matching_results_image[1][\"image_description\"],\n",
        "    \"Image:\",\n",
        "    #matching_results_image[2][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    #matching_results_image[2][\"image_description\"],\n",
        "    \"Image:\",\n",
        "    #matching_results_image[3][\"image_object\"],\n",
        "    \"Description:\",\n",
        "    #matching_results_image[3][\"image_description\"],\n",
        "]\n",
        "\n",
        "# Generate Gemini response with streaming output\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_2_0_flash,\n",
        "        model_input=model_input,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uykUaQvIRNYP",
        "outputId": "42720a84-a357-4d72-f3f3-cc8bb8a79621",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[91mCitation 1: Matched image path, page number and page text: \n",
            "\u001b[0m\n",
            "\u001b[94mscore: \u001b[0m 0.44\n",
            "\u001b[94mfile_name: \u001b[0m gemma_technical_paper.pdf\n",
            "\u001b[94mpath: \u001b[0m images/gemma_technical_paper.pdf_image_0_0_153.jpeg\n",
            "\u001b[94mpage number: \u001b[0m 1\n",
            "\u001b[94mpage text: \u001b[0m 2024-02-21\n",
            "Gemma: Open Models Based on Gemini\n",
            "Research and Technology\n",
            "Gemma Team, Google DeepMind1\n",
            "This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research\n",
            "and technology used to create Gemini models. Gemma models demonstrate strong performance across\n",
            "academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models\n",
            "(2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma\n",
            "outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive\n",
            "evaluations of safety and responsibility aspects of the models, alongside a detailed description of model\n",
            "development. We believe the responsible release of LLMs is critical for improving the safety of frontier\n",
            "models, and for enabling the next wave of LLM innovations.\n",
            "Introduction\n",
            "We present Gemma, a family of open models\n",
            "based on Googles Gemini models (Gemini Team,\n",
            "2023).\n",
            "We trained Gemma models on up to 6T tokens\n",
            "of text, using architectures, data, and training\n",
            "recipes inspired by the Gemini model family. Like\n",
            "Gemini, these models achieve strong generalist ca-\n",
            "pabilities in text domains, alongside state-of-the-\n",
            "art understanding and reasoning skills at scale.\n",
            "With this work, we release both pre-trained and\n",
            "fine-tuned checkpoints, as well as an open-source\n",
            "codebase for inference and serving.\n",
            "Gemma comes in two sizes: a 7 billion param-\n",
            "eter model for efficient deployment and develop-\n",
            "ment on GPU and TPU, and a 2 billion param-\n",
            "eter model for CPU and on-device applications.\n",
            "Each size is designed to address different compu-\n",
            "tational constraints, applications, and developer\n",
            "requirements. At each scale, we release raw, pre-\n",
            "trained checkpoints, as well as checkpoints fine-\n",
            "tuned for dialogue, instruction-following, help-\n",
            "fulness, and safety. We thoroughly evaluate the\n",
            "shortcomings of our models on a suite of quantita-\n",
            "tive and qualitative benchmarks. We believe the\n",
            "release of both pretrained and fine-tuned check-\n",
            "points will enable thorough research and inves-\n",
            "tigation into the impact of current instruction-\n",
            "tuning regimes, as well as the development of\n",
            "increasingly safe and responsible model develop-\n",
            "ment methodologies.\n",
            "Gemma advances state-of-the-art performance\n",
            "relative to comparable-scale (and some larger),\n",
            "open models (Almazrouei et al., 2023; Jiang\n",
            "et al., 2023; Touvron et al., 2023a,b) across a\n",
            "wide range of domains including both automated\n",
            "benchmarks and human evaluation. Example do-\n",
            "mains include question answering (Clark et al.,\n",
            "2019; Kwiatkowski et al., 2019), commonsense\n",
            "reasoning (Sakaguchi et al., 2019; Suzgun et al.,\n",
            "2022), mathematics and science (Cobbe et al.,\n",
            "2021; Hendrycks et al., 2020), and coding (Austin\n",
            "et al., 2021; Chen et al., 2021). See complete de-\n",
            "tails in the Evaluation section.\n",
            "Like Gemini, Gemma builds on recent work\n",
            "on sequence models (Sutskever et al., 2014) and\n",
            "transformers (Vaswani et al., 2017), deep learn-\n",
            "ing methods based on neural networks (LeCun\n",
            "et al., 2015), and techniques for large-scale train-\n",
            "ing on distributed systems (Barham et al., 2022;\n",
            "Dean et al., 2012; Roberts et al., 2023). Gemma\n",
            "also builds on Googles long history of open mod-\n",
            "els and ecosystems, including Word2Vec (Mikolov\n",
            "et al., 2013), the Transformer (Vaswani et al.,\n",
            "2017), BERT (Devlin et al., 2018), and T5 (Raffel\n",
            "et al., 2019) and T5X (Roberts et al., 2022).\n",
            "We believe the responsible release of LLMs is\n",
            "critical for improving the safety of frontier models,\n",
            "for ensuring equitable access to this breakthrough\n",
            "technology, for enabling rigorous evaluation and\n",
            "analysis of current techniques, and for enabling\n",
            "the development of the next wave of innovations.\n",
            "While thorough testing of all Gemma models has\n",
            "1See Contributions and Acknowledgments section for full author list. Please send correspondence to gemma-1-report@google.com.\n",
            " 2024 Google DeepMind. All rights reserved\n",
            "arXiv:2403.08295v4  [cs.CL]  16 Apr 2024\n",
            "\n",
            "\u001b[94mimage description: \u001b[0m The image shows the logos for Google and DeepMind. The Google logo is in the company's signature colors of blue, red, yellow, and green. The DeepMind logo is in gray. The background is black.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[3;35mNone\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## you can check the citations to probe further.\n",
        "## check the \"image description:\" which is a description extracted through gemini which helped search our query.\n",
        "rich_print(print_text_to_image_citation(matching_results_image, print_top=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDd9rE4NrRod"
      },
      "source": [
        "## Image Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJL6ElyEy4mc"
      },
      "source": [
        "### Search similar image with image input [using multimodal image embeddings]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReKjHleFxUu9"
      },
      "source": [
        "Imagine searching for images, but instead of typing words, you use an actual image as the clue.\n",
        "\n",
        "Think of it like searching with a mini-map instead of a written address.\n",
        "It's a different way to ask, \"Show me more stuff like this\".\n",
        "\n",
        "So, instead of typing \"various example of gemini 1.5 long context\", you show a picture of that image and say, \"Find me more like this\"\n",
        "\n",
        "For demonstration purposes, we will only be finding similar images that show the various features of Gemini in a single document below. However, you can scale this design pattern to match (find relevant images) across multiple documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        },
        "id": "DJhhS5eZw7QI",
        "outputId": "970b966d-84a5-4a2a-97f3-18a14902c278",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***Input image from user:***\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'images/M3.jpeg'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m***Input image from user:***\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Display the input image\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_query_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:3257\u001b[39m, in \u001b[36mImage.load_from_file\u001b[39m\u001b[34m(location)\u001b[39m\n\u001b[32m   3247\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m   3248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_from_file\u001b[39m(location: \u001b[38;5;28mstr\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mImage\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3249\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Loads image from file.\u001b[39;00m\n\u001b[32m   3250\u001b[39m \n\u001b[32m   3251\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3255\u001b[39m \u001b[33;03m        Loaded image as an `Image` object.\u001b[39;00m\n\u001b[32m   3256\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3257\u001b[39m     image_bytes = \u001b[43mpathlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3258\u001b[39m     image = Image()\n\u001b[32m   3259\u001b[39m     image._image_bytes = image_bytes\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/pathlib/_abc.py:625\u001b[39m, in \u001b[36mPathBase.read_bytes\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    621\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    622\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    623\u001b[39m \u001b[33;03m    Open the file in bytes mode, read it, and close the file.\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    626\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m f.read()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/pathlib/_local.py:537\u001b[39m, in \u001b[36mPath.open\u001b[39m\u001b[34m(self, mode, buffering, encoding, errors, newline)\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m    536\u001b[39m     encoding = io.text_encoding(encoding)\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'images/M3.jpeg'"
          ]
        }
      ],
      "source": [
        "# You can find a similar image as per the images you have in the metadata.\n",
        "\n",
        "image_query_path = \"images/M3.jpeg\"\n",
        "\n",
        "# Print a message indicating the input image\n",
        "print(\"***Input image from user:***\")\n",
        "\n",
        "# Display the input image\n",
        "Image.load_from_file(image_query_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\n",
            "\n",
            "📊 Imagens atuais na pasta: 1\n",
            "🔄 Extraindo imagens do PDF para ter mais dados...\n",
            "🔍 Processando PDF: map/map.pdf\n",
            "📊 PDF tem 1 páginas\n",
            "📄 Página 1: 0 imagens encontradas\n",
            "\n",
            "🎉 Total de 0 imagens extraídas!\n",
            "❌ Nenhuma imagem foi extraída do PDF\n",
            "\n",
            "📊 STATUS FINAL: 1 imagens na pasta 'images/'\n",
            "⚠️  Ainda há apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 78 (OPCIONAL) - 📄 EXTRAIR MAIS IMAGENS DO PDF PARA COMPARAÇÃO\n",
        "# Esta célula extrai imagens do map.pdf para ter mais dados para comparação\n",
        "\n",
        "print(\"=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\\n\")\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def extrair_imagens_do_pdf(pdf_path, output_dir=\"images/\", prefixo=\"map\"):\n",
        "    \"\"\"\n",
        "    Extrai imagens de um PDF e salva na pasta de imagens\n",
        "    \"\"\"\n",
        "    print(f\"🔍 Processando PDF: {pdf_path}\")\n",
        "    \n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"❌ PDF não encontrado: {pdf_path}\")\n",
        "        return []\n",
        "    \n",
        "    # Criar diretório se não existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Abrir PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    imagens_extraidas = []\n",
        "    \n",
        "    print(f\"📊 PDF tem {len(doc)} páginas\")\n",
        "    \n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        images = page.get_images()\n",
        "        \n",
        "        print(f\"📄 Página {page_num + 1}: {len(images)} imagens encontradas\")\n",
        "        \n",
        "        for img_index, img in enumerate(images):\n",
        "            try:\n",
        "                # Extrair imagem\n",
        "                xref = img[0]\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "                \n",
        "                # Converter para RGB se necessário\n",
        "                if pix.colorspace and pix.colorspace.n > 3:\n",
        "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                \n",
        "                # Nome do arquivo\n",
        "                img_filename = f\"{prefixo}_page_{page_num + 1}_img_{img_index + 1}.png\"\n",
        "                img_path = os.path.join(output_dir, img_filename)\n",
        "                \n",
        "                # Salvar imagem\n",
        "                pix.save(img_path)\n",
        "                imagens_extraidas.append(img_path)\n",
        "                \n",
        "                print(f\"  ✅ Extraída: {img_filename}\")\n",
        "                \n",
        "                pix = None  # Liberar memória\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao extrair imagem {img_index}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    doc.close()\n",
        "    print(f\"\\n🎉 Total de {len(imagens_extraidas)} imagens extraídas!\")\n",
        "    return imagens_extraidas\n",
        "\n",
        "# Verificar quantas imagens temos atualmente\n",
        "current_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"📊 Imagens atuais na pasta: {current_images}\")\n",
        "\n",
        "if current_images <= 1:\n",
        "    print(\"🔄 Extraindo imagens do PDF para ter mais dados...\")\n",
        "    \n",
        "    # Extrair do map.pdf se existir\n",
        "    if os.path.exists(\"map/map.pdf\"):\n",
        "        imagens_extraidas = extrair_imagens_do_pdf(\"map/map.pdf\", \"images/\", \"map\")\n",
        "        \n",
        "        if imagens_extraidas:\n",
        "            print(f\"\\n✅ {len(imagens_extraidas)} novas imagens adicionadas!\")\n",
        "            print(\"🚀 Agora execute a CÉLULA 76 novamente para processar todas as imagens\")\n",
        "            print(\"   Depois execute a CÉLULA 70 para testar similaridade com mais dados\")\n",
        "        else:\n",
        "            print(\"❌ Nenhuma imagem foi extraída do PDF\")\n",
        "    else:\n",
        "        print(\"❌ Arquivo map/map.pdf não encontrado\")\n",
        "        \n",
        "        # Verificar outros PDFs disponíveis\n",
        "        print(\"\\n🔍 Procurando outros PDFs...\")\n",
        "        pdf_paths = []\n",
        "        for root, dirs, files in os.walk(\".\"):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    pdf_paths.append(os.path.join(root, file))\n",
        "        \n",
        "        if pdf_paths:\n",
        "            print(\"📋 PDFs encontrados:\")\n",
        "            for i, pdf_path in enumerate(pdf_paths[:3], 1):  # Mostrar apenas os 3 primeiros\n",
        "                print(f\"  {i}. {pdf_path}\")\n",
        "                \n",
        "            # Processar o primeiro PDF encontrado\n",
        "            if pdf_paths:\n",
        "                primeiro_pdf = pdf_paths[0]\n",
        "                print(f\"\\n🔄 Processando: {primeiro_pdf}\")\n",
        "                imagens_extraidas = extrair_imagens_do_pdf(primeiro_pdf, \"images/\", \"doc\")\n",
        "                \n",
        "                if imagens_extraidas:\n",
        "                    print(f\"\\n✅ {len(imagens_extraidas)} imagens extraídas de {primeiro_pdf}!\")\n",
        "                    print(\"🚀 Execute a CÉLULA 76 novamente para processar todas as imagens\")\n",
        "        else:\n",
        "            print(\"❌ Nenhum PDF encontrado para extrair imagens\")\n",
        "            \n",
        "else:\n",
        "    print(\"✅ Já há múltiplas imagens na pasta\")\n",
        "    print(\"Execute a CÉLULA 76 para processar todas e depois a CÉLULA 70 para testar similaridade\")\n",
        "\n",
        "# Mostrar status final\n",
        "final_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"\\n📊 STATUS FINAL: {final_images} imagens na pasta 'images/'\")\n",
        "\n",
        "if final_images > 1:\n",
        "    print(\"🎉 Pronto para testar busca por similaridade!\")\n",
        "    print(\"📋 PRÓXIMOS PASSOS:\")\n",
        "    print(\"  1. Execute CÉLULA 76 (processar todas as imagens)\")\n",
        "    print(\"  2. Execute CÉLULA 70 (busca por similaridade)\")\n",
        "    print(\"  3. Execute CÉLULA 71 (análise contextual)\")\n",
        "else:\n",
        "    print(\"⚠️  Ainda há apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Função 'processar_imagens_da_pasta' criada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 75 (NOVO) - 📂 PROCESSAMENTO DIRETO DE IMAGENS DA PASTA\n",
        "# Função para ler todas as imagens da pasta images/ e gerar embeddings para RAG\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_image_embedding_from_multimodal_embedding_model,\n",
        "    get_gemini_response\n",
        ")\n",
        "\n",
        "def processar_imagens_da_pasta(\n",
        "    pasta_imagens=\"images/\",\n",
        "    embedding_size=512,\n",
        "    gerar_descricoes=True,\n",
        "    formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "):\n",
        "    \"\"\"\n",
        "    Processa todas as imagens de uma pasta, gerando embeddings e descrições para RAG\n",
        "    \n",
        "    Args:\n",
        "        pasta_imagens: Caminho da pasta com imagens\n",
        "        embedding_size: Tamanho do embedding (128, 256, 512, 1408)\n",
        "        gerar_descricoes: Se deve gerar descrições das imagens com Gemini\n",
        "        formatos_suportados: Lista de formatos de imagem aceitos\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame compatível com o sistema RAG existente\n",
        "    \"\"\"\n",
        "    print(f\"🔍 PROCESSANDO IMAGENS DA PASTA: {pasta_imagens}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Verificar se a pasta existe\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        print(f\"❌ Pasta '{pasta_imagens}' não encontrada!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Encontrar todas as imagens na pasta\n",
        "    imagens_encontradas = []\n",
        "    for formato in formatos_suportados:\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato.upper()}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "    \n",
        "    # Remover duplicatas\n",
        "    imagens_encontradas = list(set(imagens_encontradas))\n",
        "    \n",
        "    if not imagens_encontradas:\n",
        "        print(f\"❌ Nenhuma imagem encontrada na pasta '{pasta_imagens}'\")\n",
        "        print(f\"Formatos suportados: {formatos_suportados}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"📊 Encontradas {len(imagens_encontradas)} imagens:\")\n",
        "    for img in imagens_encontradas:\n",
        "        print(f\"  - {os.path.basename(img)}\")\n",
        "    \n",
        "    # Lista para armazenar dados processados\n",
        "    dados_imagens = []\n",
        "    \n",
        "    # Prompt para descrição das imagens\n",
        "    prompt_descricao = \"\"\"Analise esta imagem detalhadamente e forneça uma descrição precisa.\n",
        "    Inclua:\n",
        "    - O que você vê na imagem\n",
        "    - Elementos principais e detalhes importantes\n",
        "    - Texto visível (se houver)\n",
        "    - Tipo de imagem (mapa, diagrama, foto, etc.)\n",
        "    - Informações relevantes para busca e recuperação\n",
        "    \n",
        "    Seja específico e detalhado para facilitar buscas futuras.\"\"\"\n",
        "    \n",
        "    print(f\"\\n🚀 PROCESSANDO CADA IMAGEM...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, caminho_imagem in enumerate(imagens_encontradas, 1):\n",
        "        nome_arquivo = os.path.basename(caminho_imagem)\n",
        "        print(f\"\\n📸 PROCESSANDO {i}/{len(imagens_encontradas)}: {nome_arquivo}\")\n",
        "        \n",
        "        try:\n",
        "            # 1. Gerar embedding da imagem\n",
        "            print(\"  🔄 Gerando embedding...\")\n",
        "            image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
        "                image_uri=caminho_imagem,\n",
        "                embedding_size=embedding_size,\n",
        "                return_array=True\n",
        "            )\n",
        "            print(f\"  ✅ Embedding gerado: shape {image_embedding.shape}\")\n",
        "            \n",
        "            # 2. Gerar descrição da imagem (se solicitado)\n",
        "            descricao = \"\"\n",
        "            if gerar_descricoes:\n",
        "                print(\"  🤖 Gerando descrição com Gemini...\")\n",
        "                try:\n",
        "                    from vertexai.generative_models import Image as GeminiImage\n",
        "                    imagem_gemini = GeminiImage.load_from_file(caminho_imagem)\n",
        "                    \n",
        "                    descricao = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[prompt_descricao, imagem_gemini],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"  ✅ Descrição gerada: {len(descricao)} caracteres\")\n",
        "                    \n",
        "                except Exception as desc_error:\n",
        "                    print(f\"  ⚠️  Erro ao gerar descrição: {desc_error}\")\n",
        "                    descricao = f\"Imagem: {nome_arquivo}\"\n",
        "            \n",
        "            # 3. Gerar embedding da descrição (para compatibilidade com RAG)\n",
        "            text_embedding = None\n",
        "            if descricao:\n",
        "                try:\n",
        "                    from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
        "                    text_embedding = get_text_embedding_from_text_embedding_model(descricao)\n",
        "                    print(\"  ✅ Text embedding da descrição gerado\")\n",
        "                except Exception as text_emb_error:\n",
        "                    print(f\"  ⚠️  Erro ao gerar text embedding: {text_emb_error}\")\n",
        "            \n",
        "            # 4. Criar registro compatível com o sistema existente\n",
        "            registro = {\n",
        "                'file_name': f\"pasta_images_{nome_arquivo}\",  # Nome único\n",
        "                'page_num': 1,  # Imagens individuais = página 1\n",
        "                'img_num': i,\n",
        "                'img_path': caminho_imagem,\n",
        "                'img_desc': descricao,\n",
        "                'mm_embedding_from_img_only': image_embedding.tolist(),  # Compatibilidade\n",
        "                'text_embedding_from_image_description': text_embedding if text_embedding else None,\n",
        "                'source_type': 'pasta_imagens',  # Identificar origem\n",
        "                'original_filename': nome_arquivo\n",
        "            }\n",
        "            \n",
        "            dados_imagens.append(registro)\n",
        "            print(f\"  ✅ Processamento concluído para {nome_arquivo}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erro ao processar {nome_arquivo}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    if dados_imagens:\n",
        "        df_imagens = pd.DataFrame(dados_imagens)\n",
        "        print(f\"\\n🎉 PROCESSAMENTO CONCLUÍDO!\")\n",
        "        print(f\"📊 DataFrame criado com {len(df_imagens)} imagens processadas\")\n",
        "        print(f\"📋 Colunas: {list(df_imagens.columns)}\")\n",
        "        \n",
        "        return df_imagens\n",
        "    else:\n",
        "        print(f\"\\n❌ Nenhuma imagem foi processada com sucesso\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"✅ Função 'processar_imagens_da_pasta' criada com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\n",
            "\n",
            "🔍 PROCESSANDO IMAGENS DA PASTA: images/\n",
            "============================================================\n",
            "📊 Encontradas 1 imagens:\n",
            "  - B2_room.jpeg\n",
            "\n",
            "🚀 PROCESSANDO CADA IMAGEM...\n",
            "============================================================\n",
            "\n",
            "📸 PROCESSANDO 1/1: B2_room.jpeg\n",
            "  🔄 Gerando embedding...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/vision_models/_vision_models.py:153: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1759255557.925444   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759255557.928186   65306 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ✅ Embedding gerado: shape (512,)\n",
            "  🤖 Gerando descrição com Gemini...\n",
            "  ⚠️  Erro ao gerar descrição: 'GenerationResponse' object is not iterable\n",
            "  ✅ Text embedding da descrição gerado\n",
            "  ✅ Processamento concluído para B2_room.jpeg\n",
            "\n",
            "🎉 PROCESSAMENTO CONCLUÍDO!\n",
            "📊 DataFrame criado com 1 imagens processadas\n",
            "📋 Colunas: ['file_name', 'page_num', 'img_num', 'img_path', 'img_desc', 'mm_embedding_from_img_only', 'text_embedding_from_image_description', 'source_type', 'original_filename']\n",
            "\n",
            "🎉 SUCESSO TOTAL!\n",
            "📊 image_metadata_df criado com 1 imagens\n",
            "\n",
            "📋 RESUMO DAS IMAGENS PROCESSADAS:\n",
            "==================================================\n",
            "\n",
            "🖼️  Imagem 1:\n",
            "  📁 Arquivo: B2_room.jpeg\n",
            "  📂 Caminho: images/B2_room.jpeg\n",
            "  📊 Embedding shape: 512\n",
            "  📝 Descrição: Imagem: B2_room.jpeg\n",
            "\n",
            "✅ COMPATIBILIDADE COM SISTEMA RAG:\n",
            "  ✅ img_path: OK\n",
            "  ✅ mm_embedding_from_img_only: OK\n",
            "  ✅ img_desc: OK\n",
            "  ✅ file_name: OK\n",
            "  ✅ page_num: OK\n",
            "\n",
            "💾 DataFrame salvo em 'image_metadata_from_folder.pkl'\n",
            "\n",
            "🚀 PRÓXIMOS PASSOS:\n",
            "1. Agora você pode executar a CÉLULA 70 (Validação)\n",
            "2. Depois executar a CÉLULA 71 (Análise Contextual)\n",
            "3. O sistema RAG está pronto para perguntas sobre as imagens!\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 76 (EXECUTAR) - 🚀 PROCESSAMENTO DAS IMAGENS DA PASTA images/\n",
        "# Executa o processamento de todas as imagens e cria o image_metadata_df\n",
        "\n",
        "print(\"=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\\n\")\n",
        "\n",
        "# Executar o processamento das imagens\n",
        "try:\n",
        "    image_metadata_df = processar_imagens_da_pasta(\n",
        "        pasta_imagens=\"images/\",\n",
        "        embedding_size=512,\n",
        "        gerar_descricoes=True,  # Gerar descrições detalhadas com Gemini\n",
        "        formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "    )\n",
        "    \n",
        "    if not image_metadata_df.empty:\n",
        "        print(f\"\\n🎉 SUCESSO TOTAL!\")\n",
        "        print(f\"📊 image_metadata_df criado com {len(image_metadata_df)} imagens\")\n",
        "        \n",
        "        # Mostrar resumo das imagens processadas\n",
        "        print(f\"\\n📋 RESUMO DAS IMAGENS PROCESSADAS:\")\n",
        "        print(\"=\"*50)\n",
        "        for idx, row in image_metadata_df.iterrows():\n",
        "            print(f\"\\n🖼️  Imagem {idx + 1}:\")\n",
        "            print(f\"  📁 Arquivo: {row['original_filename']}\")\n",
        "            print(f\"  📂 Caminho: {row['img_path']}\")\n",
        "            print(f\"  📊 Embedding shape: {len(row['mm_embedding_from_img_only'])}\")\n",
        "            \n",
        "            # Mostrar início da descrição\n",
        "            desc = row['img_desc']\n",
        "            if desc and len(desc) > 10:\n",
        "                print(f\"  📝 Descrição: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
        "        \n",
        "        # Verificar compatibilidade com sistema RAG existente\n",
        "        print(f\"\\n✅ COMPATIBILIDADE COM SISTEMA RAG:\")\n",
        "        colunas_necessarias = ['img_path', 'mm_embedding_from_img_only', 'img_desc', 'file_name', 'page_num']\n",
        "        for col in colunas_necessarias:\n",
        "            if col in image_metadata_df.columns:\n",
        "                print(f\"  ✅ {col}: OK\")\n",
        "            else:\n",
        "                print(f\"  ❌ {col}: FALTANDO\")\n",
        "        \n",
        "        # Salvar para uso futuro (opcional)\n",
        "        try:\n",
        "            image_metadata_df.to_pickle(\"image_metadata_from_folder.pkl\")\n",
        "            print(f\"\\n💾 DataFrame salvo em 'image_metadata_from_folder.pkl'\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"\\n⚠️  Não foi possível salvar: {save_error}\")\n",
        "        \n",
        "        print(f\"\\n🚀 PRÓXIMOS PASSOS:\")\n",
        "        print(f\"1. Agora você pode executar a CÉLULA 70 (Validação)\")\n",
        "        print(f\"2. Depois executar a CÉLULA 71 (Análise Contextual)\")\n",
        "        print(f\"3. O sistema RAG está pronto para perguntas sobre as imagens!\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n❌ FALHA: Nenhuma imagem foi processada\")\n",
        "        print(f\"Verifique se:\")\n",
        "        print(f\"- A pasta 'images/' existe\")\n",
        "        print(f\"- Há imagens válidas na pasta\")\n",
        "        print(f\"- Os modelos estão carregados corretamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO NO PROCESSAMENTO: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\n💡 POSSÍVEIS SOLUÇÕES:\")\n",
        "    print(f\"- Verifique se os modelos estão carregados\")\n",
        "    print(f\"- Verifique se a pasta 'images/' existe\")\n",
        "    print(f\"- Execute as células de setup dos modelos primeiro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== M3.JPEG SIMILARITY SEARCH VALIDATION ===\n",
            "\n",
            "✅ image_metadata_df available!\n",
            "📊 Dataset: 1 images processed\n",
            "\n",
            "📋 IMAGES IN DATASET:\n",
            "  1. B2_room.jpeg\n",
            "\n",
            "❌ M3.jpeg not found in dataset!\n",
            "Check if the image is in the 'images/' folder and execute CELL 76 again\n"
          ]
        }
      ],
      "source": [
        "# CELL 70 (STEP 3) - 🎯 VALIDATION: Similarity search with M3.jpeg in the dataset\n",
        "# This cell uses the image_metadata_df created by Cell 76 to search for similar images\n",
        "\n",
        "print(\"=== M3.JPEG SIMILARITY SEARCH VALIDATION ===\\n\")\n",
        "\n",
        "# Check if we have the image metadata DataFrame\n",
        "if 'image_metadata_df' not in locals():\n",
        "    print(\"❌ image_metadata_df not found!\")\n",
        "    print(\"\\n💡 To resolve:\")\n",
        "    print(\"1. Execute CELL 78 first (Extract images from PDF)\")\n",
        "    print(\"2. Execute CELL 76 next (Process Images)\")\n",
        "    print(\"3. Then execute this cell again\")\n",
        "    \n",
        "elif image_metadata_df.empty:\n",
        "    print(\"❌ image_metadata_df is empty!\")\n",
        "    print(\"Execute CELL 76 to process images from the folder\")\n",
        "    \n",
        "else:\n",
        "    print(\"✅ image_metadata_df available!\")\n",
        "    print(f\"📊 Dataset: {len(image_metadata_df)} images processed\")\n",
        "    \n",
        "    # Show all images in the dataset\n",
        "    print(f\"\\n📋 IMAGES IN DATASET:\")\n",
        "    for idx, row in image_metadata_df.iterrows():\n",
        "        print(f\"  {idx + 1}. {row['original_filename']}\")\n",
        "    \n",
        "    # Find M3.jpeg in the dataset\n",
        "    m3_rows = image_metadata_df[image_metadata_df['original_filename'].str.contains('M3.jpeg', case=False, na=False)]\n",
        "    \n",
        "    if m3_rows.empty:\n",
        "        print(\"\\n❌ M3.jpeg not found in dataset!\")\n",
        "        print(\"Check if the image is in the 'images/' folder and execute CELL 76 again\")\n",
        "    else:\n",
        "        print(f\"\\n✅ M3.jpeg found in dataset!\")\n",
        "        m3_row = m3_rows.iloc[0]\n",
        "        print(f\"  📁 File: {m3_row['original_filename']}\")\n",
        "        print(f\"  📂 Path: {m3_row['img_path']}\")\n",
        "        \n",
        "        # Extract M3.jpeg embedding\n",
        "        m3_embedding = np.array(m3_row['mm_embedding_from_img_only'])\n",
        "        print(f\"  📊 Embedding shape: {m3_embedding.shape}\")\n",
        "        \n",
        "        # Create dataset without M3.jpeg itself for comparison\n",
        "        other_images_df = image_metadata_df[~image_metadata_df['original_filename'].str.contains('M3.jpeg', case=False, na=False)]\n",
        "        \n",
        "        if other_images_df.empty:\n",
        "            print(\"\\n⚠️  Only M3.jpeg found in dataset\")\n",
        "            print(\"Execute CELL 78 first to extract more images from PDF\")\n",
        "            print(\"Then execute CELL 76 to process all images\")\n",
        "        else:\n",
        "            print(f\"\\n🔍 EXECUTING SIMILARITY SEARCH...\")\n",
        "            print(f\"📊 Comparing M3.jpeg with {len(other_images_df)} other images\")\n",
        "            \n",
        "            # Use our robust alternative function\n",
        "            try:\n",
        "                similar_results = buscar_imagens_similares_com_embedding(\n",
        "                    image_embedding=m3_embedding,\n",
        "                    image_metadata_df=other_images_df,\n",
        "                    top_n=min(5, len(other_images_df))  # Top N most similar images\n",
        "                )\n",
        "                \n",
        "                if similar_results:\n",
        "                    print(f\"\\n🎉 SUCCESS! Found {len(similar_results)} images similar to M3.jpeg:\")\n",
        "                    print(\"=\"*80)\n",
        "                    \n",
        "                    for i, result in enumerate(similar_results, 1):\n",
        "                        print(f\"\\n🖼️  RESULT {i}:\")\n",
        "                        print(f\"  📈 Similarity: {result['cosine_score']:.4f}\")\n",
        "                        print(f\"  📁 File: {result['file_name']}\")\n",
        "                        print(f\"  📂 Path: {result['img_path']}\")\n",
        "                        \n",
        "                        # Show description if available\n",
        "                        desc = result['img_desc']\n",
        "                        if desc and desc != 'N/A' and len(str(desc)) > 10:\n",
        "                            desc_str = str(desc)\n",
        "                            print(f\"  📝 Description: {desc_str[:200]}{'...' if len(desc_str) > 200 else ''}\")\n",
        "                    \n",
        "                    # Save results for later use\n",
        "                    globals()['matching_results'] = similar_results\n",
        "                    print(f\"\\n💾 Results saved to 'matching_results' variable\")\n",
        "                    \n",
        "                    # Similarity score analysis\n",
        "                    scores = [r['cosine_score'] for r in similar_results]\n",
        "                    print(f\"\\n📊 SCORE ANALYSIS:\")\n",
        "                    print(f\"  - Maximum score: {max(scores):.4f}\")\n",
        "                    print(f\"  - Minimum score: {min(scores):.4f}\")\n",
        "                    print(f\"  - Average score: {sum(scores)/len(scores):.4f}\")\n",
        "                    \n",
        "                    print(f\"\\n✅ VALIDATION COMPLETED SUCCESSFULLY!\")\n",
        "                    print(f\"🚀 Now you can execute CELL 71 for contextual analysis!\")\n",
        "                    \n",
        "                else:\n",
        "                    print(\"❌ No similar images found.\")\n",
        "                    print(\"This may indicate problems with embeddings or very different data.\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error during search: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                \n",
        "                print(f\"\\n💡 DIAGNOSIS:\")\n",
        "                print(f\"- Check if function 'buscar_imagens_similares_com_embedding' was defined (CELL 75)\")\n",
        "                print(f\"- Check if there are other images besides M3.jpeg in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ANÁLISE CONTEXTUAL DA M3.JPEG ===\n",
            "\n",
            "❌ Nenhum resultado de busca por similaridade encontrado.\n",
            "Execute a célula anterior (70) primeiro para obter os resultados.\n",
            "\n",
            "💡 Como alternativa, vou fazer uma análise direta da M3.jpeg:\n",
            "❌ Erro na análise direta: [Errno 2] No such file or directory: 'images/M3.jpeg'\n",
            "\n",
            "✅ ANÁLISE CONTEXTUAL CONCLUÍDA!\n",
            "A M3.jpeg foi analisada usando o contexto das imagens similares encontradas via embedding.\n"
          ]
        }
      ],
      "source": [
        "# Novo Código\n",
        "\n",
        "# CÉLULA 71 (PASSO 4) - 🤖 ANÁLISE CONTEXTUAL: Perguntas sobre M3.jpeg com base nas imagens similares\n",
        "# Esta célula usa os resultados da busca por similaridade para análise contextual\n",
        "\n",
        "print(\"=== ANÁLISE CONTEXTUAL DA M3.JPEG ===\\n\")\n",
        "\n",
        "# Verificar se temos resultados da busca anterior\n",
        "if 'matching_results' not in locals() or not matching_results:\n",
        "    print(\"❌ Nenhum resultado de busca por similaridade encontrado.\")\n",
        "    print(\"Execute a célula anterior (70) primeiro para obter os resultados.\")\n",
        "    \n",
        "    print(\"\\n💡 Como alternativa, vou fazer uma análise direta da M3.jpeg:\")\n",
        "    \n",
        "    try:\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        m3_image = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "        \n",
        "        pergunta_direta = \"\"\"Analise esta imagem detalhadamente. \n",
        "        O que você vê? Descreva todos os elementos visíveis.\n",
        "        Se é um mapa ou planta, identifique os elementos principais.\"\"\"\n",
        "        \n",
        "        resposta_direta = get_gemini_response(\n",
        "            multimodal_model_2_0_flash,\n",
        "            model_input=[pergunta_direta, m3_image],\n",
        "            stream=False,\n",
        "        )\n",
        "        \n",
        "        print(f\"🤖 ANÁLISE DIRETA DA M3.JPEG:\")\n",
        "        print(f\"{resposta_direta}\")\n",
        "        \n",
        "    except Exception as direct_error:\n",
        "        print(f\"❌ Erro na análise direta: {direct_error}\")\n",
        "\n",
        "else:\n",
        "    print(f\"✅ Temos {len(matching_results)} resultados de busca por similaridade!\")\n",
        "    \n",
        "    # Preparar contexto baseado nos resultados similares\n",
        "    contexto_descricoes = []\n",
        "    contexto_caminhos = []\n",
        "    \n",
        "    for i, result in enumerate(matching_results):\n",
        "        desc = result.get('img_desc', '')\n",
        "        caminho = result.get('img_path', '')\n",
        "        arquivo = result.get('file_name', '')\n",
        "        score = result.get('cosine_score', 0)\n",
        "        \n",
        "        if desc and desc != 'N/A' and len(str(desc)) > 10:\n",
        "            contexto_descricoes.append(f\"Imagem similar {i+1} (similaridade: {score:.3f}): {desc}\")\n",
        "        \n",
        "        if caminho and caminho != 'N/A':\n",
        "            contexto_caminhos.append(caminho)\n",
        "    \n",
        "    print(f\"📝 Coletadas {len(contexto_descricoes)} descrições de imagens similares\")\n",
        "    \n",
        "    # Perguntas específicas sobre a M3.jpeg\n",
        "    perguntas_contextualizadas = [\n",
        "        \"Baseado nas imagens similares encontradas, o que você pode me dizer sobre a M3.jpeg?\",\n",
        "        \"Quais elementos em comum existem entre a M3.jpeg e as imagens similares?\",\n",
        "        \"Se a M3.jpeg é um mapa ou planta, quais informações específicas posso extrair?\",\n",
        "        \"Há algum padrão arquitetônico ou de layout visível na M3.jpeg?\",\n",
        "        \"What are the rooms in this floor? (baseado no contexto das imagens similares)\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n🤖 ANÁLISE CONTEXTUAL COM GEMINI:\")\n",
        "    \n",
        "    try:\n",
        "        # Carregar a imagem M3.jpeg\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        m3_image = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "        \n",
        "        # Preparar contexto das imagens similares\n",
        "        contexto_texto = \"\\n\".join(contexto_descricoes[:3])  # Top 3 descrições\n",
        "        \n",
        "        for i, pergunta in enumerate(perguntas_contextualizadas, 1):\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(f\"📋 PERGUNTA {i}: {pergunta}\")\n",
        "            print(\"=\"*70)\n",
        "            \n",
        "            # Criar prompt contextualizado\n",
        "            prompt_contextualizado = f\"\"\"\n",
        "            Analise a imagem fornecida considerando o seguinte contexto de imagens similares:\n",
        "            \n",
        "            CONTEXTO DE IMAGENS SIMILARES ENCONTRADAS:\n",
        "            {contexto_texto}\n",
        "            \n",
        "            PERGUNTA ESPECÍFICA:\n",
        "            {pergunta}\n",
        "            \n",
        "            Por favor, forneça uma resposta detalhada baseada tanto na análise visual da imagem \n",
        "            quanto no contexto das imagens similares fornecido acima.\n",
        "            \"\"\"\n",
        "            \n",
        "            try:\n",
        "                resposta = get_gemini_response(\n",
        "                    multimodal_model_2_0_flash,\n",
        "                    model_input=[prompt_contextualizado, m3_image],\n",
        "                    stream=False,\n",
        "                )\n",
        "                \n",
        "                print(f\"🤖 RESPOSTA CONTEXTUALIZADA:\")\n",
        "                print(f\"{resposta}\")\n",
        "                \n",
        "            except Exception as gemini_error:\n",
        "                print(f\"❌ Erro na análise contextual: {gemini_error}\")\n",
        "                \n",
        "                # Fallback: análise simples sem contexto\n",
        "                try:\n",
        "                    resposta_simples = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[pergunta, m3_image],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"🤖 RESPOSTA SIMPLES (sem contexto):\")\n",
        "                    print(f\"{resposta_simples}\")\n",
        "                    \n",
        "                except Exception as simple_error:\n",
        "                    print(f\"❌ Erro na análise simples: {simple_error}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Erro ao carregar imagem: {e}\")\n",
        "    \n",
        "    # Mostrar resumo final\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"📊 RESUMO DOS RESULTADOS DE SIMILARIDADE:\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, result in enumerate(matching_results, 1):\n",
        "        print(f\"\\n🖼️  Imagem Similar {i}:\")\n",
        "        print(f\"  📈 Similaridade: {result.get('cosine_score', 0):.4f}\")\n",
        "        print(f\"  📁 Arquivo: {result.get('file_name', 'N/A')}\")\n",
        "        print(f\"  📄 Página: {result.get('page_num', 'N/A')}\")\n",
        "        print(f\"  📂 Caminho: {result.get('img_path', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n✅ ANÁLISE CONTEXTUAL CONCLUÍDA!\")\n",
        "print(f\"A M3.jpeg foi analisada usando o contexto das imagens similares encontradas via embedding.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DIRECT M3.JPEG ANALYSIS WITH GEMINI ===\n",
            "\n",
            "✅ Gemini 2.0 Flash model available\n",
            "🔍 Analyzing image with Gemini...\n",
            "✅ Image loaded: images/B2_room.jpeg\n",
            "\n",
            "📋 QUESTION 1: What are the rooms or areas shown in this floor plan?\n",
            "------------------------------------------------------------\n",
            "🤖 RESPONSE:\n",
            "Based on the floor plan, here's a breakdown of the rooms and areas:\n",
            "\n",
            "*   **Rooms/Offices:** Rooms numbered 2001 through 2012, 2015, 2025, and 2029, and 2032 through 2037.\n",
            "*   **Washrooms:** Washrooms, Women's Washroom, Accessible Washrooms, Men's Washroom, Accessible Shower & Changeroom\n",
            "*   **Elevator:** Shows the location of the elevators.\n",
            "*   **Stairs:** It also shows where the stairs are located.\n",
            "*   **Bus Stop:** Shown on the amenities legend\n",
            "*   **Food:** Shown on the amenities legend\n",
            "*   **Bike Parking:** Shown on the amenities legend\n",
            "*   **Interior and Exterior Pathways** Shown on the amenities legend\n",
            "\n",
            "Other areas include a \"You Are Here\" indicator, question mark icon (presumably indicating information), and areas labeled \"T\" and \"D.\"\n",
            "\n",
            "📋 QUESTION 2: How can I go from room 2001 to room 2037?\n",
            "------------------------------------------------------------\n",
            "🤖 RESPONSE:\n",
            "From room 2001, exit the room and turn left (north on the map).  Walk along the hallway, passing rooms 2003, 2004, 2006, 2010, 2012, 2007, 2005, and 2002 on your right. Then turn left. Room 2037 will be the first door on your left.\n",
            "\n",
            "✅ Analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# CELL 72 - 🤖 DIRECT M3.JPEG ANALYSIS WITH GEMINI (CORRECTED)\n",
        "# Corrected function to ask for figure details using the Gemini model\n",
        "\n",
        "print(\"=== DIRECT M3.JPEG ANALYSIS WITH GEMINI ===\\n\")\n",
        "\n",
        "def ask_figure_details_corrected(model, image_path=None):\n",
        "    \"\"\"\n",
        "    Automatically asks for figure details from the multimodal Gemini model.\n",
        "    Args:\n",
        "        model: loaded Gemini model\n",
        "        image_path: path to the image (string)\n",
        "    \"\"\"\n",
        "    print(\"🔍 Analyzing image with Gemini...\")\n",
        "    \n",
        "    # Specific questions about the image\n",
        "    questions = [\n",
        "        \"What are the rooms or areas shown in this floor plan?\",\n",
        "        \"How can I go from room 2001 to room 2037?\"\n",
        "    ]\n",
        "    try:\n",
        "        # Load the image\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        image_object = GeminiImage.load_from_file(image_path)\n",
        "        print(f\"✅ Image loaded: {image_path}\")\n",
        "        \n",
        "        # Ask each question\n",
        "        for i, question in enumerate(questions, 1):\n",
        "            print(f\"\\n📋 QUESTION {i}: {question}\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            try:\n",
        "                # Use the model directly (most reliable method)\n",
        "                response = model.generate_content([question, image_object])\n",
        "                response_text = response.text if hasattr(response, 'text') else str(response)\n",
        "                \n",
        "                print(f\"🤖 RESPONSE:\")\n",
        "                print(f\"{response_text}\")\n",
        "                \n",
        "            except Exception as question_error:\n",
        "                print(f\"❌ Error in question {i}: {question_error}\")\n",
        "                \n",
        "                # Try alternative method with get_gemini_response\n",
        "                try:\n",
        "                    alt_response = get_gemini_response(\n",
        "                        model,\n",
        "                        model_input=[question, image_object],\n",
        "                        stream=False\n",
        "                    )\n",
        "                    print(f\"🤖 RESPONSE (alternative method):\")\n",
        "                    print(f\"{alt_response}\")\n",
        "                except Exception as alt_error:\n",
        "                    print(f\"❌ Alternative method also failed: {alt_error}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ General error analyzing image: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Test the corrected function automatically\n",
        "try:\n",
        "    # Check if the model is available\n",
        "    if 'multimodal_model_2_0_flash' in locals():\n",
        "        print(\"✅ Gemini 2.0 Flash model available\")\n",
        "        \n",
        "        # Test with M3.jpeg\n",
        "        ask_figure_details_corrected(\n",
        "            multimodal_model_2_0_flash, \n",
        "            image_path=\"images/B2_room.jpeg\"\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        print(\"❌ multimodal_model_2_0_flash model not found\")\n",
        "        print(\"Execute the model setup cells first\")\n",
        "        \n",
        "        # Try to load basic model\n",
        "        try:\n",
        "            from vertexai.generative_models import GenerativeModel\n",
        "            temp_model = GenerativeModel(\"gemini-1.5-flash\")\n",
        "            print(\"🔄 Using Gemini 1.5 Flash model as alternative...\")\n",
        "            \n",
        "            ask_figure_details_corrected(\n",
        "                temp_model,\n",
        "                image_path=\"images/M3.jpeg\"\n",
        "            )\n",
        "        except Exception as model_error:\n",
        "            print(f\"❌ Error loading alternative model: {model_error}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error executing analysis: {e}\")\n",
        "    print(\"Check if:\")\n",
        "    print(\"- The image 'images/M3.jpeg' exists\")\n",
        "    print(\"- Vertex AI models are configured\")\n",
        "    print(\"- Dependencies are installed\")\n",
        "\n",
        "print(f\"\\n✅ Analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Função 'processar_imagens_da_pasta' criada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 75 (NOVO) - 📂 PROCESSAMENTO DIRETO DE IMAGENS DA PASTA\n",
        "# Função para ler todas as imagens da pasta images/ e gerar embeddings para RAG\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_image_embedding_from_multimodal_embedding_model,\n",
        "    get_gemini_response\n",
        ")\n",
        "\n",
        "def processar_imagens_da_pasta(\n",
        "    pasta_imagens=\"images/\",\n",
        "    embedding_size=512,\n",
        "    gerar_descricoes=True,\n",
        "    formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "):\n",
        "    \"\"\"\n",
        "    Processa todas as imagens de uma pasta, gerando embeddings e descrições para RAG\n",
        "    \n",
        "    Args:\n",
        "        pasta_imagens: Caminho da pasta com imagens\n",
        "        embedding_size: Tamanho do embedding (128, 256, 512, 1408)\n",
        "        gerar_descricoes: Se deve gerar descrições das imagens com Gemini\n",
        "        formatos_suportados: Lista de formatos de imagem aceitos\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame compatível com o sistema RAG existente\n",
        "    \"\"\"\n",
        "    print(f\"🔍 PROCESSANDO IMAGENS DA PASTA: {pasta_imagens}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Verificar se a pasta existe\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        print(f\"❌ Pasta '{pasta_imagens}' não encontrada!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Encontrar todas as imagens na pasta\n",
        "    imagens_encontradas = []\n",
        "    for formato in formatos_suportados:\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato.upper()}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "    \n",
        "    # Remover duplicatas\n",
        "    imagens_encontradas = list(set(imagens_encontradas))\n",
        "    \n",
        "    if not imagens_encontradas:\n",
        "        print(f\"❌ Nenhuma imagem encontrada na pasta '{pasta_imagens}'\")\n",
        "        print(f\"Formatos suportados: {formatos_suportados}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"📊 Encontradas {len(imagens_encontradas)} imagens:\")\n",
        "    for img in imagens_encontradas:\n",
        "        print(f\"  - {os.path.basename(img)}\")\n",
        "    \n",
        "    # Lista para armazenar dados processados\n",
        "    dados_imagens = []\n",
        "    \n",
        "    # Prompt para descrição das imagens\n",
        "    prompt_descricao = \"\"\"Analise esta imagem detalhadamente e forneça uma descrição precisa.\n",
        "    Inclua:\n",
        "    - O que você vê na imagem\n",
        "    - Elementos principais e detalhes importantes\n",
        "    - Texto visível (se houver)\n",
        "    - Tipo de imagem (mapa, diagrama, foto, etc.)\n",
        "    - Informações relevantes para busca e recuperação\n",
        "    \n",
        "    Seja específico e detalhado para facilitar buscas futuras.\"\"\"\n",
        "    \n",
        "    print(f\"\\n🚀 PROCESSANDO CADA IMAGEM...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, caminho_imagem in enumerate(imagens_encontradas, 1):\n",
        "        nome_arquivo = os.path.basename(caminho_imagem)\n",
        "        print(f\"\\n📸 PROCESSANDO {i}/{len(imagens_encontradas)}: {nome_arquivo}\")\n",
        "        \n",
        "        try:\n",
        "            # 1. Gerar embedding da imagem\n",
        "            print(\"  🔄 Gerando embedding...\")\n",
        "            image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
        "                image_uri=caminho_imagem,\n",
        "                embedding_size=embedding_size,\n",
        "                return_array=True\n",
        "            )\n",
        "            print(f\"  ✅ Embedding gerado: shape {image_embedding.shape}\")\n",
        "            \n",
        "            # 2. Gerar descrição da imagem (se solicitado)\n",
        "            descricao = \"\"\n",
        "            if gerar_descricoes:\n",
        "                print(\"  🤖 Gerando descrição com Gemini...\")\n",
        "                try:\n",
        "                    from vertexai.generative_models import Image as GeminiImage\n",
        "                    imagem_gemini = GeminiImage.load_from_file(caminho_imagem)\n",
        "                    \n",
        "                    descricao = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[prompt_descricao, imagem_gemini],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"  ✅ Descrição gerada: {len(descricao)} caracteres\")\n",
        "                    \n",
        "                except Exception as desc_error:\n",
        "                    print(f\"  ⚠️  Erro ao gerar descrição: {desc_error}\")\n",
        "                    descricao = f\"Imagem: {nome_arquivo}\"\n",
        "            \n",
        "            # 3. Gerar embedding da descrição (para compatibilidade com RAG)\n",
        "            text_embedding = None\n",
        "            if descricao:\n",
        "                try:\n",
        "                    from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
        "                    text_embedding = get_text_embedding_from_text_embedding_model(descricao)\n",
        "                    print(\"  ✅ Text embedding da descrição gerado\")\n",
        "                except Exception as text_emb_error:\n",
        "                    print(f\"  ⚠️  Erro ao gerar text embedding: {text_emb_error}\")\n",
        "            \n",
        "            # 4. Criar registro compatível com o sistema existente\n",
        "            registro = {\n",
        "                'file_name': f\"pasta_images_{nome_arquivo}\",  # Nome único\n",
        "                'page_num': 1,  # Imagens individuais = página 1\n",
        "                'img_num': i,\n",
        "                'img_path': caminho_imagem,\n",
        "                'img_desc': descricao,\n",
        "                'mm_embedding_from_img_only': image_embedding.tolist(),  # Compatibilidade\n",
        "                'text_embedding_from_image_description': text_embedding if text_embedding else None,\n",
        "                'source_type': 'pasta_imagens',  # Identificar origem\n",
        "                'original_filename': nome_arquivo\n",
        "            }\n",
        "            \n",
        "            dados_imagens.append(registro)\n",
        "            print(f\"  ✅ Processamento concluído para {nome_arquivo}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Erro ao processar {nome_arquivo}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    if dados_imagens:\n",
        "        df_imagens = pd.DataFrame(dados_imagens)\n",
        "        print(f\"\\n🎉 PROCESSAMENTO CONCLUÍDO!\")\n",
        "        print(f\"📊 DataFrame criado com {len(df_imagens)} imagens processadas\")\n",
        "        print(f\"📋 Colunas: {list(df_imagens.columns)}\")\n",
        "        \n",
        "        return df_imagens\n",
        "    else:\n",
        "        print(f\"\\n❌ Nenhuma imagem foi processada com sucesso\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"✅ Função 'processar_imagens_da_pasta' criada com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\n",
            "\n",
            "🔍 PROCESSANDO IMAGENS DA PASTA: images/\n",
            "============================================================\n",
            "📊 Encontradas 1 imagens:\n",
            "  - M3.jpeg\n",
            "\n",
            "🚀 PROCESSANDO CADA IMAGEM...\n",
            "============================================================\n",
            "\n",
            "📸 PROCESSANDO 1/1: M3.jpeg\n",
            "  🔄 Gerando embedding...\n",
            "  ✅ Embedding gerado: shape (512,)\n",
            "  🤖 Gerando descrição com Gemini...\n",
            "  ⚠️  Erro ao gerar descrição: 'GenerationResponse' object is not iterable\n",
            "  ✅ Text embedding da descrição gerado\n",
            "  ✅ Processamento concluído para M3.jpeg\n",
            "\n",
            "🎉 PROCESSAMENTO CONCLUÍDO!\n",
            "📊 DataFrame criado com 1 imagens processadas\n",
            "📋 Colunas: ['file_name', 'page_num', 'img_num', 'img_path', 'img_desc', 'mm_embedding_from_img_only', 'text_embedding_from_image_description', 'source_type', 'original_filename']\n",
            "\n",
            "🎉 SUCESSO TOTAL!\n",
            "📊 image_metadata_df criado com 1 imagens\n",
            "\n",
            "📋 RESUMO DAS IMAGENS PROCESSADAS:\n",
            "==================================================\n",
            "\n",
            "🖼️  Imagem 1:\n",
            "  📁 Arquivo: M3.jpeg\n",
            "  📂 Caminho: images/M3.jpeg\n",
            "  📊 Embedding shape: 512\n",
            "  📝 Descrição: Imagem: M3.jpeg\n",
            "\n",
            "✅ COMPATIBILIDADE COM SISTEMA RAG:\n",
            "  ✅ img_path: OK\n",
            "  ✅ mm_embedding_from_img_only: OK\n",
            "  ✅ img_desc: OK\n",
            "  ✅ file_name: OK\n",
            "  ✅ page_num: OK\n",
            "\n",
            "💾 DataFrame salvo em 'image_metadata_from_folder.pkl'\n",
            "\n",
            "🚀 PRÓXIMOS PASSOS:\n",
            "1. Agora você pode executar a CÉLULA 70 (Validação)\n",
            "2. Depois executar a CÉLULA 71 (Análise Contextual)\n",
            "3. O sistema RAG está pronto para perguntas sobre as imagens!\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 76 (EXECUTAR) - 🚀 PROCESSAMENTO DAS IMAGENS DA PASTA images/\n",
        "# Executa o processamento de todas as imagens e cria o image_metadata_df\n",
        "\n",
        "print(\"=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\\n\")\n",
        "\n",
        "# Executar o processamento das imagens\n",
        "try:\n",
        "    image_metadata_df = processar_imagens_da_pasta(\n",
        "        pasta_imagens=\"images/\",\n",
        "        embedding_size=512,\n",
        "        gerar_descricoes=True,  # Gerar descrições detalhadas com Gemini\n",
        "        formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "    )\n",
        "    \n",
        "    if not image_metadata_df.empty:\n",
        "        print(f\"\\n🎉 SUCESSO TOTAL!\")\n",
        "        print(f\"📊 image_metadata_df criado com {len(image_metadata_df)} imagens\")\n",
        "        \n",
        "        # Mostrar resumo das imagens processadas\n",
        "        print(f\"\\n📋 RESUMO DAS IMAGENS PROCESSADAS:\")\n",
        "        print(\"=\"*50)\n",
        "        for idx, row in image_metadata_df.iterrows():\n",
        "            print(f\"\\n🖼️  Imagem {idx + 1}:\")\n",
        "            print(f\"  📁 Arquivo: {row['original_filename']}\")\n",
        "            print(f\"  📂 Caminho: {row['img_path']}\")\n",
        "            print(f\"  📊 Embedding shape: {len(row['mm_embedding_from_img_only'])}\")\n",
        "            \n",
        "            # Mostrar início da descrição\n",
        "            desc = row['img_desc']\n",
        "            if desc and len(desc) > 10:\n",
        "                print(f\"  📝 Descrição: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
        "        \n",
        "        # Verificar compatibilidade com sistema RAG existente\n",
        "        print(f\"\\n✅ COMPATIBILIDADE COM SISTEMA RAG:\")\n",
        "        colunas_necessarias = ['img_path', 'mm_embedding_from_img_only', 'img_desc', 'file_name', 'page_num']\n",
        "        for col in colunas_necessarias:\n",
        "            if col in image_metadata_df.columns:\n",
        "                print(f\"  ✅ {col}: OK\")\n",
        "            else:\n",
        "                print(f\"  ❌ {col}: FALTANDO\")\n",
        "        \n",
        "        # Salvar para uso futuro (opcional)\n",
        "        try:\n",
        "            image_metadata_df.to_pickle(\"image_metadata_from_folder.pkl\")\n",
        "            print(f\"\\n💾 DataFrame salvo em 'image_metadata_from_folder.pkl'\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"\\n⚠️  Não foi possível salvar: {save_error}\")\n",
        "        \n",
        "        print(f\"\\n🚀 PRÓXIMOS PASSOS:\")\n",
        "        print(f\"1. Agora você pode executar a CÉLULA 70 (Validação)\")\n",
        "        print(f\"2. Depois executar a CÉLULA 71 (Análise Contextual)\")\n",
        "        print(f\"3. O sistema RAG está pronto para perguntas sobre as imagens!\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n❌ FALHA: Nenhuma imagem foi processada\")\n",
        "        print(f\"Verifique se:\")\n",
        "        print(f\"- A pasta 'images/' existe\")\n",
        "        print(f\"- Há imagens válidas na pasta\")\n",
        "        print(f\"- Os modelos estão carregados corretamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ ERRO NO PROCESSAMENTO: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\n💡 POSSÍVEIS SOLUÇÕES:\")\n",
        "    print(f\"- Verifique se os modelos estão carregados\")\n",
        "    print(f\"- Verifique se a pasta 'images/' existe\")\n",
        "    print(f\"- Execute as células de setup dos modelos primeiro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\n",
            "\n",
            "📊 Imagens atuais na pasta: 1\n",
            "🔄 Extraindo imagens do PDF para ter mais dados...\n",
            "🔍 Processando PDF: map/map.pdf\n",
            "📊 PDF tem 1 páginas\n",
            "📄 Página 1: 0 imagens encontradas\n",
            "\n",
            "🎉 Total de 0 imagens extraídas!\n",
            "❌ Nenhuma imagem foi extraída do PDF\n",
            "\n",
            "📊 STATUS FINAL: 1 imagens na pasta 'images/'\n",
            "⚠️  Ainda há apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\n"
          ]
        }
      ],
      "source": [
        "# CÉLULA 78 (OPCIONAL) - 📄 EXTRAIR MAIS IMAGENS DO PDF PARA COMPARAÇÃO\n",
        "# Esta célula extrai imagens do map.pdf para ter mais dados para comparação\n",
        "\n",
        "print(\"=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\\n\")\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def extrair_imagens_do_pdf(pdf_path, output_dir=\"images/\", prefixo=\"map\"):\n",
        "    \"\"\"\n",
        "    Extrai imagens de um PDF e salva na pasta de imagens\n",
        "    \"\"\"\n",
        "    print(f\"🔍 Processando PDF: {pdf_path}\")\n",
        "    \n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"❌ PDF não encontrado: {pdf_path}\")\n",
        "        return []\n",
        "    \n",
        "    # Criar diretório se não existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Abrir PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    imagens_extraidas = []\n",
        "    \n",
        "    print(f\"📊 PDF tem {len(doc)} páginas\")\n",
        "    \n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        images = page.get_images()\n",
        "        \n",
        "        print(f\"📄 Página {page_num + 1}: {len(images)} imagens encontradas\")\n",
        "        \n",
        "        for img_index, img in enumerate(images):\n",
        "            try:\n",
        "                # Extrair imagem\n",
        "                xref = img[0]\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "                \n",
        "                # Converter para RGB se necessário\n",
        "                if pix.colorspace and pix.colorspace.n > 3:\n",
        "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                \n",
        "                # Nome do arquivo\n",
        "                img_filename = f\"{prefixo}_page_{page_num + 1}_img_{img_index + 1}.png\"\n",
        "                img_path = os.path.join(output_dir, img_filename)\n",
        "                \n",
        "                # Salvar imagem\n",
        "                pix.save(img_path)\n",
        "                imagens_extraidas.append(img_path)\n",
        "                \n",
        "                print(f\"  ✅ Extraída: {img_filename}\")\n",
        "                \n",
        "                pix = None  # Liberar memória\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ❌ Erro ao extrair imagem {img_index}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    doc.close()\n",
        "    print(f\"\\n🎉 Total de {len(imagens_extraidas)} imagens extraídas!\")\n",
        "    return imagens_extraidas\n",
        "\n",
        "# Verificar quantas imagens temos atualmente\n",
        "current_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"📊 Imagens atuais na pasta: {current_images}\")\n",
        "\n",
        "if current_images <= 1:\n",
        "    print(\"🔄 Extraindo imagens do PDF para ter mais dados...\")\n",
        "    \n",
        "    # Extrair do map.pdf se existir\n",
        "    if os.path.exists(\"map/map.pdf\"):\n",
        "        imagens_extraidas = extrair_imagens_do_pdf(\"map/map.pdf\", \"images/\", \"map\")\n",
        "        \n",
        "        if imagens_extraidas:\n",
        "            print(f\"\\n✅ {len(imagens_extraidas)} novas imagens adicionadas!\")\n",
        "            print(\"🚀 Agora execute a CÉLULA 76 novamente para processar todas as imagens\")\n",
        "            print(\"   Depois execute a CÉLULA 70 para testar similaridade com mais dados\")\n",
        "        else:\n",
        "            print(\"❌ Nenhuma imagem foi extraída do PDF\")\n",
        "    else:\n",
        "        print(\"❌ Arquivo map/map.pdf não encontrado\")\n",
        "        \n",
        "        # Verificar outros PDFs disponíveis\n",
        "        print(\"\\n🔍 Procurando outros PDFs...\")\n",
        "        pdf_paths = []\n",
        "        for root, dirs, files in os.walk(\".\"):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    pdf_paths.append(os.path.join(root, file))\n",
        "        \n",
        "        if pdf_paths:\n",
        "            print(\"📋 PDFs encontrados:\")\n",
        "            for i, pdf_path in enumerate(pdf_paths[:3], 1):  # Mostrar apenas os 3 primeiros\n",
        "                print(f\"  {i}. {pdf_path}\")\n",
        "                \n",
        "            # Processar o primeiro PDF encontrado\n",
        "            if pdf_paths:\n",
        "                primeiro_pdf = pdf_paths[0]\n",
        "                print(f\"\\n🔄 Processando: {primeiro_pdf}\")\n",
        "                imagens_extraidas = extrair_imagens_do_pdf(primeiro_pdf, \"images/\", \"doc\")\n",
        "                \n",
        "                if imagens_extraidas:\n",
        "                    print(f\"\\n✅ {len(imagens_extraidas)} imagens extraídas de {primeiro_pdf}!\")\n",
        "                    print(\"🚀 Execute a CÉLULA 76 novamente para processar todas as imagens\")\n",
        "        else:\n",
        "            print(\"❌ Nenhum PDF encontrado para extrair imagens\")\n",
        "            \n",
        "else:\n",
        "    print(\"✅ Já há múltiplas imagens na pasta\")\n",
        "    print(\"Execute a CÉLULA 76 para processar todas e depois a CÉLULA 70 para testar similaridade\")\n",
        "\n",
        "# Mostrar status final\n",
        "final_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"\\n📊 STATUS FINAL: {final_images} imagens na pasta 'images/'\")\n",
        "\n",
        "if final_images > 1:\n",
        "    print(\"🎉 Pronto para testar busca por similaridade!\")\n",
        "    print(\"📋 PRÓXIMOS PASSOS:\")\n",
        "    print(\"  1. Execute CÉLULA 76 (processar todas as imagens)\")\n",
        "    print(\"  2. Execute CÉLULA 70 (busca por similaridade)\")\n",
        "    print(\"  3. Execute CÉLULA 71 (análise contextual)\")\n",
        "else:\n",
        "    print(\"⚠️  Ainda há apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== USANDO IMAGE EMBEDDING PARA ANÁLISE DA M3.JPEG ===\n",
            "\n",
            "✅ Image embedding disponível com shape: (512,)\n",
            "Embedding type: <class 'numpy.ndarray'>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/vision_models/_vision_models.py:153: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "❌ Erro ao buscar imagens similares: DataFrame.nlargest() missing 1 required positional argument: 'columns'\n",
            "Vamos tentar uma abordagem alternativa...\n"
          ]
        }
      ],
      "source": [
        "# Como usar o image embedding da M3.jpeg para obter respostas\n",
        "# Vamos demonstrar diferentes formas de usar o embedding gerado\n",
        "\n",
        "print(\"=== USANDO IMAGE EMBEDDING PARA ANÁLISE DA M3.JPEG ===\\n\")\n",
        "\n",
        "# 1. Primeiro, vamos verificar se temos o embedding\n",
        "if 'image_embedding' in locals():\n",
        "    print(f\"✅ Image embedding disponível com shape: {image_embedding.shape}\")\n",
        "    print(f\"Embedding type: {type(image_embedding)}\")\n",
        "else:\n",
        "    print(\"❌ Image embedding não foi gerado ainda. Execute a célula anterior primeiro.\")\n",
        "\n",
        "# 2. Buscar imagens similares usando o embedding\n",
        "try:\n",
        "    # Usando a função get_similar_image_from_query com image_emb=True\n",
        "    matching_results = get_similar_image_from_query(\n",
        "        text_metadata_df,\n",
        "        image_metadata_df,\n",
        "        query=\"What are the rooms in this floor?\",  # Query de texto opcional\n",
        "        column_name=\"mm_embedding_from_img_only\",  # Usar embeddings de imagem\n",
        "        image_emb=True,  # Usar embedding de imagem para comparação\n",
        "        image_query_path=\"images/M3.jpeg\",  # Caminho da imagem de consulta\n",
        "        top_n=3,  # Top 3 resultados mais similares\n",
        "        embedding_size=512,  # Tamanho do embedding usado\n",
        "    )\n",
        "    \n",
        "    print(\"\\n🔍 IMAGENS SIMILARES ENCONTRADAS:\")\n",
        "    for i, result in enumerate(matching_results):\n",
        "        print(f\"\\nResultado {i+1}:\")\n",
        "        print(f\"  - Score de similaridade: {result.get('cosine_score', 'N/A'):.4f}\")\n",
        "        print(f\"  - Arquivo: {result.get('file_name', 'N/A')}\")\n",
        "        print(f\"  - Página: {result.get('page_num', 'N/A')}\")\n",
        "        print(f\"  - Caminho da imagem: {result.get('img_path', 'N/A')}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erro ao buscar imagens similares: {e}\")\n",
        "    print(\"Vamos tentar uma abordagem alternativa...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== ANÁLISE DIRETA DA IMAGEM M3.JPEG COM GEMINI ===\n",
            "\n",
            "🤖 ANÁLISE COM GEMINI:\n",
            "\n",
            "📋 Pergunta 1: O que você vê nesta imagem? Descreva detalhadamente.\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[53]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Usar get_gemini_response para analisar a imagem\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m response = \u001b[43mget_gemini_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultimodal_model_2_0_flash\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Usar o modelo Gemini 2.0 Flash\u001b[39;49;00m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_for_analysis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Não usar streaming para melhor formatação\u001b[39;49;00m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m💬 Resposta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/multimodal_qa_with_rag_utils.py:350\u001b[39m, in \u001b[36mget_gemini_response\u001b[39m\u001b[34m(generative_multimodal_model, model_input, stream, generation_config, safety_settings, print_exception)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_gemini_response\u001b[39m(\n\u001b[32m    326\u001b[39m     generative_multimodal_model,\n\u001b[32m    327\u001b[39m     model_input: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    338\u001b[39m     print_exception: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    339\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    340\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[33;03m    This function generates text in response to a list of model inputs.\u001b[39;00m\n\u001b[32m    342\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    348\u001b[39m \u001b[33;03m        The generated text as a string.\u001b[39;00m\n\u001b[32m    349\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m     response = \u001b[43mgenerative_multimodal_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m     response_list = []\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m response:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:710\u001b[39m, in \u001b[36m_GenerativeModel.generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, tools, tool_config, labels, stream)\u001b[39m\n\u001b[32m    701\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate_content_streaming(\n\u001b[32m    702\u001b[39m         contents=contents,\n\u001b[32m    703\u001b[39m         generation_config=generation_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    707\u001b[39m         labels=labels,\n\u001b[32m    708\u001b[39m     )\n\u001b[32m    709\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:833\u001b[39m, in \u001b[36m_GenerativeModel._generate_content\u001b[39m\u001b[34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generates content.\u001b[39;00m\n\u001b[32m    807\u001b[39m \n\u001b[32m    808\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m \u001b[33;03m    A single GenerationResponse object\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    825\u001b[39m request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m    826\u001b[39m     contents=contents,\n\u001b[32m    827\u001b[39m     generation_config=generation_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    831\u001b[39m     labels=labels,\n\u001b[32m    832\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m gapic_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prediction_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_response(gapic_response)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:2302\u001b[39m, in \u001b[36mPredictionServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m   2299\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m   2301\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2307\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2309\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m   2310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(callable_)\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34merror_remapped_callable\u001b[39m(*args, **kwargs):\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:329\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _FailureOutcome(exception, sys.exc_info()[\u001b[32m2\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interceptor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintercept_unary_unary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontinuation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m call.result(), call\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/prediction_service/transports/grpc.py:83\u001b[39m, in \u001b[36m_LoggingClientInterceptor.intercept_unary_unary\u001b[39m\u001b[34m(self, continuation, client_call_details, request)\u001b[39m\n\u001b[32m     69\u001b[39m     grpc_request = {\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpayload\u001b[39m\u001b[33m\"\u001b[39m: request_payload,\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrequestMethod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mgrpc\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(request_metadata),\n\u001b[32m     73\u001b[39m     }\n\u001b[32m     74\u001b[39m     _LOGGER.debug(\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSending request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclient_call_details.method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m         extra={\n\u001b[32m   (...)\u001b[39m\u001b[32m     81\u001b[39m         },\n\u001b[32m     82\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m response = \u001b[43mcontinuation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_call_details\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_enabled:  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n\u001b[32m     85\u001b[39m     response_metadata = response.trailing_metadata()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    306\u001b[39m (\n\u001b[32m    307\u001b[39m     new_method,\n\u001b[32m    308\u001b[39m     new_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     new_compression,\n\u001b[32m    313\u001b[39m ) = _unwrap_client_call_details(new_details, client_call_details)\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m rpc_error:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:1192\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1183\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_call\u001b[39m(\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1185\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1190\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1191\u001b[39m ) -> Tuple[Any, grpc.Call]:\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     state, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_blocking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:1165\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._blocking\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1148\u001b[39m state.target = _common.decode(\u001b[38;5;28mself\u001b[39m._target)\n\u001b[32m   1149\u001b[39m call = \u001b[38;5;28mself\u001b[39m._channel.segregated_call(\n\u001b[32m   1150\u001b[39m     cygrpc.PropagationConstants.GRPC_PROPAGATE_DEFAULTS,\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28mself\u001b[39m._method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1163\u001b[39m     \u001b[38;5;28mself\u001b[39m._registered_call_handle,\n\u001b[32m   1164\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1165\u001b[39m event = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnext_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1166\u001b[39m _handle_event(event, state, \u001b[38;5;28mself\u001b[39m._response_deserializer)\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m state, call\n",
            "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:388\u001b[39m, in \u001b[36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:211\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:205\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next_call_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:97\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:80\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._internal_latent_event\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[39m, in \u001b[36mgrpc._cython.cygrpc._next\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Método alternativo: Análise direta da imagem M3.jpeg usando Gemini\n",
        "# Esta abordagem usa diretamente o modelo Gemini para analisar a imagem\n",
        "\n",
        "print(\"\\n=== ANÁLISE DIRETA DA IMAGEM M3.JPEG COM GEMINI ===\\n\")\n",
        "\n",
        "try:\n",
        "    # Carregar a imagem M3.jpeg\n",
        "    from vertexai.generative_models import Image as GeminiImage\n",
        "    \n",
        "    # Carregar a imagem\n",
        "    image_for_analysis = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "    \n",
        "    # Definir perguntas específicas sobre a imagem\n",
        "    questions = [\n",
        "        \"O que você vê nesta imagem? Descreva detalhadamente.\",\n",
        "        \"Quais são os elementos principais desta imagem?\", \n",
        "        \"Se esta é uma imagem de um mapa, quais edifícios ou locais você consegue identificar?\",\n",
        "        \"Há algum texto ou legenda visível na imagem?\"\n",
        "    ]\n",
        "    \n",
        "    print(\"🤖 ANÁLISE COM GEMINI:\")\n",
        "    \n",
        "    for i, question in enumerate(questions, 1):\n",
        "        print(f\"\\n📋 Pergunta {i}: {question}\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        # Usar get_gemini_response para analisar a imagem\n",
        "        response = get_gemini_response(\n",
        "            multimodal_model_2_0_flash,  # Usar o modelo Gemini 2.0 Flash\n",
        "            model_input=[question, image_for_analysis],\n",
        "            stream=False,  # Não usar streaming para melhor formatação\n",
        "        )\n",
        "        \n",
        "        print(f\"💬 Resposta: {response}\")\n",
        "        print()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"❌ Erro na análise com Gemini: {e}\")\n",
        "    print(\"Verifique se o modelo multimodal_model_2_0_flash está disponível.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== SOLUÇÃO DEFINITIVA ===\n",
            "\n",
            "🗑️  Removendo multimodal_qa_with_rag_utils do cache...\n",
            "📥 Reimportando módulo...\n",
            "✅ Módulo reimportado com sucesso!\n",
            "✅ Correção confirmada no código!\n",
            "\n",
            "🎯 TESTE FINAL COM M3.JPEG...\n",
            "❌ Variáveis necessárias não encontradas:\n",
            "Faltando: ['image_embedding', 'text_metadata_df', 'image_metadata_df']\n",
            "Execute as células de carregamento de dados primeiro.\n",
            "\n",
            "🎯 RESUMO:\n",
            "- Se viu 'SUCESSO TOTAL': a correção funcionou!\n",
            "- Se viu 'Método alternativo funcionou': use os resultados obtidos\n",
            "- Se ambos falharam: há um problema mais profundo nos dados\n",
            "- Em qualquer caso, agora você tem resultados para usar!\n"
          ]
        }
      ],
      "source": [
        "# 🚀 SOLUÇÃO DEFINITIVA: Forçar reload e testar\n",
        "# Esta célula força o reload do módulo e testa definitivamente\n",
        "\n",
        "print(\"=== SOLUÇÃO DEFINITIVA ===\\n\")\n",
        "\n",
        "# 1. Forçar reload completo\n",
        "import sys\n",
        "import importlib\n",
        "\n",
        "# Remover módulo da cache se existir\n",
        "module_name = 'multimodal_qa_with_rag_utils'\n",
        "if module_name in sys.modules:\n",
        "    print(f\"🗑️  Removendo {module_name} do cache...\")\n",
        "    del sys.modules[module_name]\n",
        "\n",
        "# Importar novamente\n",
        "print(\"📥 Reimportando módulo...\")\n",
        "try:\n",
        "    from multimodal_qa_with_rag_utils import (\n",
        "        get_similar_image_from_query,\n",
        "        get_image_embedding_from_multimodal_embedding_model,\n",
        "        get_gemini_response,\n",
        "    )\n",
        "    print(\"✅ Módulo reimportado com sucesso!\")\n",
        "    \n",
        "    # Verificar se a correção está presente\n",
        "    import inspect\n",
        "    source = inspect.getsource(get_similar_image_from_query)\n",
        "    \n",
        "    if \"isinstance(cosine_scores, pd.DataFrame)\" in source:\n",
        "        print(\"✅ Correção confirmada no código!\")\n",
        "    else:\n",
        "        print(\"❌ Correção não encontrada - usando método alternativo\")\n",
        "        \n",
        "except Exception as import_error:\n",
        "    print(f\"❌ Erro na importação: {import_error}\")\n",
        "\n",
        "# 2. TESTE FINAL com a M3.jpeg\n",
        "print(\"\\n🎯 TESTE FINAL COM M3.JPEG...\")\n",
        "\n",
        "if all(var in locals() for var in ['image_embedding', 'text_metadata_df', 'image_metadata_df']):\n",
        "    try:\n",
        "        print(\"🔍 Executando busca por similaridade...\")\n",
        "        \n",
        "        final_results = get_similar_image_from_query(\n",
        "            text_metadata_df,\n",
        "            image_metadata_df,\n",
        "            query=\"What are the rooms in this floor?\",\n",
        "            column_name=\"mm_embedding_from_img_only\",\n",
        "            image_emb=True,\n",
        "            image_query_path=\"images/M3.jpeg\",\n",
        "            top_n=3,\n",
        "            embedding_size=512,\n",
        "        )\n",
        "        \n",
        "        print(\"🎉 SUCESSO TOTAL! Busca funcionou perfeitamente!\")\n",
        "        print(f\"📊 Encontradas {len(final_results)} imagens similares:\\n\")\n",
        "        \n",
        "        for i, result in enumerate(final_results):\n",
        "            print(f\"🖼️  Resultado {i+1}:\")\n",
        "            print(f\"  📈 Similaridade: {result.get('cosine_score', 'N/A'):.4f}\")\n",
        "            print(f\"  📁 Arquivo: {result.get('file_name', 'N/A')}\")\n",
        "            print(f\"  📄 Página: {result.get('page_num', 'N/A')}\")\n",
        "            print(f\"  🗂️  Caminho: {result.get('img_path', 'N/A')}\")\n",
        "            \n",
        "            # Mostrar descrição se disponível\n",
        "            desc = result.get('image_description', result.get('img_desc', ''))\n",
        "            if desc and desc != 'N/A':\n",
        "                print(f\"  📝 Descrição: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
        "            print()\n",
        "        \n",
        "        # Salvar resultados para uso posterior\n",
        "        globals()['matching_results'] = final_results\n",
        "        \n",
        "        print(\"💾 Resultados salvos na variável 'matching_results'\")\n",
        "        print(\"✅ Agora você pode usar a célula 76 para fazer perguntas contextualizadas!\")\n",
        "        \n",
        "    except Exception as final_error:\n",
        "        print(f\"❌ Erro no teste final: {final_error}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        \n",
        "        print(\"\\n🔄 USANDO MÉTODO ALTERNATIVO...\")\n",
        "        \n",
        "        # Usar função alternativa se a original ainda falhar\n",
        "        try:\n",
        "            if 'buscar_imagens_similares_simples' in locals():\n",
        "                alt_results = buscar_imagens_similares_simples(\n",
        "                    image_embedding, \n",
        "                    image_metadata_df, \n",
        "                    top_n=3\n",
        "                )\n",
        "                \n",
        "                if alt_results:\n",
        "                    print(\"✅ Método alternativo funcionou!\")\n",
        "                    print(f\"📊 Encontrados {len(alt_results)} resultados alternativos:\")\n",
        "                    \n",
        "                    for i, result in enumerate(alt_results):\n",
        "                        print(f\"\\n🖼️  Resultado {i+1}:\")\n",
        "                        print(f\"  📈 Similaridade: {result.get('cosine_score', 'N/A'):.4f}\")\n",
        "                        print(f\"  📁 Arquivo: {result.get('file_name', 'N/A')}\")\n",
        "                        print(f\"  📄 Página: {result.get('page_num', 'N/A')}\")\n",
        "                    \n",
        "                    globals()['matching_results'] = alt_results\n",
        "                    print(\"💾 Resultados alternativos salvos!\")\n",
        "                    \n",
        "            else:\n",
        "                print(\"❌ Função alternativa não disponível. Execute a célula 74 primeiro.\")\n",
        "                \n",
        "        except Exception as alt_error:\n",
        "            print(f\"❌ Método alternativo também falhou: {alt_error}\")\n",
        "            \n",
        "else:\n",
        "    print(\"❌ Variáveis necessárias não encontradas:\")\n",
        "    missing = [var for var in ['image_embedding', 'text_metadata_df', 'image_metadata_df'] if var not in locals()]\n",
        "    print(f\"Faltando: {missing}\")\n",
        "    print(\"Execute as células de carregamento de dados primeiro.\")\n",
        "\n",
        "print(\"\\n🎯 RESUMO:\")\n",
        "print(\"- Se viu 'SUCESSO TOTAL': a correção funcionou!\")\n",
        "print(\"- Se viu 'Método alternativo funcionou': use os resultados obtidos\")\n",
        "print(\"- Se ambos falharam: há um problema mais profundo nos dados\")\n",
        "print(\"- Em qualquer caso, agora você tem resultados para usar!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zBTtGChTmrd"
      },
      "source": [
        "You expect to find images that are similar in terms of \"long context prompts for gemini 1.5 pro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search for Similar Images Based on Input Image and Image Embedding\n",
        "\n",
        "matching_results_image = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,  # Use query text for additional filtering (optional)\n",
        "    column_name=\"mm_embedding_from_img_only\",  # Use image embedding for similarity calculation\n",
        "    image_emb=True,\n",
        "    image_query_path=image_query_path,  # Use input image for similarity calculation\n",
        "    top_n=3,  # Retrieve top 3 matching images\n",
        "    embedding_size=1408,  # Use embedding size of 1408\n",
        ")\n",
        "\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "# Display the Top Matching Image\n",
        "display(\n",
        "    matching_results_image[0][\"image_object\"]\n",
        ")  # Display the top matching image object (Pillow Image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "id": "nZcU7vZC-8vr",
        "outputId": "f18c0e6a-38fb-4ffb-8069-610d499e56c6",
        "tags": []
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "DataFrame.nlargest() missing 1 required positional argument: 'columns'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Search for Similar Images Based on Input Image and Image Embedding\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m matching_results_image = \u001b[43mget_similar_image_from_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_metadata_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_metadata_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use query text for additional filtering (optional)\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmm_embedding_from_img_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use image embedding for similarity calculation\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_query_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_query_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use input image for similarity calculation\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Retrieve top 3 matching images\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1408\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use embedding size of 1408\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m **** Result: ***** \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Display the Top Matching Image\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/multimodal_qa_with_rag_utils.py:816\u001b[39m, in \u001b[36mget_similar_image_from_query\u001b[39m\u001b[34m(text_metadata_df, image_metadata_df, query, image_query_path, column_name, image_emb, top_n, embedding_size)\u001b[39m\n\u001b[32m    813\u001b[39m cosine_scores = cosine_scores[cosine_scores < \u001b[32m1.0\u001b[39m]\n\u001b[32m    815\u001b[39m \u001b[38;5;66;03m# Get top N cosine scores and their indices\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m816\u001b[39m top_n_cosine_scores = \u001b[43mcosine_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnlargest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m)\u001b[49m.index.tolist()\n\u001b[32m    817\u001b[39m top_n_cosine_values = cosine_scores.nlargest(top_n).values.tolist()\n\u001b[32m    819\u001b[39m \u001b[38;5;66;03m# Create a dictionary to store matched images and their information\u001b[39;00m\n",
            "\u001b[31mTypeError\u001b[39m: DataFrame.nlargest() missing 1 required positional argument: 'columns'"
          ]
        }
      ],
      "source": [
        "# Search for Similar Images Based on Input Image and Image Embedding\n",
        "\n",
        "matching_results_image = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,  # Use query text for additional filtering (optional)\n",
        "    column_name=\"mm_embedding_from_img_only\",  # Use image embedding for similarity calculation\n",
        "    image_emb=True,\n",
        "    image_query_path=image_query_path,  # Use input image for similarity calculation\n",
        "    top_n=3,  # Retrieve top 3 matching images\n",
        "    embedding_size=1408,  # Use embedding size of 1408\n",
        ")\n",
        "\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "\n",
        "# Display the Top Matching Image\n",
        "display(\n",
        "    matching_results_image[0][\"image_object\"]\n",
        ")  # Display the top matching image object (Pillow Image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhT17rke15XY"
      },
      "source": [
        "\n",
        "You can also print the citation to see what it has matched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mksXQoezweg0",
        "outputId": "c20e6857-bbef-4250-fa8f-e5b437bc7aca",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Display citation details for the top matching image\n",
        "print_text_to_image_citation(\n",
        "    matching_results_image, print_top=True\n",
        ")  # Print citation details for the top matching image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "VJWnhDJwI-uO",
        "outputId": "1d034c2f-870a-488b-d22c-7f4abcfe643f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check Other Matched Images (Optional)\n",
        "# You can access the other two matched images using:\n",
        "\n",
        "print(\"---------------Matched Images------------------\\n\")\n",
        "display_images(\n",
        "    [\n",
        "        matching_results_image[0][\"img_path\"],\n",
        "        matching_results_image[1][\"img_path\"],\n",
        "        matching_results_image[2][\"img_path\"],\n",
        "    ],\n",
        "    resize_ratio=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvwZIgD84CNc"
      },
      "source": [
        "The ability to identify similar text and images based on user input, powered by Gemini and embeddings, forms a crucial foundation for development of Multimodal Question Answering System with multimodal RAG design pattern, which you will explore in the coming sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUnsv5Co6pJF"
      },
      "source": [
        "### Comparative reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AFbqHiz5vvo"
      },
      "source": [
        "Next, let's apply what you have done so far in doing comparative reasoning.\n",
        "\n",
        "For this example:\n",
        "\n",
        "* **Step 1:** You will search all the images for a specific query\n",
        "\n",
        "* **Step 2:** Send those images to Gemini 2.0 Flash to ask multiple questions, where it has to compare among those images and provide you with answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6AHCSwojyX0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "matching_results_image_query_1 = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=\"Show me all the images that can describe llms and tpu v5e scaling\",\n",
        "    column_name=\"text_embedding_from_image_description\",  # Use image description text embedding # mm_embedding_from_img_only text_embedding_from_image_description\n",
        "    image_emb=False,  # Use text embedding instead of image embedding\n",
        "    top_n=5,\n",
        "    embedding_size=1408,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jja-9iGFRNYQ",
        "outputId": "794e97b1-c20e-484e-beeb-6c2311fc2b22",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check Matched Images\n",
        "# You can access the other two matched images using:\n",
        "\n",
        "print(\"---------------Matched Images------------------\\n\")\n",
        "display_images(\n",
        "    [\n",
        "        matching_results_image_query_1[0][\"img_path\"],\n",
        "        matching_results_image_query_1[1][\"img_path\"],\n",
        "        matching_results_image_query_1[2][\"img_path\"],\n",
        "        matching_results_image_query_1[3][\"img_path\"],\n",
        "        matching_results_image_query_1[4][\"img_path\"],\n",
        "    ],\n",
        "    resize_ratio=0.2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSR_JWkSC_7p",
        "tags": []
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points.\n",
        "Instructions:\n",
        "1. Analyze the provided images focusing on the relationship between TPU v5e scaling efficiency, LLM model size growth, performance metrics, and quantization effects.\n",
        "2. Answer the following questions in detail, providing clear reasoning and evidence from the images in bullet points\n",
        "3. Cite the image sources to support your explanations. Mention the file name.\n",
        "\n",
        "Additional Considerations:\n",
        "* Clearly define any technical terms (e.g., EMFU, TFLOP/chip/s) within your answers for better understanding.\n",
        "* Use specific examples and data points from the images to support your explanations.\n",
        "* Feel free to request additional information or clarification if the images are unclear or ambiguous.\n",
        "\n",
        "Question:\n",
        " - How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time?\n",
        " - How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU v5e chips (e.g., 256)?\n",
        " - For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed TFLOP/chip/s?\n",
        " - how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this affect the \"Total observed Perf\"?\n",
        " - how might the MFU be impacted by increasing LLM model size?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wYkzpB4PTSfm",
        "outputId": "d9d03602-2d6b-4389-d2d2-66be50cb1660",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Generate response with Gemini 2.0 Flash\n",
        "print(\"\\n **** Result: ***** \\n\")\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_2_0_flash,\n",
        "        model_input=[\n",
        "            prompt,\n",
        "            \"Images:\",\n",
        "            matching_results_image_query_1[0][\"image_object\"],\n",
        "            matching_results_image_query_1[1][\"image_object\"],\n",
        "            matching_results_image_query_1[2][\"image_object\"],\n",
        "            matching_results_image_query_1[3][\"image_object\"],\n",
        "            matching_results_image_query_1[4][\"image_object\"],\n",
        "        ],\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efJPPrzRhvIT"
      },
      "source": [
        "## Building Multimodal QA System with retrieval augmented generation (mRAG)\n",
        "\n",
        "Let's bring everything together to implement multimodal RAG. You will use all the elements that you've explored in previous sections to implement the multimodal RAG. These are the steps:\n",
        "\n",
        "* **Step 1:** The user gives a query in text format where the expected information is available in the document and is embedded in images and text.\n",
        "* **Step 2:** Find all text chunks from the pages in the documents using a method similar to the one you explored in `Text Search`.\n",
        "* **Step 3:** Find all similar images from the pages based on the user query matched with `image_description` using a method identical to the one you explored in `Image Search`.\n",
        "* **Step 4:** Combine all similar text and images found in steps 2 and 3 as `context_text` and `context_images`.\n",
        "* **Step 5:** With the help of Gemini, we can pass the user query with text and image context found in steps 2 & 3. You can also add a specific instruction the model should remember while answering the user query.\n",
        "* **Step 6:** Gemini produces the answer, and you can print the citations to check all relevant text and images used to address the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI62Hzuw_0_b"
      },
      "source": [
        "### Step 1: User query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvTKFwOPHLQ_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# this time we are not passing any images, but just a simple text query.\n",
        "\n",
        "query = \"\"\"- How does the scaling efficiency of TPU v5e compare to the overall growth in LLM model size over time?\n",
        " - How does the model size impact the observed Per-chip performance and EMFU for a fixed number of TPU v5e chips (e.g., 256)?\n",
        " - For the INT8 Quant training with 32B parameters, how does its high EMFU relate to the observed TFLOP/chip/s?\n",
        " - how does the \"per device batch (seq)\" for a 16B model compare to a 128B model, and how does this affect the \"Total observed Perf\"?\n",
        " - how might the MFU be impacted by increasing LLM model size?\n",
        " \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUqlkKUaYvZA"
      },
      "source": [
        "### Step 2: Get all relevant text chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r65yBb5gR_NG",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Retrieve relevant chunks of text based on the query\n",
        "matching_results_chunks_data = get_similar_text_from_query(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    column_name=\"text_embedding_chunk\",\n",
        "    top_n=20,\n",
        "    chunk_text=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIgXgVIpYzxj"
      },
      "source": [
        "### Step 3: Get all relevant images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzu5Gf4yR_J4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Get all relevant images based on user query\n",
        "matching_results_image_fromdescription_data = get_similar_image_from_query(\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    query=query,\n",
        "    column_name=\"text_embedding_from_image_description\",\n",
        "    image_emb=False,\n",
        "    top_n=10,\n",
        "    embedding_size=1408,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhUpWlGAY2uG"
      },
      "source": [
        "### Step 4: Create context_text and context_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_EEuuLCe6Y5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Task: Answer the following questions in detail, providing clear reasoning and evidence from the images and text in bullet points.\n",
        "Instructions:\n",
        "\n",
        "1. **Analyze:** Carefully examine the provided images and text context.\n",
        "2. **Synthesize:** Integrate information from both the visual and textual elements.\n",
        "3. **Reason:**  Deduce logical connections and inferences to address the question.\n",
        "4. **Respond:** Provide a concise, accurate answer in the following format:\n",
        "\n",
        "   * **Question:** [Question]\n",
        "   * **Answer:** [Direct response to the question]\n",
        "   * **Explanation:** [Bullet-point reasoning steps if applicable]\n",
        "   * **Source** [name of the file, page, image from where the information is citied]\n",
        "\n",
        "5. **Ambiguity:** If the context is insufficient to answer, respond \"Not enough context to answer.\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# combine all the selected relevant text chunks\n",
        "context_text = [\"Text Context: \"]\n",
        "for key, value in matching_results_chunks_data.items():\n",
        "    context_text.extend(\n",
        "        [\n",
        "            \"Text Source: \",\n",
        "            f\"\"\"file_name: \"{value[\"file_name\"]}\" Page: \"{value[\"page_num\"]}\"\"\",\n",
        "            \"Text\",\n",
        "            value[\"chunk_text\"],\n",
        "        ]\n",
        "    )\n",
        "\n",
        "# combine all the selected relevant images\n",
        "gemini_content = [\n",
        "    instruction,\n",
        "    \"Questions: \",\n",
        "    query,\n",
        "    \"Image Context: \",\n",
        "]\n",
        "for key, value in matching_results_image_fromdescription_data.items():\n",
        "    gemini_content.extend(\n",
        "        [\n",
        "            \"Image Path: \",\n",
        "            value[\"img_path\"],\n",
        "            \"Image Description: \",\n",
        "            value[\"image_description\"],\n",
        "            \"Image:\",\n",
        "            value[\"image_object\"],\n",
        "        ]\n",
        "    )\n",
        "gemini_content.extend(context_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHrtodcBAEu9"
      },
      "source": [
        "### Step 5: Pass context to Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aZuhtJu7fW4n",
        "outputId": "e750c25a-e930-4cc1-e67a-453c1645365a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Generate Gemini response with streaming output\n",
        "rich_Markdown(\n",
        "    get_gemini_response(\n",
        "        multimodal_model_2_0_flash,\n",
        "        model_input=gemini_content,\n",
        "        stream=True,\n",
        "        safety_settings=safety_settings,\n",
        "        generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0FtXYl1fzKh"
      },
      "source": [
        "### Step 6: Print citations and references [Optional]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voThgteH-Tm8"
      },
      "source": [
        "**Optional:** Uncomment to see the detailed citations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYRLQ47or1I8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# print(\"---------------Matched Images------------------\\n\")\n",
        "# display_images(\n",
        "#     [\n",
        "#         matching_results_image_fromdescription_data[0][\"img_path\"],\n",
        "#         matching_results_image_fromdescription_data[1][\"img_path\"],\n",
        "#     ],\n",
        "#     resize_ratio=0.2,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buwd_gp6HJ5K",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Image citations. You can check how Gemini generated metadata helped in grounding the answer.\n",
        "\n",
        "# print_text_to_image_citation(\n",
        "#     matching_results_image_fromdescription_data, print_top=True\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06vYM4MOHJ1-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Text citations\n",
        "\n",
        "# print_text_to_text_citation(\n",
        "#     matching_results_chunks_data,\n",
        "#     print_top=True,\n",
        "#     chunk_text=True,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIVF-QHuGVDD"
      },
      "source": [
        "### Multimodal RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U82wS4nIB8IS"
      },
      "source": [
        "### More questions with Multimodal QA System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZAyQVQ54B8X0",
        "outputId": "f47e0e1d-3e75-428b-9995-9eaacb22a2c9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Some questions to try\n",
        "# this time we are not passing any images, but just a simple text query.\n",
        "query = \"\"\"Question 1: Imagine a patient presents with new onset prurigo nodularis.\n",
        "Could Med-Gemini-M 1.5 be used to analyze dermatological images of the patient’s lesions in conjunction with a comprehensive history taken\n",
        "from an EHR dialogue to help a clinician reach a diagnosis and develop a treatment plan?\n",
        "What are the limitations and potential ethical considerations of using the model in this way?\n",
        "\n",
        "Question 2: The paper focuses on uncertainty-guided search for text-based reasoning tasks.\n",
        "How could this approach be extended to multimodal tasks?\n",
        "For instance, if Med-Gemini-M 1.5 encounters uncertainty when analyzing a dermatology image, could it generate queries to\n",
        "search for relevant visual examples or supplemental clinical information to refine its interpretation?\n",
        "\n",
        "Question 3:  Considering the potential benefits and risks highlighted in the paper, what specific steps should be taken during the development,\n",
        "validation, and deployment of Med-Gemini models to ensure they are used safely, fairly, and effectively in real-world clinical settings?\n",
        "How can these steps be informed by ongoing collaboration between researchers, clinicians, regulators, and patient communities?\n",
        " \"\"\"\n",
        "\n",
        "(\n",
        "    response,\n",
        "    matching_results_chunks_data,\n",
        "    matching_results_image_fromdescription_data,\n",
        ") = get_answer_from_qa_system(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    top_n_text=10,\n",
        "    top_n_image=5,\n",
        "    model=multimodal_model_2_0_flash,\n",
        "    safety_settings=safety_settings,\n",
        "    generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        ")\n",
        "\n",
        "rich_Markdown(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "dTHdui6jCHzc",
        "outputId": "6ee15be8-fede-448b-c2a2-a74bac3228d3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Some questions to try\n",
        "\n",
        "query = \"\"\"Question 1: How does the mixture-of-experts architecture in Gemini 1.5 Pro contribute to its ability to handle long\n",
        "context while maintaining performance on core capabilities? Discuss the potential trade-offs involved.\n",
        "\n",
        "Question 2: Gemini 1.5 Pro incorporates various safety mitigations, including supervised fine-tuning and reinforcement learning.\n",
        "Discuss the effectiveness of these mitigations in addressing content safety and representational harms in both text-to-text and\n",
        "image-to-text modalities. How can these evaluations be improved?\n",
        "\n",
        "Question 3: Gemini 1.5 Pro demonstrates surprising in-context language learning capabilities for Kalamang,\n",
        "a low-resource language. What are the implications of this finding for language preservation and revitalization?\n",
        "What challenges need to be addressed for broader applicability of this approach?\n",
        "\"\"\"\n",
        "(\n",
        "    response,\n",
        "    matching_results_chunks_data,\n",
        "    matching_results_image_fromdescription_data,\n",
        ") = get_answer_from_qa_system(\n",
        "    query,\n",
        "    text_metadata_df,\n",
        "    image_metadata_df,\n",
        "    top_n_text=10,\n",
        "    top_n_image=5,\n",
        "    model=multimodal_model_2_0_flash,\n",
        "    safety_settings=safety_settings,\n",
        "    generation_config=GenerationConfig(temperature=1, max_output_tokens=8192),\n",
        ")\n",
        "\n",
        "rich_Markdown(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwNrHCqbi3xi"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05jynhZnkgxn"
      },
      "source": [
        "Congratulations on making it through this multimodal RAG notebook!\n",
        "\n",
        "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
        "\n",
        "* **Data dependency:** Needs high-quality paired text and visuals.\n",
        "* **Computationally demanding:** Processing multimodal data is resource-intensive.\n",
        "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
        "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
        "\n",
        "\n",
        "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m116",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
