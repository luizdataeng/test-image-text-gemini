{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqZ76rJh2rM",
        "outputId": "c7648628-a72f-467b-bca4-b7fb003885fe",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your active configuration is: [personal]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your project ID is: gen-lang-client-0303567819\n"
          ]
        }
      ],
      "source": [
        "# Define project information\n",
        "\n",
        "import sys\n",
        "\n",
        "PROJECT_ID = \"gen-lang-client-0303567819\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D48gUW5-h2rM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rtMowvm-yQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rich import print as rich_print\n",
        "from rich.markdown import Markdown as rich_Markdown\n",
        "from IPython.display import Markdown, display\n",
        "from vertexai.generative_models import (\n",
        "    Content,\n",
        "    GenerationConfig,\n",
        "    GenerationResponse,\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    Image,\n",
        "    Part,\n",
        ")\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from vertexai.vision_models import MultiModalEmbeddingModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SvMwSRJJh2rM",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759326867.865175  120384 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1759326868.826325  120384 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "\n",
        "multimodal_model_2_0_flash = GenerativeModel(\n",
        "    \"gemini-2.0-flash-001\"\n",
        ") # Gemini latest Gemini 2.0 Flash Model\n",
        "\n",
        "multimodal_model_15 = GenerativeModel(\n",
        "    \"gemini-1.5-pro-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
        "\n",
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "multimodal_model_15_flash = GenerativeModel(\n",
        "    \"gemini-1.5-flash-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
        "\n",
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
        "\n",
        "# Load multimodal embedding model from pre-trained source\n",
        "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
        "    \"multimodalembedding@001\"\n",
        ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tStqXX32RNYK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_document_metadata,\n",
        "    set_global_variable,\n",
        ")\n",
        "\n",
        "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
        "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\n",
            "\n",
            "üìä Imagens atuais na pasta: 1\n",
            "üîÑ Extraindo imagens do PDF para ter mais dados...\n",
            "üîç Processando PDF: map/map.pdf\n",
            "üìä PDF tem 1 p√°ginas\n",
            "üìÑ P√°gina 1: 0 imagens encontradas\n",
            "\n",
            "üéâ Total de 0 imagens extra√≠das!\n",
            "‚ùå Nenhuma imagem foi extra√≠da do PDF\n",
            "\n",
            "üìä STATUS FINAL: 1 imagens na pasta 'images/'\n",
            "‚ö†Ô∏è  Ainda h√° apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\n"
          ]
        }
      ],
      "source": [
        "# C√âLULA 78 (OPCIONAL) - üìÑ EXTRAIR MAIS IMAGENS DO PDF PARA COMPARA√á√ÉO\n",
        "# Esta c√©lula extrai imagens do map.pdf para ter mais dados para compara√ß√£o\n",
        "\n",
        "print(\"=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\\n\")\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def extrair_imagens_do_pdf(pdf_path, output_dir=\"images/\", prefixo=\"map\"):\n",
        "    \"\"\"\n",
        "    Extrai imagens de um PDF e salva na pasta de imagens\n",
        "    \"\"\"\n",
        "    print(f\"üîç Processando PDF: {pdf_path}\")\n",
        "    \n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"‚ùå PDF n√£o encontrado: {pdf_path}\")\n",
        "        return []\n",
        "    \n",
        "    # Criar diret√≥rio se n√£o existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Abrir PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    imagens_extraidas = []\n",
        "    \n",
        "    print(f\"üìä PDF tem {len(doc)} p√°ginas\")\n",
        "    \n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        images = page.get_images()\n",
        "        \n",
        "        print(f\"üìÑ P√°gina {page_num + 1}: {len(images)} imagens encontradas\")\n",
        "        \n",
        "        for img_index, img in enumerate(images):\n",
        "            try:\n",
        "                # Extrair imagem\n",
        "                xref = img[0]\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "                \n",
        "                # Converter para RGB se necess√°rio\n",
        "                if pix.colorspace and pix.colorspace.n > 3:\n",
        "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                \n",
        "                # Nome do arquivo\n",
        "                img_filename = f\"{prefixo}_page_{page_num + 1}_img_{img_index + 1}.png\"\n",
        "                img_path = os.path.join(output_dir, img_filename)\n",
        "                \n",
        "                # Salvar imagem\n",
        "                pix.save(img_path)\n",
        "                imagens_extraidas.append(img_path)\n",
        "                \n",
        "                print(f\"  ‚úÖ Extra√≠da: {img_filename}\")\n",
        "                \n",
        "                pix = None  # Liberar mem√≥ria\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Erro ao extrair imagem {img_index}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    doc.close()\n",
        "    print(f\"\\nüéâ Total de {len(imagens_extraidas)} imagens extra√≠das!\")\n",
        "    return imagens_extraidas\n",
        "\n",
        "# Verificar quantas imagens temos atualmente\n",
        "current_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"üìä Imagens atuais na pasta: {current_images}\")\n",
        "\n",
        "if current_images <= 1:\n",
        "    print(\"üîÑ Extraindo imagens do PDF para ter mais dados...\")\n",
        "    \n",
        "    # Extrair do map.pdf se existir\n",
        "    if os.path.exists(\"map/map.pdf\"):\n",
        "        imagens_extraidas = extrair_imagens_do_pdf(\"map/map.pdf\", \"images/\", \"map\")\n",
        "        \n",
        "        if imagens_extraidas:\n",
        "            print(f\"\\n‚úÖ {len(imagens_extraidas)} novas imagens adicionadas!\")\n",
        "            print(\"üöÄ Agora execute a C√âLULA 76 novamente para processar todas as imagens\")\n",
        "            print(\"   Depois execute a C√âLULA 70 para testar similaridade com mais dados\")\n",
        "        else:\n",
        "            print(\"‚ùå Nenhuma imagem foi extra√≠da do PDF\")\n",
        "    else:\n",
        "        print(\"‚ùå Arquivo map/map.pdf n√£o encontrado\")\n",
        "        \n",
        "        # Verificar outros PDFs dispon√≠veis\n",
        "        print(\"\\nüîç Procurando outros PDFs...\")\n",
        "        pdf_paths = []\n",
        "        for root, dirs, files in os.walk(\".\"):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    pdf_paths.append(os.path.join(root, file))\n",
        "        \n",
        "        if pdf_paths:\n",
        "            print(\"üìã PDFs encontrados:\")\n",
        "            for i, pdf_path in enumerate(pdf_paths[:3], 1):  # Mostrar apenas os 3 primeiros\n",
        "                print(f\"  {i}. {pdf_path}\")\n",
        "                \n",
        "            # Processar o primeiro PDF encontrado\n",
        "            if pdf_paths:\n",
        "                primeiro_pdf = pdf_paths[0]\n",
        "                print(f\"\\nüîÑ Processando: {primeiro_pdf}\")\n",
        "                imagens_extraidas = extrair_imagens_do_pdf(primeiro_pdf, \"images/\", \"doc\")\n",
        "                \n",
        "                if imagens_extraidas:\n",
        "                    print(f\"\\n‚úÖ {len(imagens_extraidas)} imagens extra√≠das de {primeiro_pdf}!\")\n",
        "                    print(\"üöÄ Execute a C√âLULA 76 novamente para processar todas as imagens\")\n",
        "        else:\n",
        "            print(\"‚ùå Nenhum PDF encontrado para extrair imagens\")\n",
        "            \n",
        "else:\n",
        "    print(\"‚úÖ J√° h√° m√∫ltiplas imagens na pasta\")\n",
        "    print(\"Execute a C√âLULA 76 para processar todas e depois a C√âLULA 70 para testar similaridade\")\n",
        "\n",
        "# Mostrar status final\n",
        "final_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"\\nüìä STATUS FINAL: {final_images} imagens na pasta 'images/'\")\n",
        "\n",
        "if final_images > 1:\n",
        "    print(\"üéâ Pronto para testar busca por similaridade!\")\n",
        "    print(\"üìã PR√ìXIMOS PASSOS:\")\n",
        "    print(\"  1. Execute C√âLULA 76 (processar todas as imagens)\")\n",
        "    print(\"  2. Execute C√âLULA 70 (busca por similaridade)\")\n",
        "    print(\"  3. Execute C√âLULA 71 (an√°lise contextual)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Ainda h√° apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fun√ß√£o 'processar_imagens_da_pasta' criada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# C√âLULA 75 (NOVO) - üìÇ PROCESSAMENTO DIRETO DE IMAGENS DA PASTA\n",
        "# Fun√ß√£o para ler todas as imagens da pasta images/ e gerar embeddings para RAG\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_image_embedding_from_multimodal_embedding_model,\n",
        "    get_gemini_response\n",
        ")\n",
        "\n",
        "def processar_imagens_da_pasta(\n",
        "    pasta_imagens=\"images/\",\n",
        "    embedding_size=512,\n",
        "    gerar_descricoes=True,\n",
        "    formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "):\n",
        "    \"\"\"\n",
        "    Processa todas as imagens de uma pasta, gerando embeddings e descri√ß√µes para RAG\n",
        "    \n",
        "    Args:\n",
        "        pasta_imagens: Caminho da pasta com imagens\n",
        "        embedding_size: Tamanho do embedding (128, 256, 512, 1408)\n",
        "        gerar_descricoes: Se deve gerar descri√ß√µes das imagens com Gemini\n",
        "        formatos_suportados: Lista de formatos de imagem aceitos\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame compat√≠vel com o sistema RAG existente\n",
        "    \"\"\"\n",
        "    print(f\"üîç PROCESSANDO IMAGENS DA PASTA: {pasta_imagens}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Verificar se a pasta existe\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        print(f\"‚ùå Pasta '{pasta_imagens}' n√£o encontrada!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Encontrar todas as imagens na pasta\n",
        "    imagens_encontradas = []\n",
        "    for formato in formatos_suportados:\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato.upper()}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "    \n",
        "    # Remover duplicatas\n",
        "    imagens_encontradas = list(set(imagens_encontradas))\n",
        "    \n",
        "    if not imagens_encontradas:\n",
        "        print(f\"‚ùå Nenhuma imagem encontrada na pasta '{pasta_imagens}'\")\n",
        "        print(f\"Formatos suportados: {formatos_suportados}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"üìä Encontradas {len(imagens_encontradas)} imagens:\")\n",
        "    for img in imagens_encontradas:\n",
        "        print(f\"  - {os.path.basename(img)}\")\n",
        "    \n",
        "    # Lista para armazenar dados processados\n",
        "    dados_imagens = []\n",
        "    \n",
        "    # Prompt para descri√ß√£o das imagens\n",
        "    prompt_descricao = \"\"\"Analise esta imagem detalhadamente e forne√ßa uma descri√ß√£o precisa.\n",
        "    Inclua:\n",
        "    - O que voc√™ v√™ na imagem\n",
        "    - Elementos principais e detalhes importantes\n",
        "    - Texto vis√≠vel (se houver)\n",
        "    - Tipo de imagem (mapa, diagrama, foto, etc.)\n",
        "    - Informa√ß√µes relevantes para busca e recupera√ß√£o\n",
        "    \n",
        "    Seja espec√≠fico e detalhado para facilitar buscas futuras.\"\"\"\n",
        "    \n",
        "    print(f\"\\nüöÄ PROCESSANDO CADA IMAGEM...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, caminho_imagem in enumerate(imagens_encontradas, 1):\n",
        "        nome_arquivo = os.path.basename(caminho_imagem)\n",
        "        print(f\"\\nüì∏ PROCESSANDO {i}/{len(imagens_encontradas)}: {nome_arquivo}\")\n",
        "        \n",
        "        try:\n",
        "            # 1. Gerar embedding da imagem\n",
        "            print(\"  üîÑ Gerando embedding...\")\n",
        "            image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
        "                image_uri=caminho_imagem,\n",
        "                embedding_size=embedding_size,\n",
        "                return_array=True\n",
        "            )\n",
        "            print(f\"  ‚úÖ Embedding gerado: shape {image_embedding.shape}\")\n",
        "            \n",
        "            # 2. Gerar descri√ß√£o da imagem (se solicitado)\n",
        "            descricao = \"\"\n",
        "            if gerar_descricoes:\n",
        "                print(\"  ü§ñ Gerando descri√ß√£o com Gemini...\")\n",
        "                try:\n",
        "                    from vertexai.generative_models import Image as GeminiImage\n",
        "                    imagem_gemini = GeminiImage.load_from_file(caminho_imagem)\n",
        "                    \n",
        "                    descricao = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[prompt_descricao, imagem_gemini],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"  ‚úÖ Descri√ß√£o gerada: {len(descricao)} caracteres\")\n",
        "                    \n",
        "                except Exception as desc_error:\n",
        "                    print(f\"  ‚ö†Ô∏è  Erro ao gerar descri√ß√£o: {desc_error}\")\n",
        "                    descricao = f\"Imagem: {nome_arquivo}\"\n",
        "            \n",
        "            # 3. Gerar embedding da descri√ß√£o (para compatibilidade com RAG)\n",
        "            text_embedding = None\n",
        "            if descricao:\n",
        "                try:\n",
        "                    from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
        "                    text_embedding = get_text_embedding_from_text_embedding_model(descricao)\n",
        "                    print(\"  ‚úÖ Text embedding da descri√ß√£o gerado\")\n",
        "                except Exception as text_emb_error:\n",
        "                    print(f\"  ‚ö†Ô∏è  Erro ao gerar text embedding: {text_emb_error}\")\n",
        "            \n",
        "            # 4. Criar registro compat√≠vel com o sistema existente\n",
        "            registro = {\n",
        "                'file_name': f\"pasta_images_{nome_arquivo}\",  # Nome √∫nico\n",
        "                'page_num': 1,  # Imagens individuais = p√°gina 1\n",
        "                'img_num': i,\n",
        "                'img_path': caminho_imagem,\n",
        "                'img_desc': descricao,\n",
        "                'mm_embedding_from_img_only': image_embedding.tolist(),  # Compatibilidade\n",
        "                'text_embedding_from_image_description': text_embedding if text_embedding else None,\n",
        "                'source_type': 'pasta_imagens',  # Identificar origem\n",
        "                'original_filename': nome_arquivo\n",
        "            }\n",
        "            \n",
        "            dados_imagens.append(registro)\n",
        "            print(f\"  ‚úÖ Processamento conclu√≠do para {nome_arquivo}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erro ao processar {nome_arquivo}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    if dados_imagens:\n",
        "        df_imagens = pd.DataFrame(dados_imagens)\n",
        "        print(f\"\\nüéâ PROCESSAMENTO CONCLU√çDO!\")\n",
        "        print(f\"üìä DataFrame criado com {len(df_imagens)} imagens processadas\")\n",
        "        print(f\"üìã Colunas: {list(df_imagens.columns)}\")\n",
        "        \n",
        "        return df_imagens\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Nenhuma imagem foi processada com sucesso\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o 'processar_imagens_da_pasta' criada com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\n",
            "\n",
            "üîç PROCESSANDO IMAGENS DA PASTA: images/\n",
            "============================================================\n",
            "üìä Encontradas 1 imagens:\n",
            "  - B2_room.jpeg\n",
            "\n",
            "üöÄ PROCESSANDO CADA IMAGEM...\n",
            "============================================================\n",
            "\n",
            "üì∏ PROCESSANDO 1/1: B2_room.jpeg\n",
            "  üîÑ Gerando embedding...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/vision_models/_vision_models.py:153: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1759327067.668656  120384 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759327067.672835  120384 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚úÖ Embedding gerado: shape (512,)\n",
            "  ü§ñ Gerando descri√ß√£o com Gemini...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759327068.756107  120384 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  ‚ö†Ô∏è  Erro ao gerar descri√ß√£o: 'GenerationResponse' object is not iterable\n",
            "  ‚úÖ Text embedding da descri√ß√£o gerado\n",
            "  ‚úÖ Processamento conclu√≠do para B2_room.jpeg\n",
            "\n",
            "üéâ PROCESSAMENTO CONCLU√çDO!\n",
            "üìä DataFrame criado com 1 imagens processadas\n",
            "üìã Colunas: ['file_name', 'page_num', 'img_num', 'img_path', 'img_desc', 'mm_embedding_from_img_only', 'text_embedding_from_image_description', 'source_type', 'original_filename']\n",
            "\n",
            "üéâ SUCESSO TOTAL!\n",
            "üìä image_metadata_df criado com 1 imagens\n",
            "\n",
            "üìã RESUMO DAS IMAGENS PROCESSADAS:\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è  Imagem 1:\n",
            "  üìÅ Arquivo: B2_room.jpeg\n",
            "  üìÇ Caminho: images/B2_room.jpeg\n",
            "  üìä Embedding shape: 512\n",
            "  üìù Descri√ß√£o: Imagem: B2_room.jpeg\n",
            "\n",
            "‚úÖ COMPATIBILIDADE COM SISTEMA RAG:\n",
            "  ‚úÖ img_path: OK\n",
            "  ‚úÖ mm_embedding_from_img_only: OK\n",
            "  ‚úÖ img_desc: OK\n",
            "  ‚úÖ file_name: OK\n",
            "  ‚úÖ page_num: OK\n",
            "\n",
            "üíæ DataFrame salvo em 'image_metadata_from_folder.pkl'\n",
            "\n",
            "üöÄ PR√ìXIMOS PASSOS:\n",
            "1. Agora voc√™ pode executar a C√âLULA 70 (Valida√ß√£o)\n",
            "2. Depois executar a C√âLULA 71 (An√°lise Contextual)\n",
            "3. O sistema RAG est√° pronto para perguntas sobre as imagens!\n"
          ]
        }
      ],
      "source": [
        "# C√âLULA 76 (EXECUTAR) - üöÄ PROCESSAMENTO DAS IMAGENS DA PASTA images/\n",
        "# Executa o processamento de todas as imagens e cria o image_metadata_df\n",
        "\n",
        "print(\"=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\\n\")\n",
        "\n",
        "# Executar o processamento das imagens\n",
        "try:\n",
        "    image_metadata_df = processar_imagens_da_pasta(\n",
        "        pasta_imagens=\"images/\",\n",
        "        embedding_size=512,\n",
        "        gerar_descricoes=True,  # Gerar descri√ß√µes detalhadas com Gemini\n",
        "        formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "    )\n",
        "    \n",
        "    if not image_metadata_df.empty:\n",
        "        print(f\"\\nüéâ SUCESSO TOTAL!\")\n",
        "        print(f\"üìä image_metadata_df criado com {len(image_metadata_df)} imagens\")\n",
        "        \n",
        "        # Mostrar resumo das imagens processadas\n",
        "        print(f\"\\nüìã RESUMO DAS IMAGENS PROCESSADAS:\")\n",
        "        print(\"=\"*50)\n",
        "        for idx, row in image_metadata_df.iterrows():\n",
        "            print(f\"\\nüñºÔ∏è  Imagem {idx + 1}:\")\n",
        "            print(f\"  üìÅ Arquivo: {row['original_filename']}\")\n",
        "            print(f\"  üìÇ Caminho: {row['img_path']}\")\n",
        "            print(f\"  üìä Embedding shape: {len(row['mm_embedding_from_img_only'])}\")\n",
        "            \n",
        "            # Mostrar in√≠cio da descri√ß√£o\n",
        "            desc = row['img_desc']\n",
        "            if desc and len(desc) > 10:\n",
        "                print(f\"  üìù Descri√ß√£o: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
        "        \n",
        "        # Verificar compatibilidade com sistema RAG existente\n",
        "        print(f\"\\n‚úÖ COMPATIBILIDADE COM SISTEMA RAG:\")\n",
        "        colunas_necessarias = ['img_path', 'mm_embedding_from_img_only', 'img_desc', 'file_name', 'page_num']\n",
        "        for col in colunas_necessarias:\n",
        "            if col in image_metadata_df.columns:\n",
        "                print(f\"  ‚úÖ {col}: OK\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå {col}: FALTANDO\")\n",
        "        \n",
        "        # Salvar para uso futuro (opcional)\n",
        "        try:\n",
        "            image_metadata_df.to_pickle(\"image_metadata_from_folder.pkl\")\n",
        "            print(f\"\\nüíæ DataFrame salvo em 'image_metadata_from_folder.pkl'\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"\\n‚ö†Ô∏è  N√£o foi poss√≠vel salvar: {save_error}\")\n",
        "        \n",
        "        print(f\"\\nüöÄ PR√ìXIMOS PASSOS:\")\n",
        "        print(f\"1. Agora voc√™ pode executar a C√âLULA 70 (Valida√ß√£o)\")\n",
        "        print(f\"2. Depois executar a C√âLULA 71 (An√°lise Contextual)\")\n",
        "        print(f\"3. O sistema RAG est√° pronto para perguntas sobre as imagens!\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n‚ùå FALHA: Nenhuma imagem foi processada\")\n",
        "        print(f\"Verifique se:\")\n",
        "        print(f\"- A pasta 'images/' existe\")\n",
        "        print(f\"- H√° imagens v√°lidas na pasta\")\n",
        "        print(f\"- Os modelos est√£o carregados corretamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO NO PROCESSAMENTO: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\nüí° POSS√çVEIS SOLU√á√ïES:\")\n",
        "    print(f\"- Verifique se os modelos est√£o carregados\")\n",
        "    print(f\"- Verifique se a pasta 'images/' existe\")\n",
        "    print(f\"- Execute as c√©lulas de setup dos modelos primeiro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== M3.JPEG SIMILARITY SEARCH VALIDATION ===\n",
            "\n",
            "‚úÖ image_metadata_df available!\n",
            "üìä Dataset: 1 images processed\n",
            "\n",
            "üìã IMAGES IN DATASET:\n",
            "  1. B2_room.jpeg\n",
            "\n",
            "‚ùå M3.jpeg not found in dataset!\n",
            "Check if the image is in the 'images/' folder and execute CELL 76 again\n"
          ]
        }
      ],
      "source": [
        "# CELL 70 (STEP 3) - üéØ VALIDATION: Similarity search with M3.jpeg in the dataset\n",
        "# This cell uses the image_metadata_df created by Cell 76 to search for similar images\n",
        "\n",
        "print(\"=== M3.JPEG SIMILARITY SEARCH VALIDATION ===\\n\")\n",
        "\n",
        "# Check if we have the image metadata DataFrame\n",
        "if 'image_metadata_df' not in locals():\n",
        "    print(\"‚ùå image_metadata_df not found!\")\n",
        "    print(\"\\nüí° To resolve:\")\n",
        "    print(\"1. Execute CELL 78 first (Extract images from PDF)\")\n",
        "    print(\"2. Execute CELL 76 next (Process Images)\")\n",
        "    print(\"3. Then execute this cell again\")\n",
        "    \n",
        "elif image_metadata_df.empty:\n",
        "    print(\"‚ùå image_metadata_df is empty!\")\n",
        "    print(\"Execute CELL 76 to process images from the folder\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚úÖ image_metadata_df available!\")\n",
        "    print(f\"üìä Dataset: {len(image_metadata_df)} images processed\")\n",
        "    \n",
        "    # Show all images in the dataset\n",
        "    print(f\"\\nüìã IMAGES IN DATASET:\")\n",
        "    for idx, row in image_metadata_df.iterrows():\n",
        "        print(f\"  {idx + 1}. {row['original_filename']}\")\n",
        "    \n",
        "    # Find M3.jpeg in the dataset\n",
        "    m3_rows = image_metadata_df[image_metadata_df['original_filename'].str.contains('M3.jpeg', case=False, na=False)]\n",
        "    \n",
        "    if m3_rows.empty:\n",
        "        print(\"\\n‚ùå M3.jpeg not found in dataset!\")\n",
        "        print(\"Check if the image is in the 'images/' folder and execute CELL 76 again\")\n",
        "    else:\n",
        "        print(f\"\\n‚úÖ M3.jpeg found in dataset!\")\n",
        "        m3_row = m3_rows.iloc[0]\n",
        "        print(f\"  üìÅ File: {m3_row['original_filename']}\")\n",
        "        print(f\"  üìÇ Path: {m3_row['img_path']}\")\n",
        "        \n",
        "        # Extract M3.jpeg embedding\n",
        "        m3_embedding = np.array(m3_row['mm_embedding_from_img_only'])\n",
        "        print(f\"  üìä Embedding shape: {m3_embedding.shape}\")\n",
        "        \n",
        "        # Create dataset without M3.jpeg itself for comparison\n",
        "        other_images_df = image_metadata_df[~image_metadata_df['original_filename'].str.contains('M3.jpeg', case=False, na=False)]\n",
        "        \n",
        "        if other_images_df.empty:\n",
        "            print(\"\\n‚ö†Ô∏è  Only M3.jpeg found in dataset\")\n",
        "            print(\"Execute CELL 78 first to extract more images from PDF\")\n",
        "            print(\"Then execute CELL 76 to process all images\")\n",
        "        else:\n",
        "            print(f\"\\nüîç EXECUTING SIMILARITY SEARCH...\")\n",
        "            print(f\"üìä Comparing M3.jpeg with {len(other_images_df)} other images\")\n",
        "            \n",
        "            # Use our robust alternative function\n",
        "            try:\n",
        "                similar_results = buscar_imagens_similares_com_embedding(\n",
        "                    image_embedding=m3_embedding,\n",
        "                    image_metadata_df=other_images_df,\n",
        "                    top_n=min(5, len(other_images_df))  # Top N most similar images\n",
        "                )\n",
        "                \n",
        "                if similar_results:\n",
        "                    print(f\"\\nüéâ SUCCESS! Found {len(similar_results)} images similar to M3.jpeg:\")\n",
        "                    print(\"=\"*80)\n",
        "                    \n",
        "                    for i, result in enumerate(similar_results, 1):\n",
        "                        print(f\"\\nüñºÔ∏è  RESULT {i}:\")\n",
        "                        print(f\"  üìà Similarity: {result['cosine_score']:.4f}\")\n",
        "                        print(f\"  üìÅ File: {result['file_name']}\")\n",
        "                        print(f\"  üìÇ Path: {result['img_path']}\")\n",
        "                        \n",
        "                        # Show description if available\n",
        "                        desc = result['img_desc']\n",
        "                        if desc and desc != 'N/A' and len(str(desc)) > 10:\n",
        "                            desc_str = str(desc)\n",
        "                            print(f\"  üìù Description: {desc_str[:200]}{'...' if len(desc_str) > 200 else ''}\")\n",
        "                    \n",
        "                    # Save results for later use\n",
        "                    globals()['matching_results'] = similar_results\n",
        "                    print(f\"\\nüíæ Results saved to 'matching_results' variable\")\n",
        "                    \n",
        "                    # Similarity score analysis\n",
        "                    scores = [r['cosine_score'] for r in similar_results]\n",
        "                    print(f\"\\nüìä SCORE ANALYSIS:\")\n",
        "                    print(f\"  - Maximum score: {max(scores):.4f}\")\n",
        "                    print(f\"  - Minimum score: {min(scores):.4f}\")\n",
        "                    print(f\"  - Average score: {sum(scores)/len(scores):.4f}\")\n",
        "                    \n",
        "                    print(f\"\\n‚úÖ VALIDATION COMPLETED SUCCESSFULLY!\")\n",
        "                    print(f\"üöÄ Now you can execute CELL 71 for contextual analysis!\")\n",
        "                    \n",
        "                else:\n",
        "                    print(\"‚ùå No similar images found.\")\n",
        "                    print(\"This may indicate problems with embeddings or very different data.\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error during search: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                \n",
        "                print(f\"\\nüí° DIAGNOSIS:\")\n",
        "                print(f\"- Check if function 'buscar_imagens_similares_com_embedding' was defined (CELL 75)\")\n",
        "                print(f\"- Check if there are other images besides M3.jpeg in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== AN√ÅLISE CONTEXTUAL DA M3.JPEG ===\n",
            "\n",
            "‚ùå Nenhum resultado de busca por similaridade encontrado.\n",
            "Execute a c√©lula anterior (70) primeiro para obter os resultados.\n",
            "\n",
            "üí° Como alternativa, vou fazer uma an√°lise direta da M3.jpeg:\n",
            "‚ùå Erro na an√°lise direta: [Errno 2] No such file or directory: 'images/M3.jpeg'\n",
            "\n",
            "‚úÖ AN√ÅLISE CONTEXTUAL CONCLU√çDA!\n",
            "A M3.jpeg foi analisada usando o contexto das imagens similares encontradas via embedding.\n"
          ]
        }
      ],
      "source": [
        "# Novo C√≥digo\n",
        "\n",
        "# C√âLULA 71 (PASSO 4) - ü§ñ AN√ÅLISE CONTEXTUAL: Perguntas sobre M3.jpeg com base nas imagens similares\n",
        "# Esta c√©lula usa os resultados da busca por similaridade para an√°lise contextual\n",
        "\n",
        "print(\"=== AN√ÅLISE CONTEXTUAL DA M3.JPEG ===\\n\")\n",
        "\n",
        "# Verificar se temos resultados da busca anterior\n",
        "if 'matching_results' not in locals() or not matching_results:\n",
        "    print(\"‚ùå Nenhum resultado de busca por similaridade encontrado.\")\n",
        "    print(\"Execute a c√©lula anterior (70) primeiro para obter os resultados.\")\n",
        "    \n",
        "    print(\"\\nüí° Como alternativa, vou fazer uma an√°lise direta da M3.jpeg:\")\n",
        "    \n",
        "    try:\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        m3_image = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "        \n",
        "        pergunta_direta = \"\"\"Analise esta imagem detalhadamente. \n",
        "        O que voc√™ v√™? Descreva todos os elementos vis√≠veis.\n",
        "        Se √© um mapa ou planta, identifique os elementos principais.\"\"\"\n",
        "        \n",
        "        resposta_direta = get_gemini_response(\n",
        "            multimodal_model_2_0_flash,\n",
        "            model_input=[pergunta_direta, m3_image],\n",
        "            stream=False,\n",
        "        )\n",
        "        \n",
        "        print(f\"ü§ñ AN√ÅLISE DIRETA DA M3.JPEG:\")\n",
        "        print(f\"{resposta_direta}\")\n",
        "        \n",
        "    except Exception as direct_error:\n",
        "        print(f\"‚ùå Erro na an√°lise direta: {direct_error}\")\n",
        "\n",
        "else:\n",
        "    print(f\"‚úÖ Temos {len(matching_results)} resultados de busca por similaridade!\")\n",
        "    \n",
        "    # Preparar contexto baseado nos resultados similares\n",
        "    contexto_descricoes = []\n",
        "    contexto_caminhos = []\n",
        "    \n",
        "    for i, result in enumerate(matching_results):\n",
        "        desc = result.get('img_desc', '')\n",
        "        caminho = result.get('img_path', '')\n",
        "        arquivo = result.get('file_name', '')\n",
        "        score = result.get('cosine_score', 0)\n",
        "        \n",
        "        if desc and desc != 'N/A' and len(str(desc)) > 10:\n",
        "            contexto_descricoes.append(f\"Imagem similar {i+1} (similaridade: {score:.3f}): {desc}\")\n",
        "        \n",
        "        if caminho and caminho != 'N/A':\n",
        "            contexto_caminhos.append(caminho)\n",
        "    \n",
        "    print(f\"üìù Coletadas {len(contexto_descricoes)} descri√ß√µes de imagens similares\")\n",
        "    \n",
        "    # Perguntas espec√≠ficas sobre a M3.jpeg\n",
        "    perguntas_contextualizadas = [\n",
        "        \"Baseado nas imagens similares encontradas, o que voc√™ pode me dizer sobre a M3.jpeg?\",\n",
        "        \"Quais elementos em comum existem entre a M3.jpeg e as imagens similares?\",\n",
        "        \"Se a M3.jpeg √© um mapa ou planta, quais informa√ß√µes espec√≠ficas posso extrair?\",\n",
        "        \"H√° algum padr√£o arquitet√¥nico ou de layout vis√≠vel na M3.jpeg?\",\n",
        "        \"What are the rooms in this floor? (baseado no contexto das imagens similares)\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\nü§ñ AN√ÅLISE CONTEXTUAL COM GEMINI:\")\n",
        "    \n",
        "    try:\n",
        "        # Carregar a imagem M3.jpeg\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        m3_image = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "        \n",
        "        # Preparar contexto das imagens similares\n",
        "        contexto_texto = \"\\n\".join(contexto_descricoes[:3])  # Top 3 descri√ß√µes\n",
        "        \n",
        "        for i, pergunta in enumerate(perguntas_contextualizadas, 1):\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(f\"üìã PERGUNTA {i}: {pergunta}\")\n",
        "            print(\"=\"*70)\n",
        "            \n",
        "            # Criar prompt contextualizado\n",
        "            prompt_contextualizado = f\"\"\"\n",
        "            Analise a imagem fornecida considerando o seguinte contexto de imagens similares:\n",
        "            \n",
        "            CONTEXTO DE IMAGENS SIMILARES ENCONTRADAS:\n",
        "            {contexto_texto}\n",
        "            \n",
        "            PERGUNTA ESPEC√çFICA:\n",
        "            {pergunta}\n",
        "            \n",
        "            Por favor, forne√ßa uma resposta detalhada baseada tanto na an√°lise visual da imagem \n",
        "            quanto no contexto das imagens similares fornecido acima.\n",
        "            \"\"\"\n",
        "            \n",
        "            try:\n",
        "                resposta = get_gemini_response(\n",
        "                    multimodal_model_2_0_flash,\n",
        "                    model_input=[prompt_contextualizado, m3_image],\n",
        "                    stream=False,\n",
        "                )\n",
        "                \n",
        "                print(f\"ü§ñ RESPOSTA CONTEXTUALIZADA:\")\n",
        "                print(f\"{resposta}\")\n",
        "                \n",
        "            except Exception as gemini_error:\n",
        "                print(f\"‚ùå Erro na an√°lise contextual: {gemini_error}\")\n",
        "                \n",
        "                # Fallback: an√°lise simples sem contexto\n",
        "                try:\n",
        "                    resposta_simples = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[pergunta, m3_image],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"ü§ñ RESPOSTA SIMPLES (sem contexto):\")\n",
        "                    print(f\"{resposta_simples}\")\n",
        "                    \n",
        "                except Exception as simple_error:\n",
        "                    print(f\"‚ùå Erro na an√°lise simples: {simple_error}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erro ao carregar imagem: {e}\")\n",
        "    \n",
        "    # Mostrar resumo final\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"üìä RESUMO DOS RESULTADOS DE SIMILARIDADE:\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, result in enumerate(matching_results, 1):\n",
        "        print(f\"\\nüñºÔ∏è  Imagem Similar {i}:\")\n",
        "        print(f\"  üìà Similaridade: {result.get('cosine_score', 0):.4f}\")\n",
        "        print(f\"  üìÅ Arquivo: {result.get('file_name', 'N/A')}\")\n",
        "        print(f\"  üìÑ P√°gina: {result.get('page_num', 'N/A')}\")\n",
        "        print(f\"  üìÇ Caminho: {result.get('img_path', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n‚úÖ AN√ÅLISE CONTEXTUAL CONCLU√çDA!\")\n",
        "print(f\"A M3.jpeg foi analisada usando o contexto das imagens similares encontradas via embedding.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DIRECT M3.JPEG ANALYSIS WITH GEMINI ===\n",
            "\n",
            "‚úÖ Gemini 2.0 Flash model available\n",
            "üîç Analyzing image with Gemini...\n",
            "‚úÖ Image loaded: images/B2_room.jpeg\n",
            "\n",
            "üìã QUESTION 1: What are the rooms or areas shown in this floor plan?\n",
            "------------------------------------------------------------\n",
            "ü§ñ RESPONSE:\n",
            "Based on the floor plan provided, here are the rooms or areas that are visible:\n",
            "\n",
            "*   **Rooms:**\n",
            "    *   Room 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2010, 2012, 2015, 2025, 2029, 2032, 2034, 2035, 2036, 2037\n",
            "*   **Amenities:**\n",
            "    *   Washrooms\n",
            "    *   Women's Washroom\n",
            "    *   Women's Accessible Washroom\n",
            "    *   Accessible Washroom\n",
            "    *   Men's Washroom\n",
            "    *   Men's Accessible Washroom\n",
            "    *   Accessible Shower and Changeroom\n",
            "    *   Elevator\n",
            "    *   Stairs\n",
            "    *   Bus Stop\n",
            "    *   Bike Parking\n",
            "    *   Information\n",
            "    *   Food\n",
            "    *   Interior and exterior pathways\n",
            "\n",
            "Other items:\n",
            "\n",
            "*   T\n",
            "*   D\n",
            "*   You are here\n",
            "*   Exit\n",
            "*   Adjacent Buildings\n",
            "\n",
            "üìã QUESTION 2: How can I go from room 2001 to room 2037?\n",
            "------------------------------------------------------------\n",
            "ü§ñ RESPONSE:\n",
            "Okay, based on the map provided:\n",
            "\n",
            "1.  **Exit Room 2001:** Leave room 2001.\n",
            "2.  **Turn Left:** Turn left as you exit the room.\n",
            "3.  **Walk to the End of the Hall:** Walk to the end of the hall that contains the rooms in the 2000s. Room 2037 is at the other end of that hallway.\n",
            "\n",
            "‚úÖ Analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# CELL 72 - ü§ñ DIRECT M3.JPEG ANALYSIS WITH GEMINI (CORRECTED)\n",
        "# Corrected function to ask for figure details using the Gemini model\n",
        "\n",
        "print(\"=== DIRECT M3.JPEG ANALYSIS WITH GEMINI ===\\n\")\n",
        "\n",
        "def ask_figure_details_corrected(model, image_path=None):\n",
        "    \"\"\"\n",
        "    Automatically asks for figure details from the multimodal Gemini model.\n",
        "    Args:\n",
        "        model: loaded Gemini model\n",
        "        image_path: path to the image (string)\n",
        "    \"\"\"\n",
        "    print(\"üîç Analyzing image with Gemini...\")\n",
        "    \n",
        "    # Specific questions about the image\n",
        "    questions = [\n",
        "        \"What are the rooms or areas shown in this floor plan?\",\n",
        "        \"How can I go from room 2001 to room 2037?\"\n",
        "    ]\n",
        "    try:\n",
        "        # Load the image\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        image_object = GeminiImage.load_from_file(image_path)\n",
        "        print(f\"‚úÖ Image loaded: {image_path}\")\n",
        "        \n",
        "        # Ask each question\n",
        "        for i, question in enumerate(questions, 1):\n",
        "            print(f\"\\nüìã QUESTION {i}: {question}\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            try:\n",
        "                # Use the model directly (most reliable method)\n",
        "                response = model.generate_content([question, image_object])\n",
        "                response_text = response.text if hasattr(response, 'text') else str(response)\n",
        "                \n",
        "                print(f\"ü§ñ RESPONSE:\")\n",
        "                print(f\"{response_text}\")\n",
        "                \n",
        "            except Exception as question_error:\n",
        "                print(f\"‚ùå Error in question {i}: {question_error}\")\n",
        "                \n",
        "                # Try alternative method with get_gemini_response\n",
        "                try:\n",
        "                    alt_response = get_gemini_response(\n",
        "                        model,\n",
        "                        model_input=[question, image_object],\n",
        "                        stream=False\n",
        "                    )\n",
        "                    print(f\"ü§ñ RESPONSE (alternative method):\")\n",
        "                    print(f\"{alt_response}\")\n",
        "                except Exception as alt_error:\n",
        "                    print(f\"‚ùå Alternative method also failed: {alt_error}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå General error analyzing image: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Test the corrected function automatically\n",
        "try:\n",
        "    # Check if the model is available\n",
        "    if 'multimodal_model_2_0_flash' in locals():\n",
        "        print(\"‚úÖ Gemini 2.0 Flash model available\")\n",
        "        \n",
        "        # Test with M3.jpeg\n",
        "        ask_figure_details_corrected(\n",
        "            multimodal_model_2_0_flash, \n",
        "            image_path=\"images/B2_room.jpeg\"\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        print(\"‚ùå multimodal_model_2_0_flash model not found\")\n",
        "        print(\"Execute the model setup cells first\")\n",
        "        \n",
        "        # Try to load basic model\n",
        "        try:\n",
        "            from vertexai.generative_models import GenerativeModel\n",
        "            temp_model = GenerativeModel(\"gemini-1.5-flash\")\n",
        "            print(\"üîÑ Using Gemini 1.5 Flash model as alternative...\")\n",
        "            \n",
        "            ask_figure_details_corrected(\n",
        "                temp_model,\n",
        "                image_path=\"images/M3.jpeg\"\n",
        "            )\n",
        "        except Exception as model_error:\n",
        "            print(f\"‚ùå Error loading alternative model: {model_error}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error executing analysis: {e}\")\n",
        "    print(\"Check if:\")\n",
        "    print(\"- The image 'images/M3.jpeg' exists\")\n",
        "    print(\"- Vertex AI models are configured\")\n",
        "    print(\"- Dependencies are installed\")\n",
        "\n",
        "print(f\"\\n‚úÖ Analysis completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m116",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
    },
    "kernelspec": {
      "display_name": "test-image-text-gemini",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
