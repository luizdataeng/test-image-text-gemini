{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqZ76rJh2rM",
        "outputId": "c7648628-a72f-467b-bca4-b7fb003885fe",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your project ID is: gen-lang-client-0303567819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your active configuration is: [personal]\n"
          ]
        }
      ],
      "source": [
        "# Define project information\n",
        "\n",
        "import sys\n",
        "\n",
        "PROJECT_ID = \"gen-lang-client-0303567819\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D48gUW5-h2rM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rtMowvm-yQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rich import print as rich_print\n",
        "from rich.markdown import Markdown as rich_Markdown\n",
        "from IPython.display import Markdown, display\n",
        "from vertexai.generative_models import (\n",
        "    Content,\n",
        "    GenerationConfig,\n",
        "    GenerationResponse,\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    Image,\n",
        "    Part,\n",
        ")\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from vertexai.vision_models import MultiModalEmbeddingModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SvMwSRJJh2rM",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1762613845.227259   13116 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "ename": "RetryError",
          "evalue": "Timeout of 120.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:332\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:440\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1192\u001b[39m state, call = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1193\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1194\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:1009\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
            "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\", grpc_status:14}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
            "\u001b[31mServiceUnavailable\u001b[39m: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     12\u001b[39m multimodal_model_15_flash = GenerativeModel(\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgemini-1.5-flash-001\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m )  \u001b[38;5;66;03m# works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Load text embedding model from pre-trained source\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m text_embedding_model = \u001b[43mTextEmbeddingModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-embedding-005\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load multimodal embedding model from pre-trained source\u001b[39;00m\n\u001b[32m     20\u001b[39m multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmultimodalembedding@001\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m )  \u001b[38;5;66;03m# works with image, image with caption(~32 words), video, video with caption(~32 words)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:290\u001b[39m, in \u001b[36m_ModelGardenModel.from_pretrained\u001b[39m\u001b[34m(cls, model_name)\u001b[39m\n\u001b[32m    279\u001b[39m credential_exception_str = (\n\u001b[32m    280\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUnable to authenticate your request.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDepending on your runtime environment, you can complete authentication by:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    287\u001b[39m )\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions.GoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m auth_exceptions.GoogleAuthError(credential_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:207\u001b[39m, in \u001b[36m_from_pretrained\u001b[39m\u001b[34m(interface_class, model_name, publisher_model, tuned_vertex_model)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interface_class._INSTANCE_SCHEMA_URI:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterface_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a correct model interface class since it does not have an instance schema URI.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     model_info = \u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_to_class_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_INSTANCE_SCHEMA_URI\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    213\u001b[39m     schema_uri = publisher_model._gca_resource.predict_schemata.instance_schema_uri\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:123\u001b[39m, in \u001b[36m_get_model_info\u001b[39m\u001b[34m(model_id, schema_to_class_map, interface_class, publisher_model_res, tuned_vertex_model)\u001b[39m\n\u001b[32m    119\u001b[39m     model_id = \u001b[33m\"\u001b[39m\u001b[33mpublishers/google/models/\u001b[39m\u001b[33m\"\u001b[39m + model_id\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res:\n\u001b[32m    122\u001b[39m     publisher_model_res = (\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[43m_publisher_models\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_PublisherModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m._gca_resource\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mpublishers/google/models/\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly Google models are currently supported. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublisher_model_res.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/cloud/aiplatform/_publisher_models.py:77\u001b[39m, in \u001b[36m_PublisherModel.__init__\u001b[39m\u001b[34m(self, resource_name, project, location, credentials)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` is not a valid PublisherModel resource \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname or model garden id.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28mself\u001b[39m._gca_resource = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getter_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_resource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_DEFAULT_RETRY\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/model_garden_service/client.py:927\u001b[39m, in \u001b[36mModelGardenServiceClient.get_publisher_model\u001b[39m\u001b[34m(self, request, name, retry, timeout, metadata)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:229\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time.monotonic() + next_sleep > deadline:\n\u001b[32m    224\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    225\u001b[39m         error_list,\n\u001b[32m    226\u001b[39m         RetryFailureReason.TIMEOUT,\n\u001b[32m    227\u001b[39m         original_timeout,\n\u001b[32m    228\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    230\u001b[39m _LOGGER.debug(\n\u001b[32m    231\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[33ms ...\u001b[39m\u001b[33m\"\u001b[39m.format(error_list[-\u001b[32m1\u001b[39m], next_sleep)\n\u001b[32m    232\u001b[39m )\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m next_sleep\n",
            "\u001b[31mRetryError\u001b[39m: Timeout of 120.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate."
          ]
        }
      ],
      "source": [
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "\n",
        "multimodal_model_2_0_flash = GenerativeModel(\n",
        "    \"gemini-2.0-flash-001\"\n",
        ") # Gemini latest Gemini 2.0 Flash Model\n",
        "\n",
        "multimodal_model_15 = GenerativeModel(\n",
        "    \"gemini-1.5-pro-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
        "\n",
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "multimodal_model_15_flash = GenerativeModel(\n",
        "    \"gemini-1.5-flash-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
        "\n",
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "\n",
        "# Load multimodal embedding model from pre-trained source\n",
        "multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
        "    \"multimodalembedding@001\"\n",
        ")  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1762615718.527455   13116 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "ename": "RetryError",
          "evalue": "Timeout of 120.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31m_InactiveRpcError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:76\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:277\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.__call__\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m    269\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    270\u001b[39m     request: Any,\n\u001b[32m   (...)\u001b[39m\u001b[32m    275\u001b[39m     compression: Optional[grpc.Compression] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m ) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     response, ignored_call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_with_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:332\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m    329\u001b[39m call = \u001b[38;5;28mself\u001b[39m._interceptor.intercept_unary_unary(\n\u001b[32m    330\u001b[39m     continuation, client_call_details, request\n\u001b[32m    331\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, call\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:440\u001b[39m, in \u001b[36m_InactiveRpcError.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_interceptor.py:315\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[39m\u001b[34m(new_details, request)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     response, call = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_thunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_credentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwait_for_ready\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_wait_for_ready\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_compression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:1195\u001b[39m, in \u001b[36m_UnaryUnaryMultiCallable.with_call\u001b[39m\u001b[34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[39m\n\u001b[32m   1192\u001b[39m state, call = \u001b[38;5;28mself\u001b[39m._blocking(\n\u001b[32m   1193\u001b[39m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[32m   1194\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/grpc/_channel.py:1009\u001b[39m, in \u001b[36m_end_unary_response_blocking\u001b[39m\u001b[34m(state, call, with_call, deadline)\u001b[39m\n\u001b[32m   1008\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
            "\u001b[31m_InactiveRpcError\u001b[39m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\", grpc_status:14}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mServiceUnavailable\u001b[39m                        Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:147\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
            "\u001b[31mServiceUnavailable\u001b[39m: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mRetryError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load text embedding model from pre-trained source\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m text_embedding_model = \u001b[43mTextEmbeddingModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:290\u001b[39m, in \u001b[36m_ModelGardenModel.from_pretrained\u001b[39m\u001b[34m(cls, model_name)\u001b[39m\n\u001b[32m    279\u001b[39m credential_exception_str = (\n\u001b[32m    280\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUnable to authenticate your request.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDepending on your runtime environment, you can complete authentication by:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    286\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    287\u001b[39m )\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions.GoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m auth_exceptions.GoogleAuthError(credential_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:207\u001b[39m, in \u001b[36m_from_pretrained\u001b[39m\u001b[34m(interface_class, model_name, publisher_model, tuned_vertex_model)\u001b[39m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interface_class._INSTANCE_SCHEMA_URI:\n\u001b[32m    203\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    204\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterface_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a correct model interface class since it does not have an instance schema URI.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     model_info = \u001b[43m_get_model_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema_to_class_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_INSTANCE_SCHEMA_URI\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minterface_class\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    213\u001b[39m     schema_uri = publisher_model._gca_resource.predict_schemata.instance_schema_uri\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:123\u001b[39m, in \u001b[36m_get_model_info\u001b[39m\u001b[34m(model_id, schema_to_class_map, interface_class, publisher_model_res, tuned_vertex_model)\u001b[39m\n\u001b[32m    119\u001b[39m     model_id = \u001b[33m\"\u001b[39m\u001b[33mpublishers/google/models/\u001b[39m\u001b[33m\"\u001b[39m + model_id\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res:\n\u001b[32m    122\u001b[39m     publisher_model_res = (\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m         \u001b[43m_publisher_models\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_PublisherModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    124\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresource_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m._gca_resource\n\u001b[32m    126\u001b[39m     )\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res.name.startswith(\u001b[33m\"\u001b[39m\u001b[33mpublishers/google/models/\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    130\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly Google models are currently supported. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpublisher_model_res.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    131\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/cloud/aiplatform/_publisher_models.py:77\u001b[39m, in \u001b[36m_PublisherModel.__init__\u001b[39m\u001b[34m(self, resource_name, project, location, credentials)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     72\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` is not a valid PublisherModel resource \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mname or model garden id.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28mself\u001b[39m._gca_resource = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getter_method\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_resource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_DEFAULT_RETRY\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/cloud/aiplatform_v1/services/model_garden_service/client.py:927\u001b[39m, in \u001b[36mModelGardenServiceClient.get_publisher_model\u001b[39m\u001b[34m(self, request, name, retry, timeout, metadata)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_universe_domain()\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m response = \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:294\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m target = functools.partial(func, *args, **kwargs)\n\u001b[32m    291\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    293\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:156\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    155\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     next_sleep = \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[32m    167\u001b[39m     time.sleep(next_sleep)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:229\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deadline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time.monotonic() + next_sleep > deadline:\n\u001b[32m    224\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    225\u001b[39m         error_list,\n\u001b[32m    226\u001b[39m         RetryFailureReason.TIMEOUT,\n\u001b[32m    227\u001b[39m         original_timeout,\n\u001b[32m    228\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    230\u001b[39m _LOGGER.debug(\n\u001b[32m    231\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[33ms ...\u001b[39m\u001b[33m\"\u001b[39m.format(error_list[-\u001b[32m1\u001b[39m], next_sleep)\n\u001b[32m    232\u001b[39m )\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m next_sleep\n",
            "\u001b[31mRetryError\u001b[39m: Timeout of 120.0s exceeded, last exception: 503 Getting metadata from plugin failed with error: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate."
          ]
        }
      ],
      "source": [
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"gemini-embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ],
      "source": [
        "multimodal_embedding_model = GenerativeModel(\"gemini-2.5-flash\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'text_embedding_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultimodal_qa_with_rag_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     get_document_metadata,\n\u001b[32m      3\u001b[39m     set_global_variable,\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m set_global_variable(\u001b[33m\"\u001b[39m\u001b[33mtext_embedding_model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtext_embedding_model\u001b[49m)\n\u001b[32m      7\u001b[39m set_global_variable(\u001b[33m\"\u001b[39m\u001b[33mmultimodal_embedding_model\u001b[39m\u001b[33m\"\u001b[39m, multimodal_embedding_model)\n",
            "\u001b[31mNameError\u001b[39m: name 'text_embedding_model' is not defined"
          ]
        }
      ],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_document_metadata,\n",
        "    set_global_variable,\n",
        ")\n",
        "\n",
        "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
        "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tStqXX32RNYK",
        "tags": []
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'text_embedding_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmultimodal_qa_with_rag_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     get_document_metadata,\n\u001b[32m      3\u001b[39m     set_global_variable,\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m set_global_variable(\u001b[33m\"\u001b[39m\u001b[33mtext_embedding_model\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtext_embedding_model\u001b[49m)\n\u001b[32m      7\u001b[39m set_global_variable(\u001b[33m\"\u001b[39m\u001b[33mmultimodal_embedding_model\u001b[39m\u001b[33m\"\u001b[39m, multimodal_embedding_model)\n",
            "\u001b[31mNameError\u001b[39m: name 'text_embedding_model' is not defined"
          ]
        }
      ],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_document_metadata,\n",
        "    set_global_variable,\n",
        ")\n",
        "\n",
        "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
        "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\n",
            "\n",
            " Imagens atuais na pasta: 1\n",
            " Extraindo imagens do PDF para ter mais dados...\n",
            " Processando PDF: map/map.pdf\n",
            " PDF tem 1 pginas\n",
            " Pgina 1: 0 imagens encontradas\n",
            "\n",
            " Total de 0 imagens extradas!\n",
            " Nenhuma imagem foi extrada do PDF\n",
            "\n",
            " STATUS FINAL: 1 imagens na pasta 'images/'\n",
            "  Ainda h apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\n"
          ]
        }
      ],
      "source": [
        "# CLULA 78 (OPCIONAL) -  EXTRAIR MAIS IMAGENS DO PDF PARA COMPARAO\n",
        "# Esta clula extrai imagens do map.pdf para ter mais dados para comparao\n",
        "\n",
        "print(\"=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\\n\")\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def extrair_imagens_do_pdf(pdf_path, output_dir=\"images/\", prefixo=\"map\"):\n",
        "    \"\"\"\n",
        "    Extrai imagens de um PDF e salva na pasta de imagens\n",
        "    \"\"\"\n",
        "    print(f\" Processando PDF: {pdf_path}\")\n",
        "    \n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\" PDF no encontrado: {pdf_path}\")\n",
        "        return []\n",
        "    \n",
        "    # Criar diretrio se no existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Abrir PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    imagens_extraidas = []\n",
        "    \n",
        "    print(f\" PDF tem {len(doc)} pginas\")\n",
        "    \n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        images = page.get_images()\n",
        "        \n",
        "        print(f\" Pgina {page_num + 1}: {len(images)} imagens encontradas\")\n",
        "        \n",
        "        for img_index, img in enumerate(images):\n",
        "            try:\n",
        "                # Extrair imagem\n",
        "                xref = img[0]\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "                \n",
        "                # Converter para RGB se necessrio\n",
        "                if pix.colorspace and pix.colorspace.n > 3:\n",
        "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                \n",
        "                # Nome do arquivo\n",
        "                img_filename = f\"{prefixo}_page_{page_num + 1}_img_{img_index + 1}.png\"\n",
        "                img_path = os.path.join(output_dir, img_filename)\n",
        "                \n",
        "                # Salvar imagem\n",
        "                pix.save(img_path)\n",
        "                imagens_extraidas.append(img_path)\n",
        "                \n",
        "                print(f\"   Extrada: {img_filename}\")\n",
        "                \n",
        "                pix = None  # Liberar memria\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"   Erro ao extrair imagem {img_index}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    doc.close()\n",
        "    print(f\"\\n Total de {len(imagens_extraidas)} imagens extradas!\")\n",
        "    return imagens_extraidas\n",
        "\n",
        "# Verificar quantas imagens temos atualmente\n",
        "current_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\" Imagens atuais na pasta: {current_images}\")\n",
        "\n",
        "if current_images <= 1:\n",
        "    print(\" Extraindo imagens do PDF para ter mais dados...\")\n",
        "    \n",
        "    # Extrair do map.pdf se existir\n",
        "    if os.path.exists(\"map/map.pdf\"):\n",
        "        imagens_extraidas = extrair_imagens_do_pdf(\"map/map.pdf\", \"images/\", \"map\")\n",
        "        \n",
        "        if imagens_extraidas:\n",
        "            print(f\"\\n {len(imagens_extraidas)} novas imagens adicionadas!\")\n",
        "            print(\" Agora execute a CLULA 76 novamente para processar todas as imagens\")\n",
        "            print(\"   Depois execute a CLULA 70 para testar similaridade com mais dados\")\n",
        "        else:\n",
        "            print(\" Nenhuma imagem foi extrada do PDF\")\n",
        "    else:\n",
        "        print(\" Arquivo map/map.pdf no encontrado\")\n",
        "        \n",
        "        # Verificar outros PDFs disponveis\n",
        "        print(\"\\n Procurando outros PDFs...\")\n",
        "        pdf_paths = []\n",
        "        for root, dirs, files in os.walk(\".\"):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    pdf_paths.append(os.path.join(root, file))\n",
        "        \n",
        "        if pdf_paths:\n",
        "            print(\" PDFs encontrados:\")\n",
        "            for i, pdf_path in enumerate(pdf_paths[:3], 1):  # Mostrar apenas os 3 primeiros\n",
        "                print(f\"  {i}. {pdf_path}\")\n",
        "                \n",
        "            # Processar o primeiro PDF encontrado\n",
        "            if pdf_paths:\n",
        "                primeiro_pdf = pdf_paths[0]\n",
        "                print(f\"\\n Processando: {primeiro_pdf}\")\n",
        "                imagens_extraidas = extrair_imagens_do_pdf(primeiro_pdf, \"images/\", \"doc\")\n",
        "                \n",
        "                if imagens_extraidas:\n",
        "                    print(f\"\\n {len(imagens_extraidas)} imagens extradas de {primeiro_pdf}!\")\n",
        "                    print(\" Execute a CLULA 76 novamente para processar todas as imagens\")\n",
        "        else:\n",
        "            print(\" Nenhum PDF encontrado para extrair imagens\")\n",
        "            \n",
        "else:\n",
        "    print(\" J h mltiplas imagens na pasta\")\n",
        "    print(\"Execute a CLULA 76 para processar todas e depois a CLULA 70 para testar similaridade\")\n",
        "\n",
        "# Mostrar status final\n",
        "final_images = len([f for f in os.listdir(\"images/\") if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))])\n",
        "print(f\"\\n STATUS FINAL: {final_images} imagens na pasta 'images/'\")\n",
        "\n",
        "if final_images > 1:\n",
        "    print(\" Pronto para testar busca por similaridade!\")\n",
        "    print(\" PRXIMOS PASSOS:\")\n",
        "    print(\"  1. Execute CLULA 76 (processar todas as imagens)\")\n",
        "    print(\"  2. Execute CLULA 70 (busca por similaridade)\")\n",
        "    print(\"  3. Execute CLULA 71 (anlise contextual)\")\n",
        "else:\n",
        "    print(\"  Ainda h apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Funo 'processar_imagens_da_pasta' criada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# CLULA 75 (NOVO) -  PROCESSAMENTO DIRETO DE IMAGENS DA PASTA\n",
        "# Funo para ler todas as imagens da pasta images/ e gerar embeddings para RAG\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_image_embedding_from_multimodal_embedding_model,\n",
        "    get_gemini_response\n",
        ")\n",
        "\n",
        "def processar_imagens_da_pasta(\n",
        "    pasta_imagens=\"images/\",\n",
        "    embedding_size=512,\n",
        "    gerar_descricoes=True,\n",
        "    formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "):\n",
        "    \"\"\"\n",
        "    Processa todas as imagens de uma pasta, gerando embeddings e descries para RAG\n",
        "    \n",
        "    Args:\n",
        "        pasta_imagens: Caminho da pasta com imagens\n",
        "        embedding_size: Tamanho do embedding (128, 256, 512, 1408)\n",
        "        gerar_descricoes: Se deve gerar descries das imagens com Gemini\n",
        "        formatos_suportados: Lista de formatos de imagem aceitos\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame compatvel com o sistema RAG existente\n",
        "    \"\"\"\n",
        "    print(f\" PROCESSANDO IMAGENS DA PASTA: {pasta_imagens}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Verificar se a pasta existe\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        print(f\" Pasta '{pasta_imagens}' no encontrada!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Encontrar todas as imagens na pasta\n",
        "    imagens_encontradas = []\n",
        "    for formato in formatos_suportados:\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato.upper()}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "    \n",
        "    # Remover duplicatas\n",
        "    imagens_encontradas = list(set(imagens_encontradas))\n",
        "    \n",
        "    if not imagens_encontradas:\n",
        "        print(f\" Nenhuma imagem encontrada na pasta '{pasta_imagens}'\")\n",
        "        print(f\"Formatos suportados: {formatos_suportados}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\" Encontradas {len(imagens_encontradas)} imagens:\")\n",
        "    for img in imagens_encontradas:\n",
        "        print(f\"  - {os.path.basename(img)}\")\n",
        "    \n",
        "    # Lista para armazenar dados processados\n",
        "    dados_imagens = []\n",
        "    \n",
        "    # Prompt para descrio das imagens\n",
        "    prompt_descricao = \"\"\"Analise esta imagem detalhadamente e fornea uma descrio precisa.\n",
        "    Inclua:\n",
        "    - O que voc v na imagem\n",
        "    - Elementos principais e detalhes importantes\n",
        "    - Texto visvel (se houver)\n",
        "    - Tipo de imagem (mapa, diagrama, foto, etc.)\n",
        "    - Informaes relevantes para busca e recuperao\n",
        "    \n",
        "    Seja especfico e detalhado para facilitar buscas futuras.\"\"\"\n",
        "    \n",
        "    print(f\"\\n PROCESSANDO CADA IMAGEM...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, caminho_imagem in enumerate(imagens_encontradas, 1):\n",
        "        nome_arquivo = os.path.basename(caminho_imagem)\n",
        "        print(f\"\\n PROCESSANDO {i}/{len(imagens_encontradas)}: {nome_arquivo}\")\n",
        "        \n",
        "        try:\n",
        "            # 1. Gerar embedding da imagem\n",
        "            print(\"   Gerando embedding...\")\n",
        "            image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
        "                image_uri=caminho_imagem,\n",
        "                embedding_size=embedding_size,\n",
        "                return_array=True\n",
        "            )\n",
        "            print(f\"   Embedding gerado: shape {image_embedding.shape}\")\n",
        "            \n",
        "            # 2. Gerar descrio da imagem (se solicitado)\n",
        "            descricao = \"\"\n",
        "            if gerar_descricoes:\n",
        "                print(\"   Gerando descrio com Gemini...\")\n",
        "                try:\n",
        "                    from vertexai.generative_models import Image as GeminiImage\n",
        "                    imagem_gemini = GeminiImage.load_from_file(caminho_imagem)\n",
        "                    \n",
        "                    descricao = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[prompt_descricao, imagem_gemini],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"   Descrio gerada: {len(descricao)} caracteres\")\n",
        "                    \n",
        "                except Exception as desc_error:\n",
        "                    print(f\"    Erro ao gerar descrio: {desc_error}\")\n",
        "                    descricao = f\"Imagem: {nome_arquivo}\"\n",
        "            \n",
        "            # 3. Gerar embedding da descrio (para compatibilidade com RAG)\n",
        "            text_embedding = None\n",
        "            if descricao:\n",
        "                try:\n",
        "                    from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
        "                    text_embedding = get_text_embedding_from_text_embedding_model(descricao)\n",
        "                    print(\"   Text embedding da descrio gerado\")\n",
        "                except Exception as text_emb_error:\n",
        "                    print(f\"    Erro ao gerar text embedding: {text_emb_error}\")\n",
        "            \n",
        "            # 4. Criar registro compatvel com o sistema existente\n",
        "            registro = {\n",
        "                'file_name': f\"pasta_images_{nome_arquivo}\",  # Nome nico\n",
        "                'page_num': 1,  # Imagens individuais = pgina 1\n",
        "                'img_num': i,\n",
        "                'img_path': caminho_imagem,\n",
        "                'img_desc': descricao,\n",
        "                'mm_embedding_from_img_only': image_embedding.tolist(),  # Compatibilidade\n",
        "                'text_embedding_from_image_description': text_embedding if text_embedding else None,\n",
        "                'source_type': 'pasta_imagens',  # Identificar origem\n",
        "                'original_filename': nome_arquivo\n",
        "            }\n",
        "            \n",
        "            dados_imagens.append(registro)\n",
        "            print(f\"   Processamento concludo para {nome_arquivo}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   Erro ao processar {nome_arquivo}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    if dados_imagens:\n",
        "        df_imagens = pd.DataFrame(dados_imagens)\n",
        "        print(f\"\\n PROCESSAMENTO CONCLUDO!\")\n",
        "        print(f\" DataFrame criado com {len(df_imagens)} imagens processadas\")\n",
        "        print(f\" Colunas: {list(df_imagens.columns)}\")\n",
        "        \n",
        "        return df_imagens\n",
        "    else:\n",
        "        print(f\"\\n Nenhuma imagem foi processada com sucesso\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\" Funo 'processar_imagens_da_pasta' criada com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\n",
            "\n",
            " PROCESSANDO IMAGENS DA PASTA: images/\n",
            "============================================================\n",
            " Encontradas 1 imagens:\n",
            "  - B2_room.jpeg\n",
            "\n",
            " PROCESSANDO CADA IMAGEM...\n",
            "============================================================\n",
            "\n",
            " PROCESSANDO 1/1: B2_room.jpeg\n",
            "   Gerando embedding...\n",
            "   Erro ao processar B2_room.jpeg: 'GenerativeModel' object has no attribute 'get_embeddings'\n",
            "\n",
            " Nenhuma imagem foi processada com sucesso\n",
            "\n",
            " FALHA: Nenhuma imagem foi processada\n",
            "Verifique se:\n",
            "- A pasta 'images/' existe\n",
            "- H imagens vlidas na pasta\n",
            "- Os modelos esto carregados corretamente\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/vision_models/_vision_models.py:153: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n"
          ]
        }
      ],
      "source": [
        "# CLULA 76 (EXECUTAR) -  PROCESSAMENTO DAS IMAGENS DA PASTA images/\n",
        "# Executa o processamento de todas as imagens e cria o image_metadata_df\n",
        "\n",
        "print(\"=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\\n\")\n",
        "\n",
        "# Executar o processamento das imagens\n",
        "try:\n",
        "    image_metadata_df = processar_imagens_da_pasta(\n",
        "        pasta_imagens=\"images/\",\n",
        "        embedding_size=512,\n",
        "        gerar_descricoes=True,  # Gerar descries detalhadas com Gemini\n",
        "        formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "    )\n",
        "    \n",
        "    if not image_metadata_df.empty:\n",
        "        print(f\"\\n SUCESSO TOTAL!\")\n",
        "        print(f\" image_metadata_df criado com {len(image_metadata_df)} imagens\")\n",
        "        \n",
        "        # Mostrar resumo das imagens processadas\n",
        "        print(f\"\\n RESUMO DAS IMAGENS PROCESSADAS:\")\n",
        "        print(\"=\"*50)\n",
        "        for idx, row in image_metadata_df.iterrows():\n",
        "            print(f\"\\n  Imagem {idx + 1}:\")\n",
        "            print(f\"   Arquivo: {row['original_filename']}\")\n",
        "            print(f\"   Caminho: {row['img_path']}\")\n",
        "            print(f\"   Embedding shape: {len(row['mm_embedding_from_img_only'])}\")\n",
        "            \n",
        "            # Mostrar incio da descrio\n",
        "            desc = row['img_desc']\n",
        "            if desc and len(desc) > 10:\n",
        "                print(f\"   Descrio: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
        "        \n",
        "        # Verificar compatibilidade com sistema RAG existente\n",
        "        print(f\"\\n COMPATIBILIDADE COM SISTEMA RAG:\")\n",
        "        colunas_necessarias = ['img_path', 'mm_embedding_from_img_only', 'img_desc', 'file_name', 'page_num']\n",
        "        for col in colunas_necessarias:\n",
        "            if col in image_metadata_df.columns:\n",
        "                print(f\"   {col}: OK\")\n",
        "            else:\n",
        "                print(f\"   {col}: FALTANDO\")\n",
        "        \n",
        "        # Salvar para uso futuro (opcional)\n",
        "        try:\n",
        "            image_metadata_df.to_pickle(\"image_metadata_from_folder.pkl\")\n",
        "            print(f\"\\n DataFrame salvo em 'image_metadata_from_folder.pkl'\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"\\n  No foi possvel salvar: {save_error}\")\n",
        "        \n",
        "        print(f\"\\n PRXIMOS PASSOS:\")\n",
        "        print(f\"1. Agora voc pode executar a CLULA 70 (Validao)\")\n",
        "        print(f\"2. Depois executar a CLULA 71 (Anlise Contextual)\")\n",
        "        print(f\"3. O sistema RAG est pronto para perguntas sobre as imagens!\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n FALHA: Nenhuma imagem foi processada\")\n",
        "        print(f\"Verifique se:\")\n",
        "        print(f\"- A pasta 'images/' existe\")\n",
        "        print(f\"- H imagens vlidas na pasta\")\n",
        "        print(f\"- Os modelos esto carregados corretamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\" ERRO NO PROCESSAMENTO: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\n POSSVEIS SOLUES:\")\n",
        "    print(f\"- Verifique se os modelos esto carregados\")\n",
        "    print(f\"- Verifique se a pasta 'images/' existe\")\n",
        "    print(f\"- Execute as clulas de setup dos modelos primeiro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== M3.JPEG SIMILARITY SEARCH VALIDATION ===\n",
            "\n",
            " image_metadata_df available!\n",
            " Dataset: 1 images processed\n",
            "\n",
            " IMAGES IN DATASET:\n",
            "  1. B2_room.jpeg\n",
            "\n",
            " M3.jpeg not found in dataset!\n",
            "Check if the image is in the 'images/' folder and execute CELL 76 again\n"
          ]
        }
      ],
      "source": [
        "# CELL 70 (STEP 3) -  VALIDATION: Similarity search with M3.jpeg in the dataset\n",
        "# This cell uses the image_metadata_df created by Cell 76 to search for similar images\n",
        "\n",
        "print(\"=== M3.JPEG SIMILARITY SEARCH VALIDATION ===\\n\")\n",
        "\n",
        "# Check if we have the image metadata DataFrame\n",
        "if 'image_metadata_df' not in locals():\n",
        "    print(\" image_metadata_df not found!\")\n",
        "    print(\"\\n To resolve:\")\n",
        "    print(\"1. Execute CELL 78 first (Extract images from PDF)\")\n",
        "    print(\"2. Execute CELL 76 next (Process Images)\")\n",
        "    print(\"3. Then execute this cell again\")\n",
        "    \n",
        "elif image_metadata_df.empty:\n",
        "    print(\" image_metadata_df is empty!\")\n",
        "    print(\"Execute CELL 76 to process images from the folder\")\n",
        "    \n",
        "else:\n",
        "    print(\" image_metadata_df available!\")\n",
        "    print(f\" Dataset: {len(image_metadata_df)} images processed\")\n",
        "    \n",
        "    # Show all images in the dataset\n",
        "    print(f\"\\n IMAGES IN DATASET:\")\n",
        "    for idx, row in image_metadata_df.iterrows():\n",
        "        print(f\"  {idx + 1}. {row['original_filename']}\")\n",
        "    \n",
        "    # Find M3.jpeg in the dataset\n",
        "    m3_rows = image_metadata_df[image_metadata_df['original_filename'].str.contains('M3.jpeg', case=False, na=False)]\n",
        "    \n",
        "    if m3_rows.empty:\n",
        "        print(\"\\n M3.jpeg not found in dataset!\")\n",
        "        print(\"Check if the image is in the 'images/' folder and execute CELL 76 again\")\n",
        "    else:\n",
        "        print(f\"\\n M3.jpeg found in dataset!\")\n",
        "        m3_row = m3_rows.iloc[0]\n",
        "        print(f\"   File: {m3_row['original_filename']}\")\n",
        "        print(f\"   Path: {m3_row['img_path']}\")\n",
        "        \n",
        "        # Extract M3.jpeg embedding\n",
        "        m3_embedding = np.array(m3_row['mm_embedding_from_img_only'])\n",
        "        print(f\"   Embedding shape: {m3_embedding.shape}\")\n",
        "        \n",
        "        # Create dataset without M3.jpeg itself for comparison\n",
        "        other_images_df = image_metadata_df[~image_metadata_df['original_filename'].str.contains('M3.jpeg', case=False, na=False)]\n",
        "        \n",
        "        if other_images_df.empty:\n",
        "            print(\"\\n  Only M3.jpeg found in dataset\")\n",
        "            print(\"Execute CELL 78 first to extract more images from PDF\")\n",
        "            print(\"Then execute CELL 76 to process all images\")\n",
        "        else:\n",
        "            print(f\"\\n EXECUTING SIMILARITY SEARCH...\")\n",
        "            print(f\" Comparing M3.jpeg with {len(other_images_df)} other images\")\n",
        "            \n",
        "            # Use our robust alternative function\n",
        "            try:\n",
        "                similar_results = buscar_imagens_similares_com_embedding(\n",
        "                    image_embedding=m3_embedding,\n",
        "                    image_metadata_df=other_images_df,\n",
        "                    top_n=min(5, len(other_images_df))  # Top N most similar images\n",
        "                )\n",
        "                \n",
        "                if similar_results:\n",
        "                    print(f\"\\n SUCCESS! Found {len(similar_results)} images similar to M3.jpeg:\")\n",
        "                    print(\"=\"*80)\n",
        "                    \n",
        "                    for i, result in enumerate(similar_results, 1):\n",
        "                        print(f\"\\n  RESULT {i}:\")\n",
        "                        print(f\"   Similarity: {result['cosine_score']:.4f}\")\n",
        "                        print(f\"   File: {result['file_name']}\")\n",
        "                        print(f\"   Path: {result['img_path']}\")\n",
        "                        \n",
        "                        # Show description if available\n",
        "                        desc = result['img_desc']\n",
        "                        if desc and desc != 'N/A' and len(str(desc)) > 10:\n",
        "                            desc_str = str(desc)\n",
        "                            print(f\"   Description: {desc_str[:200]}{'...' if len(desc_str) > 200 else ''}\")\n",
        "                    \n",
        "                    # Save results for later use\n",
        "                    globals()['matching_results'] = similar_results\n",
        "                    print(f\"\\n Results saved to 'matching_results' variable\")\n",
        "                    \n",
        "                    # Similarity score analysis\n",
        "                    scores = [r['cosine_score'] for r in similar_results]\n",
        "                    print(f\"\\n SCORE ANALYSIS:\")\n",
        "                    print(f\"  - Maximum score: {max(scores):.4f}\")\n",
        "                    print(f\"  - Minimum score: {min(scores):.4f}\")\n",
        "                    print(f\"  - Average score: {sum(scores)/len(scores):.4f}\")\n",
        "                    \n",
        "                    print(f\"\\n VALIDATION COMPLETED SUCCESSFULLY!\")\n",
        "                    print(f\" Now you can execute CELL 71 for contextual analysis!\")\n",
        "                    \n",
        "                else:\n",
        "                    print(\" No similar images found.\")\n",
        "                    print(\"This may indicate problems with embeddings or very different data.\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                print(f\" Error during search: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                \n",
        "                print(f\"\\n DIAGNOSIS:\")\n",
        "                print(f\"- Check if function 'buscar_imagens_similares_com_embedding' was defined (CELL 75)\")\n",
        "                print(f\"- Check if there are other images besides M3.jpeg in the dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ANLISE CONTEXTUAL DA M3.JPEG ===\n",
            "\n",
            " Nenhum resultado de busca por similaridade encontrado.\n",
            "Execute a clula anterior (70) primeiro para obter os resultados.\n",
            "\n",
            " Como alternativa, vou fazer uma anlise direta da M3.jpeg:\n",
            " Erro na anlise direta: [Errno 2] No such file or directory: 'images/M3.jpeg'\n",
            "\n",
            " ANLISE CONTEXTUAL CONCLUDA!\n",
            "A M3.jpeg foi analisada usando o contexto das imagens similares encontradas via embedding.\n"
          ]
        }
      ],
      "source": [
        "# Novo Cdigo\n",
        "\n",
        "# CLULA 71 (PASSO 4) -  ANLISE CONTEXTUAL: Perguntas sobre M3.jpeg com base nas imagens similares\n",
        "# Esta clula usa os resultados da busca por similaridade para anlise contextual\n",
        "\n",
        "print(\"=== ANLISE CONTEXTUAL DA M3.JPEG ===\\n\")\n",
        "\n",
        "# Verificar se temos resultados da busca anterior\n",
        "if 'matching_results' not in locals() or not matching_results:\n",
        "    print(\" Nenhum resultado de busca por similaridade encontrado.\")\n",
        "    print(\"Execute a clula anterior (70) primeiro para obter os resultados.\")\n",
        "    \n",
        "    print(\"\\n Como alternativa, vou fazer uma anlise direta da M3.jpeg:\")\n",
        "    \n",
        "    try:\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        m3_image = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "        \n",
        "        pergunta_direta = \"\"\"Analise esta imagem detalhadamente. \n",
        "        O que voc v? Descreva todos os elementos visveis.\n",
        "        Se  um mapa ou planta, identifique os elementos principais.\"\"\"\n",
        "        \n",
        "        resposta_direta = get_gemini_response(\n",
        "            multimodal_model_2_0_flash,\n",
        "            model_input=[pergunta_direta, m3_image],\n",
        "            stream=False,\n",
        "        )\n",
        "        \n",
        "        print(f\" ANLISE DIRETA DA M3.JPEG:\")\n",
        "        print(f\"{resposta_direta}\")\n",
        "        \n",
        "    except Exception as direct_error:\n",
        "        print(f\" Erro na anlise direta: {direct_error}\")\n",
        "\n",
        "else:\n",
        "    print(f\" Temos {len(matching_results)} resultados de busca por similaridade!\")\n",
        "    \n",
        "    # Preparar contexto baseado nos resultados similares\n",
        "    contexto_descricoes = []\n",
        "    contexto_caminhos = []\n",
        "    \n",
        "    for i, result in enumerate(matching_results):\n",
        "        desc = result.get('img_desc', '')\n",
        "        caminho = result.get('img_path', '')\n",
        "        arquivo = result.get('file_name', '')\n",
        "        score = result.get('cosine_score', 0)\n",
        "        \n",
        "        if desc and desc != 'N/A' and len(str(desc)) > 10:\n",
        "            contexto_descricoes.append(f\"Imagem similar {i+1} (similaridade: {score:.3f}): {desc}\")\n",
        "        \n",
        "        if caminho and caminho != 'N/A':\n",
        "            contexto_caminhos.append(caminho)\n",
        "    \n",
        "    print(f\" Coletadas {len(contexto_descricoes)} descries de imagens similares\")\n",
        "    \n",
        "    # Perguntas especficas sobre a M3.jpeg\n",
        "    perguntas_contextualizadas = [\n",
        "        \"Baseado nas imagens similares encontradas, o que voc pode me dizer sobre a M3.jpeg?\",\n",
        "        \"Quais elementos em comum existem entre a M3.jpeg e as imagens similares?\",\n",
        "        \"Se a M3.jpeg  um mapa ou planta, quais informaes especficas posso extrair?\",\n",
        "        \"H algum padro arquitetnico ou de layout visvel na M3.jpeg?\",\n",
        "        \"What are the rooms in this floor? (baseado no contexto das imagens similares)\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n ANLISE CONTEXTUAL COM GEMINI:\")\n",
        "    \n",
        "    try:\n",
        "        # Carregar a imagem M3.jpeg\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        m3_image = GeminiImage.load_from_file(\"images/M3.jpeg\")\n",
        "        \n",
        "        # Preparar contexto das imagens similares\n",
        "        contexto_texto = \"\\n\".join(contexto_descricoes[:3])  # Top 3 descries\n",
        "        \n",
        "        for i, pergunta in enumerate(perguntas_contextualizadas, 1):\n",
        "            print(f\"\\n\" + \"=\"*70)\n",
        "            print(f\" PERGUNTA {i}: {pergunta}\")\n",
        "            print(\"=\"*70)\n",
        "            \n",
        "            # Criar prompt contextualizado\n",
        "            prompt_contextualizado = f\"\"\"\n",
        "            Analise a imagem fornecida considerando o seguinte contexto de imagens similares:\n",
        "            \n",
        "            CONTEXTO DE IMAGENS SIMILARES ENCONTRADAS:\n",
        "            {contexto_texto}\n",
        "            \n",
        "            PERGUNTA ESPECFICA:\n",
        "            {pergunta}\n",
        "            \n",
        "            Por favor, fornea uma resposta detalhada baseada tanto na anlise visual da imagem \n",
        "            quanto no contexto das imagens similares fornecido acima.\n",
        "            \"\"\"\n",
        "            \n",
        "            try:\n",
        "                resposta = get_gemini_response(\n",
        "                    multimodal_model_2_0_flash,\n",
        "                    model_input=[prompt_contextualizado, m3_image],\n",
        "                    stream=False,\n",
        "                )\n",
        "                \n",
        "                print(f\" RESPOSTA CONTEXTUALIZADA:\")\n",
        "                print(f\"{resposta}\")\n",
        "                \n",
        "            except Exception as gemini_error:\n",
        "                print(f\" Erro na anlise contextual: {gemini_error}\")\n",
        "                \n",
        "                # Fallback: anlise simples sem contexto\n",
        "                try:\n",
        "                    resposta_simples = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[pergunta, m3_image],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\" RESPOSTA SIMPLES (sem contexto):\")\n",
        "                    print(f\"{resposta_simples}\")\n",
        "                    \n",
        "                except Exception as simple_error:\n",
        "                    print(f\" Erro na anlise simples: {simple_error}\")\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\" Erro ao carregar imagem: {e}\")\n",
        "    \n",
        "    # Mostrar resumo final\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\" RESUMO DOS RESULTADOS DE SIMILARIDADE:\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for i, result in enumerate(matching_results, 1):\n",
        "        print(f\"\\n  Imagem Similar {i}:\")\n",
        "        print(f\"   Similaridade: {result.get('cosine_score', 0):.4f}\")\n",
        "        print(f\"   Arquivo: {result.get('file_name', 'N/A')}\")\n",
        "        print(f\"   Pgina: {result.get('page_num', 'N/A')}\")\n",
        "        print(f\"   Caminho: {result.get('img_path', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n ANLISE CONTEXTUAL CONCLUDA!\")\n",
        "print(f\"A M3.jpeg foi analisada usando o contexto das imagens similares encontradas via embedding.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DIRECT M3.JPEG ANALYSIS WITH GEMINI ===\n",
            "\n",
            " Gemini 2.0 Flash model available\n",
            " Analyzing image with Gemini...\n",
            " Image loaded: images/B2_room.jpeg\n",
            "\n",
            " QUESTION 1: What are the rooms or areas shown in this floor plan?\n",
            "------------------------------------------------------------\n",
            " RESPONSE:\n",
            "Based on the floor plan provided, here are the rooms or areas that are visible:\n",
            "\n",
            "*   **Rooms:**\n",
            "    *   Room 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2010, 2012, 2015, 2025, 2029, 2032, 2034, 2035, 2036, 2037\n",
            "*   **Amenities:**\n",
            "    *   Washrooms\n",
            "    *   Women's Washroom\n",
            "    *   Women's Accessible Washroom\n",
            "    *   Accessible Washroom\n",
            "    *   Men's Washroom\n",
            "    *   Men's Accessible Washroom\n",
            "    *   Accessible Shower and Changeroom\n",
            "    *   Elevator\n",
            "    *   Stairs\n",
            "    *   Bus Stop\n",
            "    *   Bike Parking\n",
            "    *   Information\n",
            "    *   Food\n",
            "    *   Interior and exterior pathways\n",
            "\n",
            "Other items:\n",
            "\n",
            "*   T\n",
            "*   D\n",
            "*   You are here\n",
            "*   Exit\n",
            "*   Adjacent Buildings\n",
            "\n",
            " QUESTION 2: How can I go from room 2001 to room 2037?\n",
            "------------------------------------------------------------\n",
            " RESPONSE:\n",
            "Okay, based on the map provided:\n",
            "\n",
            "1.  **Exit Room 2001:** Leave room 2001.\n",
            "2.  **Turn Left:** Turn left as you exit the room.\n",
            "3.  **Walk to the End of the Hall:** Walk to the end of the hall that contains the rooms in the 2000s. Room 2037 is at the other end of that hallway.\n",
            "\n",
            " Analysis completed!\n"
          ]
        }
      ],
      "source": [
        "# CELL 72 -  DIRECT M3.JPEG ANALYSIS WITH GEMINI (CORRECTED)\n",
        "# Corrected function to ask for figure details using the Gemini model\n",
        "\n",
        "print(\"=== DIRECT M3.JPEG ANALYSIS WITH GEMINI ===\\n\")\n",
        "\n",
        "def ask_figure_details_corrected(model, image_path=None):\n",
        "    \"\"\"\n",
        "    Automatically asks for figure details from the multimodal Gemini model.\n",
        "    Args:\n",
        "        model: loaded Gemini model\n",
        "        image_path: path to the image (string)\n",
        "    \"\"\"\n",
        "    print(\" Analyzing image with Gemini...\")\n",
        "    \n",
        "    # Specific questions about the image\n",
        "    questions = [\n",
        "        \"What are the rooms or areas shown in this floor plan?\",\n",
        "        \"How can I go from room 2001 to room 2037?\"\n",
        "    ]\n",
        "    try:\n",
        "        # Load the image\n",
        "        from vertexai.generative_models import Image as GeminiImage\n",
        "        image_object = GeminiImage.load_from_file(image_path)\n",
        "        print(f\" Image loaded: {image_path}\")\n",
        "        \n",
        "        # Ask each question\n",
        "        for i, question in enumerate(questions, 1):\n",
        "            print(f\"\\n QUESTION {i}: {question}\")\n",
        "            print(\"-\" * 60)\n",
        "            \n",
        "            try:\n",
        "                # Use the model directly (most reliable method)\n",
        "                response = model.generate_content([question, image_object])\n",
        "                response_text = response.text if hasattr(response, 'text') else str(response)\n",
        "                \n",
        "                print(f\" RESPONSE:\")\n",
        "                print(f\"{response_text}\")\n",
        "                \n",
        "            except Exception as question_error:\n",
        "                print(f\" Error in question {i}: {question_error}\")\n",
        "                \n",
        "                # Try alternative method with get_gemini_response\n",
        "                try:\n",
        "                    alt_response = get_gemini_response(\n",
        "                        model,\n",
        "                        model_input=[question, image_object],\n",
        "                        stream=False\n",
        "                    )\n",
        "                    print(f\" RESPONSE (alternative method):\")\n",
        "                    print(f\"{alt_response}\")\n",
        "                except Exception as alt_error:\n",
        "                    print(f\" Alternative method also failed: {alt_error}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\" General error analyzing image: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Test the corrected function automatically\n",
        "try:\n",
        "    # Check if the model is available\n",
        "    if 'multimodal_model_2_0_flash' in locals():\n",
        "        print(\" Gemini 2.0 Flash model available\")\n",
        "        \n",
        "        # Test with M3.jpeg\n",
        "        ask_figure_details_corrected(\n",
        "            multimodal_model_2_0_flash, \n",
        "            image_path=\"images/B2_room.jpeg\"\n",
        "        )\n",
        "        \n",
        "    else:\n",
        "        print(\" multimodal_model_2_0_flash model not found\")\n",
        "        print(\"Execute the model setup cells first\")\n",
        "        \n",
        "        # Try to load basic model\n",
        "        try:\n",
        "            from vertexai.generative_models import GenerativeModel\n",
        "            temp_model = GenerativeModel(\"gemini-1.5-flash\")\n",
        "            print(\" Using Gemini 1.5 Flash model as alternative...\")\n",
        "            \n",
        "            ask_figure_details_corrected(\n",
        "                temp_model,\n",
        "                image_path=\"images/M3.jpeg\"\n",
        "            )\n",
        "        except Exception as model_error:\n",
        "            print(f\" Error loading alternative model: {model_error}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\" Error executing analysis: {e}\")\n",
        "    print(\"Check if:\")\n",
        "    print(\"- The image 'images/M3.jpeg' exists\")\n",
        "    print(\"- Vertex AI models are configured\")\n",
        "    print(\"- Dependencies are installed\")\n",
        "\n",
        "print(f\"\\n Analysis completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m116",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
