{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJqZ76rJh2rM",
        "outputId": "c7648628-a72f-467b-bca4-b7fb003885fe",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your project ID is: gen-lang-client-0303567819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your active configuration is: [personal]\n"
          ]
        }
      ],
      "source": [
        "# Define project information\n",
        "\n",
        "import sys\n",
        "\n",
        "PROJECT_ID = \"gen-lang-client-0303567819\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# if not running on colab, try to get the PROJECT_ID automatically\n",
        "if \"google.colab\" not in sys.modules:\n",
        "    import subprocess\n",
        "\n",
        "    PROJECT_ID = subprocess.check_output(\n",
        "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
        "    ).strip()\n",
        "\n",
        "print(f\"Your project ID is: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "D48gUW5-h2rM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Initialize Vertex AI\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "rtMowvm-yQ97",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from rich import print as rich_print\n",
        "from rich.markdown import Markdown as rich_Markdown\n",
        "from IPython.display import Markdown, display\n",
        "from vertexai.generative_models import (\n",
        "    Content,\n",
        "    GenerationConfig,\n",
        "    GenerationResponse,\n",
        "    GenerativeModel,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    Image,\n",
        "    Part,\n",
        ")\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "# Suppress deprecation warning for vision_models (deprecated until June 2026)\n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*vision_models.*\")\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*deprecated.*\")\n",
        "    from vertexai.vision_models import MultiModalEmbeddingModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SvMwSRJJh2rM",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1762616269.378261   35855 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1762616269.657345   35855 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "\n",
        "multimodal_model_2_0_flash = GenerativeModel(\n",
        "    \"gemini-2.0-flash-001\"\n",
        ") # Gemini latest Gemini 2.0 Flash Model\n",
        "\n",
        "multimodal_model_15 = GenerativeModel(\n",
        "    \"gemini-1.5-pro-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - complex reasoning\n",
        "\n",
        "# Multimodal models: Choose based on your performance/cost needs\n",
        "multimodal_model_15_flash = GenerativeModel(\n",
        "    \"gemini-1.5-flash-001\"\n",
        ")  # works with text, code, images, video(with or without audio) and audio(mp3) with 1M input context - faster inference\n",
        "\n",
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "\n",
        "# Load multimodal embedding model from pre-trained source\n",
        "# Suppress deprecation warning when loading the model\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*vision_models.*\")\n",
        "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*deprecated.*\")\n",
        "    multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\n",
        "        \"multimodalembedding@001\"\n",
        "    )  # works with image, image with caption(~32 words), video, video with caption(~32 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/luizeng/Documents/fanshawe_repo/test-image-text-gemini/.venv/lib/python3.13/site-packages/vertexai/_model_garden/_model_garden_models.py:278: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1762616269.891412   35855 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "# Load text embedding model from pre-trained source\n",
        "text_embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NOTE: multimodal_embedding_model is already correctly defined in Cell 3\n",
        "# Do not overwrite it with a GenerativeModel - it must be a MultiModalEmbeddingModel\n",
        "# If you need a different generative model, use a different variable name\n",
        "# multimodal_embedding_model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding@001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_document_metadata,\n",
        "    set_global_variable,\n",
        ")\n",
        "\n",
        "set_global_variable(\"text_embedding_model\", text_embedding_model)\n",
        "set_global_variable(\"multimodal_embedding_model\", multimodal_embedding_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\n",
            "\n",
            "üìÅ Buscando imagens na pasta 'images/' e subdiret√≥rios...\n",
            "\n",
            "üìÇ Arquivos na pasta 'images/' (diret√≥rio raiz):\n",
            "  - A1.png (73671 bytes) [extens√£o: .png]\n",
            "  - B2_room.jpeg (103810 bytes) [extens√£o: .jpeg]\n",
            "\n",
            "üìä Total de imagens encontradas: 2\n",
            "‚úÖ Imagens detectadas:\n",
            "  - A1.png\n",
            "  - B2_room.jpeg\n",
            "‚úÖ J√° h√° m√∫ltiplas imagens na pasta\n",
            "Execute a C√âLULA 76 para processar todas e depois a C√âLULA 70 para testar similaridade\n",
            "\n",
            "üìä STATUS FINAL:\n",
            "üìä Total: 2 imagens na pasta 'images/'\n",
            "üìã Imagens encontradas:\n",
            "  - A1.png\n",
            "  - B2_room.jpeg\n",
            "üéâ Pronto para testar busca por similaridade!\n",
            "üìã PR√ìXIMOS PASSOS:\n",
            "  1. Execute C√âLULA 76 (processar todas as imagens)\n",
            "  2. Execute C√âLULA 70 (busca por similaridade)\n",
            "  3. Execute C√âLULA 71 (an√°lise contextual)\n"
          ]
        }
      ],
      "source": [
        "# C√âLULA 78 (OPCIONAL) - üìÑ EXTRAIR MAIS IMAGENS DO PDF PARA COMPARA√á√ÉO\n",
        "# Esta c√©lula extrai imagens do map.pdf para ter mais dados para compara√ß√£o\n",
        "\n",
        "print(\"=== EXTRAINDO IMAGENS DO PDF PARA AMPLIAR DATASET ===\\n\")\n",
        "\n",
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "\n",
        "def extrair_imagens_do_pdf(pdf_path, output_dir=\"images/\", prefixo=\"map\"):\n",
        "    \"\"\"\n",
        "    Extrai imagens de um PDF e salva na pasta de imagens\n",
        "    \"\"\"\n",
        "    print(f\"üîç Processando PDF: {pdf_path}\")\n",
        "    \n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"‚ùå PDF n√£o encontrado: {pdf_path}\")\n",
        "        return []\n",
        "    \n",
        "    # Criar diret√≥rio se n√£o existir\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # Abrir PDF\n",
        "    doc = fitz.open(pdf_path)\n",
        "    imagens_extraidas = []\n",
        "    \n",
        "    print(f\"üìä PDF tem {len(doc)} p√°ginas\")\n",
        "    \n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc[page_num]\n",
        "        images = page.get_images()\n",
        "        \n",
        "        print(f\"üìÑ P√°gina {page_num + 1}: {len(images)} imagens encontradas\")\n",
        "        \n",
        "        for img_index, img in enumerate(images):\n",
        "            try:\n",
        "                # Extrair imagem\n",
        "                xref = img[0]\n",
        "                pix = fitz.Pixmap(doc, xref)\n",
        "                \n",
        "                # Converter para RGB se necess√°rio\n",
        "                if pix.colorspace and pix.colorspace.n > 3:\n",
        "                    pix = fitz.Pixmap(fitz.csRGB, pix)\n",
        "                \n",
        "                # Nome do arquivo\n",
        "                img_filename = f\"{prefixo}_page_{page_num + 1}_img_{img_index + 1}.png\"\n",
        "                img_path = os.path.join(output_dir, img_filename)\n",
        "                \n",
        "                # Salvar imagem\n",
        "                pix.save(img_path)\n",
        "                imagens_extraidas.append(img_path)\n",
        "                \n",
        "                print(f\"  ‚úÖ Extra√≠da: {img_filename}\")\n",
        "                \n",
        "                pix = None  # Liberar mem√≥ria\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  ‚ùå Erro ao extrair imagem {img_index}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    doc.close()\n",
        "    print(f\"\\nüéâ Total de {len(imagens_extraidas)} imagens extra√≠das!\")\n",
        "    return imagens_extraidas\n",
        "\n",
        "# Verificar quantas imagens temos atualmente\n",
        "# Lista todos os arquivos para debug\n",
        "all_files = os.listdir(\"images/\")\n",
        "print(f\"üìÅ Todos os arquivos na pasta 'images/':\")\n",
        "for f in sorted(all_files):\n",
        "    file_path = os.path.join(\"images/\", f)\n",
        "    if os.path.isfile(file_path):\n",
        "        size = os.path.getsize(file_path)\n",
        "        print(f\"  - {f} ({size} bytes)\")\n",
        "\n",
        "# Contar imagens com extens√µes suportadas (case-insensitive)\n",
        "image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif', '.JPG', '.JPEG', '.PNG', '.BMP')\n",
        "current_images = [f for f in all_files if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif'))]\n",
        "print(f\"\\nüìä Imagens encontradas na pasta: {len(current_images)}\")\n",
        "for img in sorted(current_images):\n",
        "    print(f\"  ‚úÖ {img}\")\n",
        "\n",
        "if len(current_images) <= 1:\n",
        "    print(\"üîÑ Extraindo imagens do PDF para ter mais dados...\")\n",
        "    \n",
        "    # Extrair do map.pdf se existir\n",
        "    if os.path.exists(\"map/map.pdf\"):\n",
        "        imagens_extraidas = extrair_imagens_do_pdf(\"map/map.pdf\", \"images/\", \"map\")\n",
        "        \n",
        "        if imagens_extraidas:\n",
        "            print(f\"\\n‚úÖ {len(imagens_extraidas)} novas imagens adicionadas!\")\n",
        "            print(\"üöÄ Agora execute a C√âLULA 76 novamente para processar todas as imagens\")\n",
        "            print(\"   Depois execute a C√âLULA 70 para testar similaridade com mais dados\")\n",
        "        else:\n",
        "            print(\"‚ùå Nenhuma imagem foi extra√≠da do PDF\")\n",
        "    else:\n",
        "        print(\"‚ùå Arquivo map/map.pdf n√£o encontrado\")\n",
        "        \n",
        "        # Verificar outros PDFs dispon√≠veis\n",
        "        print(\"\\nüîç Procurando outros PDFs...\")\n",
        "        pdf_paths = []\n",
        "        for root, dirs, files in os.walk(\".\"):\n",
        "            for file in files:\n",
        "                if file.lower().endswith('.pdf'):\n",
        "                    pdf_paths.append(os.path.join(root, file))\n",
        "        \n",
        "        if pdf_paths:\n",
        "            print(\"üìã PDFs encontrados:\")\n",
        "            for i, pdf_path in enumerate(pdf_paths[:3], 1):  # Mostrar apenas os 3 primeiros\n",
        "                print(f\"  {i}. {pdf_path}\")\n",
        "                \n",
        "            # Processar o primeiro PDF encontrado\n",
        "            if pdf_paths:\n",
        "                primeiro_pdf = pdf_paths[0]\n",
        "                print(f\"\\nüîÑ Processando: {primeiro_pdf}\")\n",
        "                imagens_extraidas = extrair_imagens_do_pdf(primeiro_pdf, \"images/\", \"doc\")\n",
        "                \n",
        "                if imagens_extraidas:\n",
        "                    print(f\"\\n‚úÖ {len(imagens_extraidas)} imagens extra√≠das de {primeiro_pdf}!\")\n",
        "                    print(\"üöÄ Execute a C√âLULA 76 novamente para processar todas as imagens\")\n",
        "        else:\n",
        "            print(\"‚ùå Nenhum PDF encontrado para extrair imagens\")\n",
        "            \n",
        "else:\n",
        "    print(\"‚úÖ J√° h√° m√∫ltiplas imagens na pasta\")\n",
        "    print(\"Execute a C√âLULA 76 para processar todas e depois a C√âLULA 70 para testar similaridade\")\n",
        "\n",
        "# Mostrar status final - usar a mesma l√≥gica de contagem\n",
        "all_files_final = os.listdir(\"images/\")\n",
        "final_images = [f for f in all_files_final if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp', '.gif'))]\n",
        "print(f\"\\nüìä STATUS FINAL: {len(final_images)} imagens na pasta 'images/'\")\n",
        "print(\"üìã Imagens encontradas:\")\n",
        "for img in sorted(final_images):\n",
        "    print(f\"  - {img}\")\n",
        "\n",
        "if len(final_images) > 1:\n",
        "    print(\"üéâ Pronto para testar busca por similaridade!\")\n",
        "    print(\"üìã PR√ìXIMOS PASSOS:\")\n",
        "    print(\"  1. Execute C√âLULA 76 (processar todas as imagens)\")\n",
        "    print(\"  2. Execute C√âLULA 70 (busca por similaridade)\")\n",
        "    print(\"  3. Execute C√âLULA 71 (an√°lise contextual)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Ainda h√° apenas 1 imagem. Adicione mais imagens manualmente na pasta 'images/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Fun√ß√£o 'processar_imagens_da_pasta' criada com sucesso!\n"
          ]
        }
      ],
      "source": [
        "# C√âLULA 75 (NOVO) - üìÇ PROCESSAMENTO DIRETO DE IMAGENS DA PASTA\n",
        "# Fun√ß√£o para ler todas as imagens da pasta images/ e gerar embeddings para RAG\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from multimodal_qa_with_rag_utils import (\n",
        "    get_image_embedding_from_multimodal_embedding_model,\n",
        "    get_gemini_response\n",
        ")\n",
        "\n",
        "def processar_imagens_da_pasta(\n",
        "    pasta_imagens=\"images/\",\n",
        "    embedding_size=512,\n",
        "    gerar_descricoes=True,\n",
        "    formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "):\n",
        "    \"\"\"\n",
        "    Processa todas as imagens de uma pasta, gerando embeddings e descri√ß√µes para RAG\n",
        "    \n",
        "    Args:\n",
        "        pasta_imagens: Caminho da pasta com imagens\n",
        "        embedding_size: Tamanho do embedding (128, 256, 512, 1408)\n",
        "        gerar_descricoes: Se deve gerar descri√ß√µes das imagens com Gemini\n",
        "        formatos_suportados: Lista de formatos de imagem aceitos\n",
        "    \n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame compat√≠vel com o sistema RAG existente\n",
        "    \"\"\"\n",
        "    print(f\"üîç PROCESSANDO IMAGENS DA PASTA: {pasta_imagens}\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Verificar se a pasta existe\n",
        "    if not os.path.exists(pasta_imagens):\n",
        "        print(f\"‚ùå Pasta '{pasta_imagens}' n√£o encontrada!\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Encontrar todas as imagens na pasta\n",
        "    imagens_encontradas = []\n",
        "    for formato in formatos_suportados:\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "        pattern = os.path.join(pasta_imagens, f\"*{formato.upper()}\")\n",
        "        imagens_encontradas.extend(glob.glob(pattern))\n",
        "    \n",
        "    # Remover duplicatas\n",
        "    imagens_encontradas = list(set(imagens_encontradas))\n",
        "    \n",
        "    if not imagens_encontradas:\n",
        "        print(f\"‚ùå Nenhuma imagem encontrada na pasta '{pasta_imagens}'\")\n",
        "        print(f\"Formatos suportados: {formatos_suportados}\")\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"üìä Encontradas {len(imagens_encontradas)} imagens:\")\n",
        "    for img in imagens_encontradas:\n",
        "        print(f\"  - {os.path.basename(img)}\")\n",
        "    \n",
        "    # Lista para armazenar dados processados\n",
        "    dados_imagens = []\n",
        "    \n",
        "    # Prompt para descri√ß√£o das imagens\n",
        "    prompt_descricao = \"\"\"Analise esta imagem detalhadamente e forne√ßa uma descri√ß√£o precisa.\n",
        "    Inclua:\n",
        "    - O que voc√™ v√™ na imagem\n",
        "    - Elementos principais e detalhes importantes\n",
        "    - Texto vis√≠vel (se houver)\n",
        "    - Tipo de imagem (mapa, diagrama, foto, etc.)\n",
        "    - Informa√ß√µes relevantes para busca e recupera√ß√£o\n",
        "    \n",
        "    Seja espec√≠fico e detalhado para facilitar buscas futuras.\"\"\"\n",
        "    \n",
        "    print(f\"\\nüöÄ PROCESSANDO CADA IMAGEM...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i, caminho_imagem in enumerate(imagens_encontradas, 1):\n",
        "        nome_arquivo = os.path.basename(caminho_imagem)\n",
        "        print(f\"\\nüì∏ PROCESSANDO {i}/{len(imagens_encontradas)}: {nome_arquivo}\")\n",
        "        \n",
        "        try:\n",
        "            # 1. Gerar embedding da imagem\n",
        "            print(\"  üîÑ Gerando embedding...\")\n",
        "            image_embedding = get_image_embedding_from_multimodal_embedding_model(\n",
        "                image_uri=caminho_imagem,\n",
        "                embedding_size=embedding_size,\n",
        "                return_array=True\n",
        "            )\n",
        "            print(f\"  ‚úÖ Embedding gerado: shape {image_embedding.shape}\")\n",
        "            \n",
        "            # 2. Gerar descri√ß√£o da imagem (se solicitado)\n",
        "            descricao = \"\"\n",
        "            if gerar_descricoes:\n",
        "                print(\"  ü§ñ Gerando descri√ß√£o com Gemini...\")\n",
        "                try:\n",
        "                    from vertexai.generative_models import Image as GeminiImage\n",
        "                    imagem_gemini = GeminiImage.load_from_file(caminho_imagem)\n",
        "                    \n",
        "                    descricao = get_gemini_response(\n",
        "                        multimodal_model_2_0_flash,\n",
        "                        model_input=[prompt_descricao, imagem_gemini],\n",
        "                        stream=False,\n",
        "                    )\n",
        "                    print(f\"  ‚úÖ Descri√ß√£o gerada: {len(descricao)} caracteres\")\n",
        "                    \n",
        "                except Exception as desc_error:\n",
        "                    print(f\"  ‚ö†Ô∏è  Erro ao gerar descri√ß√£o: {desc_error}\")\n",
        "                    descricao = f\"Imagem: {nome_arquivo}\"\n",
        "            \n",
        "            # 3. Gerar embedding da descri√ß√£o (para compatibilidade com RAG)\n",
        "            text_embedding = None\n",
        "            if descricao:\n",
        "                try:\n",
        "                    from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
        "                    text_embedding = get_text_embedding_from_text_embedding_model(descricao)\n",
        "                    print(\"  ‚úÖ Text embedding da descri√ß√£o gerado\")\n",
        "                except Exception as text_emb_error:\n",
        "                    print(f\"  ‚ö†Ô∏è  Erro ao gerar text embedding: {text_emb_error}\")\n",
        "            \n",
        "            # 4. Criar registro compat√≠vel com o sistema existente\n",
        "            registro = {\n",
        "                'file_name': f\"pasta_images_{nome_arquivo}\",  # Nome √∫nico\n",
        "                'page_num': 1,  # Imagens individuais = p√°gina 1\n",
        "                'img_num': i,\n",
        "                'img_path': caminho_imagem,\n",
        "                'img_desc': descricao,\n",
        "                'mm_embedding_from_img_only': image_embedding.tolist(),  # Compatibilidade\n",
        "                'text_embedding_from_image_description': text_embedding if text_embedding else None,\n",
        "                'source_type': 'pasta_imagens',  # Identificar origem\n",
        "                'original_filename': nome_arquivo\n",
        "            }\n",
        "            \n",
        "            dados_imagens.append(registro)\n",
        "            print(f\"  ‚úÖ Processamento conclu√≠do para {nome_arquivo}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ‚ùå Erro ao processar {nome_arquivo}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Criar DataFrame\n",
        "    if dados_imagens:\n",
        "        df_imagens = pd.DataFrame(dados_imagens)\n",
        "        print(f\"\\nüéâ PROCESSAMENTO CONCLU√çDO!\")\n",
        "        print(f\"üìä DataFrame criado com {len(df_imagens)} imagens processadas\")\n",
        "        print(f\"üìã Colunas: {list(df_imagens.columns)}\")\n",
        "        \n",
        "        return df_imagens\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Nenhuma imagem foi processada com sucesso\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "print(\"‚úÖ Fun√ß√£o 'processar_imagens_da_pasta' criada com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\n",
            "\n",
            "üîç PROCESSANDO IMAGENS DA PASTA: images/\n",
            "============================================================\n",
            "üìä Encontradas 2 imagens:\n",
            "  - B2_room.jpeg\n",
            "  - A1.png\n",
            "\n",
            "üöÄ PROCESSANDO CADA IMAGEM...\n",
            "============================================================\n",
            "\n",
            "üì∏ PROCESSANDO 1/2: B2_room.jpeg\n",
            "  üîÑ Gerando embedding...\n",
            "  ‚úÖ Embedding gerado: shape (512,)\n",
            "  ü§ñ Gerando descri√ß√£o com Gemini...\n",
            "  ‚ö†Ô∏è  Erro ao gerar descri√ß√£o: 'GenerationResponse' object is not iterable\n",
            "  ‚úÖ Text embedding da descri√ß√£o gerado\n",
            "  ‚úÖ Processamento conclu√≠do para B2_room.jpeg\n",
            "\n",
            "üì∏ PROCESSANDO 2/2: A1.png\n",
            "  üîÑ Gerando embedding...\n",
            "  ‚úÖ Embedding gerado: shape (512,)\n",
            "  ü§ñ Gerando descri√ß√£o com Gemini...\n",
            "  ‚ö†Ô∏è  Erro ao gerar descri√ß√£o: 'GenerationResponse' object is not iterable\n",
            "  ‚úÖ Text embedding da descri√ß√£o gerado\n",
            "  ‚úÖ Processamento conclu√≠do para A1.png\n",
            "\n",
            "üéâ PROCESSAMENTO CONCLU√çDO!\n",
            "üìä DataFrame criado com 2 imagens processadas\n",
            "üìã Colunas: ['file_name', 'page_num', 'img_num', 'img_path', 'img_desc', 'mm_embedding_from_img_only', 'text_embedding_from_image_description', 'source_type', 'original_filename']\n",
            "\n",
            "üéâ SUCESSO TOTAL!\n",
            "üìä image_metadata_df criado com 2 imagens\n",
            "\n",
            "üìã RESUMO DAS IMAGENS PROCESSADAS:\n",
            "==================================================\n",
            "\n",
            "üñºÔ∏è  Imagem 1:\n",
            "  üìÅ Arquivo: B2_room.jpeg\n",
            "  üìÇ Caminho: images/B2_room.jpeg\n",
            "  üìä Embedding shape: 512\n",
            "  üìù Descri√ß√£o: Imagem: B2_room.jpeg\n",
            "\n",
            "üñºÔ∏è  Imagem 2:\n",
            "  üìÅ Arquivo: A1.png\n",
            "  üìÇ Caminho: images/A1.png\n",
            "  üìä Embedding shape: 512\n",
            "  üìù Descri√ß√£o: Imagem: A1.png\n",
            "\n",
            "‚úÖ COMPATIBILIDADE COM SISTEMA RAG:\n",
            "  ‚úÖ img_path: OK\n",
            "  ‚úÖ mm_embedding_from_img_only: OK\n",
            "  ‚úÖ img_desc: OK\n",
            "  ‚úÖ file_name: OK\n",
            "  ‚úÖ page_num: OK\n",
            "\n",
            "üíæ DataFrame salvo em 'image_metadata_from_folder.pkl'\n",
            "\n",
            "üöÄ PR√ìXIMOS PASSOS:\n",
            "1. Agora voc√™ pode executar a C√âLULA 70 (Valida√ß√£o)\n",
            "2. Depois executar a C√âLULA 71 (An√°lise Contextual)\n",
            "3. O sistema RAG est√° pronto para perguntas sobre as imagens!\n"
          ]
        }
      ],
      "source": [
        "# C√âLULA 76 (EXECUTAR) - üöÄ PROCESSAMENTO DAS IMAGENS DA PASTA images/\n",
        "# Executa o processamento de todas as imagens e cria o image_metadata_df\n",
        "\n",
        "print(\"=== PROCESSAMENTO COMPLETO DA PASTA IMAGES/ ===\\n\")\n",
        "\n",
        "# Executar o processamento das imagens\n",
        "try:\n",
        "    image_metadata_df = processar_imagens_da_pasta(\n",
        "        pasta_imagens=\"images/\",\n",
        "        embedding_size=512,\n",
        "        gerar_descricoes=True,  # Gerar descri√ß√µes detalhadas com Gemini\n",
        "        formatos_suportados=['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']\n",
        "    )\n",
        "    \n",
        "    if not image_metadata_df.empty:\n",
        "        print(f\"\\nüéâ SUCESSO TOTAL!\")\n",
        "        print(f\"üìä image_metadata_df criado com {len(image_metadata_df)} imagens\")\n",
        "        \n",
        "        # Mostrar resumo das imagens processadas\n",
        "        print(f\"\\nüìã RESUMO DAS IMAGENS PROCESSADAS:\")\n",
        "        print(\"=\"*50)\n",
        "        for idx, row in image_metadata_df.iterrows():\n",
        "            print(f\"\\nüñºÔ∏è  Imagem {idx + 1}:\")\n",
        "            print(f\"  üìÅ Arquivo: {row['original_filename']}\")\n",
        "            print(f\"  üìÇ Caminho: {row['img_path']}\")\n",
        "            print(f\"  üìä Embedding shape: {len(row['mm_embedding_from_img_only'])}\")\n",
        "            \n",
        "            # Mostrar in√≠cio da descri√ß√£o\n",
        "            desc = row['img_desc']\n",
        "            if desc and len(desc) > 10:\n",
        "                print(f\"  üìù Descri√ß√£o: {desc[:150]}{'...' if len(desc) > 150 else ''}\")\n",
        "        \n",
        "        # Verificar compatibilidade com sistema RAG existente\n",
        "        print(f\"\\n‚úÖ COMPATIBILIDADE COM SISTEMA RAG:\")\n",
        "        colunas_necessarias = ['img_path', 'mm_embedding_from_img_only', 'img_desc', 'file_name', 'page_num']\n",
        "        for col in colunas_necessarias:\n",
        "            if col in image_metadata_df.columns:\n",
        "                print(f\"  ‚úÖ {col}: OK\")\n",
        "            else:\n",
        "                print(f\"  ‚ùå {col}: FALTANDO\")\n",
        "        \n",
        "        # Salvar para uso futuro (opcional)\n",
        "        try:\n",
        "            image_metadata_df.to_pickle(\"image_metadata_from_folder.pkl\")\n",
        "            print(f\"\\nüíæ DataFrame salvo em 'image_metadata_from_folder.pkl'\")\n",
        "        except Exception as save_error:\n",
        "            print(f\"\\n‚ö†Ô∏è  N√£o foi poss√≠vel salvar: {save_error}\")\n",
        "        \n",
        "        print(f\"\\nüöÄ PR√ìXIMOS PASSOS:\")\n",
        "        print(f\"1. Agora voc√™ pode executar a C√âLULA 70 (Valida√ß√£o)\")\n",
        "        print(f\"2. Depois executar a C√âLULA 71 (An√°lise Contextual)\")\n",
        "        print(f\"3. O sistema RAG est√° pronto para perguntas sobre as imagens!\")\n",
        "        \n",
        "    else:\n",
        "        print(f\"\\n‚ùå FALHA: Nenhuma imagem foi processada\")\n",
        "        print(f\"Verifique se:\")\n",
        "        print(f\"- A pasta 'images/' existe\")\n",
        "        print(f\"- H√° imagens v√°lidas na pasta\")\n",
        "        print(f\"- Os modelos est√£o carregados corretamente\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERRO NO PROCESSAMENTO: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    \n",
        "    print(f\"\\nüí° POSS√çVEIS SOLU√á√ïES:\")\n",
        "    print(f\"- Verifique se os modelos est√£o carregados\")\n",
        "    print(f\"- Verifique se a pasta 'images/' existe\")\n",
        "    print(f\"- Execute as c√©lulas de setup dos modelos primeiro\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resposta: Nenhuma das imagens representa um mapa da Am√©rica do Sul. As imagens mostram plantas baixas de edif√≠cios.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Este c√≥digo permite que voc√™ fa√ßa perguntas sobre as imagens usando o embedding e o Gemini\n",
        "\n",
        "import numpy as np\n",
        "from multimodal_qa_with_rag_utils import get_text_embedding_from_text_embedding_model\n",
        "\n",
        "def responder_pergunta_sobre_imagem(pergunta, image_metadata_df, text_embedding_model, gemini_model, top_k=3):\n",
        "    \"\"\"\n",
        "    Dado uma pergunta, retorna a resposta do Gemini baseada nas imagens mais relevantes.\n",
        "    \n",
        "    Args:\n",
        "        pergunta: Texto da pergunta\n",
        "        image_metadata_df: DataFrame com metadados das imagens (deve ter coluna 'text_embedding_from_image_description')\n",
        "        text_embedding_model: Modelo de embedding de texto (TextEmbeddingModel)\n",
        "        gemini_model: Modelo Gemini para gerar respostas\n",
        "        top_k: N√∫mero de imagens mais relevantes para usar no contexto\n",
        "    \"\"\"\n",
        "\n",
        "    # Gere o embedding da pergunta usando o modelo de texto\n",
        "    pergunta_embedding = get_text_embedding_from_text_embedding_model(pergunta, return_array=True)\n",
        "    pergunta_embedding = np.array(pergunta_embedding)\n",
        "\n",
        "    # Calcule similaridades usando os embeddings de texto das descri√ß√µes das imagens\n",
        "    # Primeiro, precisamos converter os embeddings de texto das descri√ß√µes para arrays\n",
        "    text_embeddings_list = []\n",
        "    valid_indices = []\n",
        "    \n",
        "    for idx, row in image_metadata_df.iterrows():\n",
        "        text_emb = row.get('text_embedding_from_image_description')\n",
        "        if text_emb is not None:\n",
        "            if isinstance(text_emb, list):\n",
        "                text_embeddings_list.append(np.array(text_emb))\n",
        "                valid_indices.append(idx)\n",
        "            elif isinstance(text_emb, np.ndarray):\n",
        "                text_embeddings_list.append(text_emb)\n",
        "                valid_indices.append(idx)\n",
        "    \n",
        "    if not text_embeddings_list:\n",
        "        return \"‚ùå Erro: Nenhum embedding de texto encontrado nas descri√ß√µes das imagens. Execute a C√âLULA 76 primeiro para processar as imagens.\"\n",
        "    \n",
        "    # Calcular similaridades\n",
        "    text_embeddings = np.stack(text_embeddings_list)\n",
        "    similarities = np.dot(text_embeddings, pergunta_embedding) / (\n",
        "        np.linalg.norm(text_embeddings, axis=1) * np.linalg.norm(pergunta_embedding) + 1e-8\n",
        "    )\n",
        "    top_indices_local = np.argsort(similarities)[-top_k:][::-1]  # √≠ndices locais\n",
        "    top_indices = [valid_indices[i] for i in top_indices_local]  # √≠ndices originais do DataFrame\n",
        "\n",
        "    # Monte o contexto para o Gemini\n",
        "    contexto_imgs = []\n",
        "    imagens_para_gemini = []\n",
        "    for idx in top_indices:\n",
        "        row = image_metadata_df.iloc[idx]\n",
        "        contexto_imgs.append(\n",
        "            f\"Arquivo: {row['file_name']} (p√°gina {row['page_num']}), descri√ß√£o: {row['img_desc']}\"\n",
        "        )\n",
        "        # Carregar a imagem para incluir no contexto visual\n",
        "        try:\n",
        "            from vertexai.generative_models import Image as GeminiImage\n",
        "            img_obj = GeminiImage.load_from_file(row['img_path'])\n",
        "            imagens_para_gemini.append(img_obj)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è  N√£o foi poss√≠vel carregar imagem {row['img_path']}: {e}\")\n",
        "    \n",
        "    contexto = \"\\n\".join(contexto_imgs)\n",
        "    prompt = (\n",
        "        \"Considere as seguintes descri√ß√µes de imagens extra√≠das e responda a pergunta:\"\n",
        "        f\"\\n\\n{contexto}\\n\\nPergunta: {pergunta}\\nResposta:\"\n",
        "    )\n",
        "\n",
        "    # Chame o Gemini com texto e imagens\n",
        "    try:\n",
        "        if imagens_para_gemini:\n",
        "            # Incluir imagens no contexto\n",
        "            model_input = [prompt] + imagens_para_gemini\n",
        "        else:\n",
        "            model_input = [prompt]\n",
        "        \n",
        "        resposta = gemini_model.generate_content(model_input)\n",
        "        # Pode ser \"resposta.text\" ou apenas \"resposta\" dependendo do SDK\n",
        "        if hasattr(resposta, 'text'):\n",
        "            return resposta.text\n",
        "        else:\n",
        "            return str(resposta)\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Erro ao gerar resposta: {e}\"\n",
        "\n",
        "# Exemplo de uso (corrigido para usar text_embedding_model):\n",
        "resposta = responder_pergunta_sobre_imagem(\"Qual mapa mostra a Am√©rica do Sul?\", image_metadata_df, text_embedding_model, multimodal_model_2_0_flash)\n",
        "print(\"Resposta:\", resposta)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "python3",
      "name": "common-cpu.m116",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/base-cpu:m116"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
